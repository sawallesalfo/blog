---
date: 2025-05-30
authors:
    - ssawadogo
categories: 
    - IAGEN
---


## Trop de SDK pour les LLMs ? Passe Ã  une `LLMFactory`  ou `Adapters` avec LiteLLM

Dans lâ€™univers des LLMs, chaque provider a son propre dialecte.
Tu veux utiliser **OpenAI** ? Tu installes `openai`.
Tu veux **Claude** (Anthropic) ? Câ€™est `anthropic`.
Et pour tester **Groq**, **Mistral**, **Fireworks**, ou mÃªme **AWS Bedrock** ? Chacun vient avec son propre SDK, ses headers custom, sa maniÃ¨re de formuler les prompts, et son format de sortie.

Câ€™est vite le **chaos**. ðŸ˜¤

<!-- more -->

Et quand tu construis une app sÃ©rieuse â€” un backend ou un agent LLM â€” tu ne veux surtout pas que **toute la logique de ton app dÃ©pende dâ€™un SDK spÃ©cifique**.

Câ€™est pour Ã§a quâ€™il faut penser **abstraction**, dÃ¨s le dÃ©part. Et câ€™est lÃ  quâ€™on sort le pattern **`LLMFactory`**.

Perso, dans mes projets, jâ€™ai toujours un petit submodule `llm_factory` qui traÃ®ne.

---

### Pourquoi une `LLMFactory` ?

Une `LLMFactory`, câ€™est comme un **adaptateur intelligent** qui te permet de **changer de fournisseur LLM comme de chemise**, sans toucher au reste de ton code.

Ton app appelle `llm.predict("ma question")`, et la factory se dÃ©brouille â€” que tu sois en local avec Mistral, sur AWS avec Claude 3, ou en API avec GPT-4o.

> **Objectif : dÃ©coupler ton application de la faÃ§on dont tu interagis avec le LLM.**
> Tu choisis le provider au runtime, tu injectes les bonnes clÃ©s, et basta.

---

### ðŸ”Œ LiteLLM Ã  la rescousse

**[LiteLLM](https://github.com/BerriAI/litellm)** te donne une **API unique faÃ§on OpenAI** pour accÃ©der Ã  **plus de 100 modÃ¨les diffÃ©rents**.

Et quand tu combines LiteLLM avec une `LLMFactory`, tu obtiens un design propre, modulaire et **future-proof**.

#### **LiteLLM** en un clin dâ€™Å“il

**LiteLLM** est une brique open-source (librairie Python + proxy server) qui :

* **Expose une API identique Ã  OpenAI** (`completion`, `chat`, `embedding`, `image_generation`)
* **Supporte 100+ modÃ¨les** (OpenAI, Azure, Claude, Mistral, Groq, Bedrock, Cohereâ€¦)
* **Traduit automatiquement** ton payload â€œOpenAI-styleâ€ vers chaque provider
* **Uniformise les rÃ©ponses** en JSON faÃ§on OpenAI
* **GÃ¨re** retry, circuit breaker, fallback (ex. : si OpenAI est down, bascule sur Mistral)
* **IntÃ¨gre** la surveillance (OpenTelemetry, Langfuse, Langsmith) et la gestion des coÃ»ts
* **Sâ€™installe** en deux clics :

  * `pip install litellm`
  * ou en proxy : `litellm --proxy-server --default-model openai/gpt-4o --otel-endpoint ...`

> ðŸ›‘ Cette fois-ci, **on ne rentre pas dans le monitoring** avec Langfuse, Langsmith et autres â€” on garde Ã§a pour une autre fois !

---

### Ã€ quoi ressemble une rÃ©ponse LiteLLM ?

```python
from litellm import completion
import os

os.environ["OPENAI_API_KEY"] = "your-api-key"

response = completion(
  model="openai/gpt-4o",
  messages=[{ "content": "Hello, how do you feel today?", "role": "user" }],
)
```

```json
{
  "id": "chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885",
  "created": 1734366691,
  "model": "claude-3-sonnet-20240229",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "finish_reason": "stop",
      "message": {
        "role": "assistant",
        "content": "Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you..."
      }
    }
  ],
  "usage": {
    "prompt_tokens": 13,
    "completion_tokens": 43,
    "total_tokens": 56
  }
}
```

> ðŸ“ **Note** : que tu appelles GPT-4 via OpenAI, Claude via AWS Bedrock ou Mistral en local, la structure reste **la mÃªme** :
>
> * `choices[0].message.content` â†’ ta rÃ©ponse
> * `usage` â†’ les tokens consommÃ©s
> * `model` â†’ le modÃ¨le utilisÃ©

---

### Exemple de `LLMFactory` (OpenAI, Claude via AWS, etc.)

```python
from typing import Optional, Dict, Any
import litellm
import boto3

class BaseLLMClient:
    def __init__(self, api_key: Optional[str] = None, model_name: str = "", model_params: Optional[Dict[str, Any]] = None):
        self.api_key = api_key
        self.model_name = model_name
        self.model_params = model_params or {}

    def predict(self, query: str, system_prompt: Optional[str] = None) -> str:
        raise NotImplementedError

class OpenAIClient(BaseLLMClient):
    def predict(self, query: str, system_prompt: Optional[str] = None) -> str:
        messages = [{"role": "system", "content": system_prompt}] if system_prompt else []
        messages.append({"role": "user", "content": query})

        response = litellm.completion(
            model=self.model_name,
            messages=messages,
            api_key=self.api_key,
            **self.model_params
        )
        return response["choices"][0]["message"]["content"]

class ClaudeAWSClient(BaseLLMClient):
    def __init__(self, model_name="anthropic.claude-3-sonnet-20240229-v1:0", model_params: Optional[Dict[str, Any]] = None):
        super().__init__(model_name=model_name, model_params=model_params)
        session = boto3.Session()
        credentials = session.get_credentials()
        self.aws_region = "eu-west-1"
        self.aws_access_key = credentials.access_key
        self.aws_secret_key = credentials.secret_key

    def predict(self, query: str, system_prompt: Optional[str] = None) -> str:
        messages = [{"role": "system", "content": system_prompt}] if system_prompt else []
        messages.append({"role": "user", "content": query})

        response = litellm.completion(
            model=self.model_name,
            messages=messages,
            aws_region_name=self.aws_region,
            aws_secret_access_key=self.aws_secret_key,
            aws_access_key_id=self.aws_access_key,
            **self.model_params
        )
        return response["choices"][0]["message"]["content"]

class LLMFactory:
    def __init__(self, config: dict):
        self.config = config

    def get_client(self):
        provider = self.config.get("provider")
        model_name = self.config.get("model_name")
        params = self.config.get("model_params", {})

        if provider == "openai":
            return OpenAIClient(api_key=self.config["api_key"], model_name=model_name, model_params=params)
        elif provider == "aws_claude":
            return ClaudeAWSClient(model_name=model_name, model_params=params)
        else:
            raise ValueError(f"Provider {provider} non supportÃ©.")
```

Et dans ton code principal :

```python
config = {
    "provider": "openai",
    "api_key": os.getenv("OPENAI_KEY"),
    "model_name": "gpt-4o",
    "model_params": {"temperature": 0.3}
}

llm_client = LLMFactory(config).get_client()
query = "Explique-moi le haki de l'observation."
system_prompt = "Tu es expert One Piece et tu ne rÃ©ponds qu'Ã  des questions sur One Piece. Sois jovial comme Luffy."
print(llm_client.predict(query, system_prompt))
```

---

### Conclusion

En 2025, tu ne codes plus ton app autour dâ€™un seul SDK. Tu construis ton backend comme une vraie plateforme :

+ **IndÃ©pendante du fournisseur LLM**
+ **Facile Ã  faire Ã©voluer (changer de modÃ¨le, ajouter un fallback, etc.)**
+ **Modulaire et testable**

Et la `LLMFactory` ou les `ADAPTERS`, câ€™est ta clef pour y arriver â€” surtout si tu tâ€™appuies sur une brique comme **LiteLLM** qui fait le sale boulot dâ€™unifier les appels.


 Je tâ€™invite Ã  lire la [ðŸ“š documentation officielle de LiteLLM](https://github.com/BerriAI/litellm) pour :

* dÃ©ployer ton propre serveur LiteLLM (proxy),
* gÃ©rer tes coÃ»ts,
* monitorer les appels,
* et centraliser lâ€™usage de tes clÃ©s API en toute sÃ©curitÃ©.
