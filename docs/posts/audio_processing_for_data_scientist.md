
---
date : 2025-03-1023  
authors : 
  - ssawadogo  
categories : 
  - Processing
  - IAGEN
---

# Traitement des audios pour la cr√©ation de datasets en Moore

√áa fait un moment que je n‚Äôai pas publi√©, et c‚Äôest surtout parce que j‚Äôai √©t√© absorb√© par mon projet open source de cr√©ation de datasets en Moore et l‚Äôentra√Ænement de mod√®les locaux. Beaucoup de choses ont √©t√© r√©alis√©es en coulisses, et j‚Äôai d√©cid√© de publier un article par mois pour vous tenir au courant. Pour plus de d√©tails, n‚Äôh√©sitez pas √† faire un tour sur mon GitHub [ici](https://github.com/sawadogosalif) ou √† consulter mon profil Hugging Face  [ici](https://huggingface.co/sawadogosalif).

Aujourd‚Äôhui, je vais vous raconter comment j‚Äôai abord√© le traitement de fichiers audio, depuis leur chargement jusqu‚Äô√† leur agr√©gation dans un dataset, en passant par la segmentation des audios. On va voir ensemble comment un simple fichier audio se transforme en un tas d‚Äôarray, pr√™t √† √™tre exploit√© pour du machine learning. C‚Äôest parti !

<!-- more -->

## 1. De l‚Äôaudio aux s√©ries temporelles

Quand j‚Äô√©tais √† l‚Äô√©cole de statistique, on m‚Äôapprenait surtout les s√©ries temporelles sous forme de donn√©es num√©riques. Mais d√®s mes premiers pas en industrie, j‚Äôai r√©alis√© qu‚Äôun fichier audio n‚Äô√©tait rien d‚Äôautre qu‚Äôune s√©rie temporelle, une succession de valeurs qui fluctuent dans le temps. En fait, d√©biter des paroles, c‚Äôest exactement comme analyser les signaux d‚Äôun capteur‚ÄØ! Chaque chiffre dans l‚Äôarray repr√©sente l‚Äôamplitude du signal √† un instant donn√©, et c‚Äôest en les traitant qu‚Äôon peut extraire la musique ou la parole cach√©e dans l‚Äôaudio.

Pour les plus matheux,  Un **signal audio** est math√©matiquement une **s√©rie temporelle continue**, qui repr√©sente l‚Äô√©volution d‚Äôune onde sonore en fonction du temps.  
Un signal audio peut √™tre d√©fini comme une fonction d√©pendant du temps :  

$ S(t) : \mathbb{R}^+ \to \mathbb{R} $
o√π :  
- $ S(t) $ repr√©sente l‚Äôamplitude du signal sonore √† l‚Äôinstant $ t $,  
- $ t $ est le temps en secondes.  

### **Lien avec les s√©ries temporelles**  
Un signal audio est une **s√©rie temporelle**, car c‚Äôest une suite de valeurs mesur√©es √† diff√©rents instants. Lorsqu‚Äôun son est enregistr√©, il est discr√©tis√© en une **s√©rie temporelle discr√®te** selon une fr√©quence d‚Äô√©chantillonnage $ f_s $ :

$
S[n] = S(nT_s), \quad n \in \mathbb{N}
$
o√π :  
- $ S[n] $ est l‚Äôamplitude du signal √† l‚Äôinstant $ n $,  
- $ T_s = \frac{1}{f_s} $ est la p√©riode d‚Äô√©chantillonnage,  
- $ f_s $ est la fr√©quence d‚Äô√©chantillonnage (exemple : 16 kHz signifie 16 000 points par seconde).  

Ainsi, un fichier audio num√©rique (comme un enregistrement vocal) est fondamentalement une **s√©rie temporelle de valeurs d‚Äôamplitude**, tout comme le PIB mesur√© chaque ann√©e est une s√©rie temporelle √©conomique.


## 2. Pr√©-requis : Installation de FFmpeg

Avant de commencer, assurez-vous d‚Äôavoir **FFmpeg** install√©. Cet outil est indispensable pour manipuler vos fichiers audio sur votre pc avaec python

- **Sur Linux (Docker par exemple) :**

  ```dockerfile
  RUN apt-get update && \
      apt-get install -y --no-install-recommends ffmpeg
  ```

- **Sur Windows :**

  Utilisez `winget` :
  ```sh
  winget install ffmpeg
  ```
  Sinon, suivez ce guide complet : [Installation de FFmpeg sur Windows](https://phoenixnap.com/kb/ffmpeg-windows).

---

## 3. Chargement et visualisation d‚Äôun fichier audio

Pour commencer, on utilise **pydub** pour charger le fichier :

```python
from pydub import AudioSegment

file_path = "./nwt_01_Ge_MM_03.mp3"
audio = AudioSegment.from_file(file_path)
```

Ensuite, il est super utile de visualiser le signal audio en d√©cibels (dBFS) pour rep√©rer les silences :

```python
import numpy as np
import matplotlib.pyplot as plt

segment_ms = 100  # D√©coupage en segments de 100 ms
segments = [audio[i:i+segment_ms] for i in range(0, len(audio), segment_ms)]
dbfs_values = [segment.dBFS for segment in segments]
times = np.arange(len(dbfs_values)) * (segment_ms / 1000)

min_dbfs = -80  # Pour remplacer les valeurs -inf des silences
dbfs_values = [max(db, min_dbfs) if db != float('-inf') else min_dbfs for db in dbfs_values]

plt.figure(figsize=(15, 6))
plt.plot(times, dbfs_values)
plt.xlabel('Temps (secondes)')
plt.ylabel('Niveau sonore (dBFS)')
plt.title('Variation du niveau sonore en dBFS')
plt.grid(True)
plt.axhline(y=-35, color='r', linestyle='--', label='Seuil de silence (-35 dBFS)')
plt.legend()
plt.show()
```
![alt text](./audio_processing_for_data_scientist/audio_plot.png)
Comme on le voit clairement, il s'agit bel et bien d'une serie temporelle. 

## 4. D√©tection et segmentation des silences

Quand on fait du deep learning, on a souvent des limites d'entr√©e. Par exemple Whisper a besoin d'audios  de max 30 secondes pour donner de bons resultat en fine tuning.
Comme g√©n√©ralement, nous avons de longs fichiers audios, la solution revient √† decouper le signal en p√©tit segments.  Une des fa√ßon les plus simple est de les decouper en fonction des silences pour garder des verbatims qui ont du context.


L‚Äôidentification des silences permet de d√©couper automatiquement un audio en segments exploitables. G√©n√©ralement, le seuil se situe autour de -40 dB, mais une analyse visuelle vous permettra d‚Äôajuster ce param√®tre au mieux. Par exemple :

```python
start, segments = 0, []
segment_folder = "segments/"
for i, (silence_start, silence_end) in enumerate(silences):
    segment = audio[start:silence_start]
    filename = f"{segment_folder}segment_{i+1}.wav"
    segment.export(filename, format="wav")
    print(f"Segment saved: {filename}")
    start = silence_end
    segments.append(filename)
```


## 5. Cr√©ation d‚Äôun dataset audio sur Hugging Face ! <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" width="30"/>

### 5.1 Hugging Face, la r√©f√©rence des datasets  

Avant l‚Äôav√®nement de plateformes comme Hugging Face, la constitution de datasets audio √©tait souvent un processus artisanal. Il fallait assembler manuellement les fichiers, les annoter avec des scripts personnalis√©s et g√©rer les m√©tadonn√©es de fa√ßon dispers√©e. Les chercheurs utilisaient des fichiers encod√©es en base64, des r√©pertoires organis√©s √† la main ou des formats propri√©taires.  

Avec la mont√©e en puissance du deep learning, il est devenu indispensable de partager des jeux de donn√©es standardis√©s. Hugging Face s‚Äôest impos√© comme la r√©f√©rence pour la gestion et le partage de datasets, notamment dans le domaine de l‚Äôaudio. Gr√¢ce √† leur librairie **datasets**, il est d√©sormais possible de :  

- **Centraliser et standardiser** vos donn√©es audio, facilitant ainsi leur int√©gration dans diff√©rents mod√®les.  
- **Explorer des champs riches** : Au-del√† de l‚Äôaudio brut, vous pouvez ajouter des transcriptions, des informations sur la dur√©e, le locuteur, le contexte de l‚Äôenregistrement et bien plus.  
- **Collaborer et partager** : La communaut√© Hugging Face vous permet de b√©n√©ficier de jeux de donn√©es d√©j√† construits et de contribuer √† une base de connaissances collective, essentielle pour faire avancer la recherche.  


### 5.2 Cr√©ation d‚Äôun dataset audio  

```python
from datasets import Dataset, Audio

# Supposons que vous avez d√©j√† une liste de chemins audio et leurs transcriptions associ√©es
data = {
    "audio": audio_paths,          # Chemins vers vos fichiers audio
    "transcript": transcripts,     # Transcriptions textuelles
    "duration": durations,         # Dur√©e de chaque enregistrement (optionnel)
    "speaker": speakers,           # Informations sur le locuteur (optionnel)
    "recording_date": dates        # Date d'enregistrement (optionnel)
}

# Cr√©ation du dataset
dataset = Dataset.from_dict(data)

# Conversion de la colonne audio au format Audio de Hugging Face
dataset = dataset.cast_column("audio", Audio())

# Ajout d‚Äôun champ s√©quentiel pour le suivi des enregistrements
dataset = dataset.add_column("audio_sequence", list(range(1, len(dataset) + 1)))

print(dataset)
```

Adopter Hugging Face, c‚Äôest b√©n√©ficier d‚Äôun cadre flexible qui r√©volutionne la mani√®re de g√©rer les donn√©es audio. Aujourd‚Äôhui, c‚Äôest devenu **la r√©f√©rence** pour quiconque veut explorer et exploiter au mieux les potentialit√©s du deep learning appliqu√© √† l‚Äôaudio. D'ailleurs les datasets sur le hub sont **versionn√©s**. Donc plus de soucis de revenir en arri√®re.



### 5.3 Autres fonctionnalit√©s importantes   de huggin face

##### **Chargement de datasets prot√©g√©s et gestion de la confidentialit√©**  

Le Hub Hugging Face est une mine d‚Äôor pour les datasets, mais certains sont prot√©g√©s par des autorisations d‚Äôacc√®s. Cela signifie que leurs propri√©taires contr√¥lent qui peut les t√©l√©charger. Hugging Face assure ainsi la confidentialit√© et la conformit√©. Imaginer que vous avez cr√©er votre dataset et que son cout de cr√©ation vaut 5000 euros. Naturellement, vous ne publierer pas en open source  par defaut. Vous decidez de qui peut avoir acc√®s √† vos donn√©es. Pour plus de details, consuter l'article sur la gestion des [ droits des datasets](https://huggingface.co/docs/hub/en/datasets-gated)

##### **Chargement d‚Äôun dataset prot√©g√© : autorisation requise**  

Pour charger un dataset prot√©g√©, vous devez fournir un token d‚Äôacc√®s. Ce token prouve que vous avez l‚Äôautorisation de t√©l√©charger les donn√©es.  

```python
from datasets import DownloadConfig, load_dataset
import os

DATA_FILE = "sawadogosalif/MooreFRCollections_BibleOnlyText"
dataset = load_dataset(DATA_FILE, split="train", download_config=DownloadConfig(token=os.environ["HF_TOKEN"]))
```

`os.environ["HF_TOKEN"]` r√©cup√®re votre token d‚Äôacc√®s depuis les variables d‚Äôenvironnement. Assurez-vous de configurer cette variable avec votre token Hugging Face.  

##### **Chargement d‚Äôun dataset public : acc√®s direct**  

Si le dataset est public, vous pouvez le charger directement sans token.  

```python
from datasets import load_dataset

ds = load_dataset("glaiveai/reasoning-v1-20m", split="train")
```


### **Sauvegarde Locale et sur Serveur S3 : Options de Stockage Flexibles**  

Hugging Face offre une flexibilit√© de stockage. Vous pouvez sauvegarder vos datasets **localement** ou sur des **serveurs cloud** comme S3. 
J'oubliais, la gestion du cache des datasets hugginface est juste insane . Je vous laisse tester √ßa.

##### **Sauvegarde locale : simplicit√© et rapidit√©**  

```python
final_dataset.save_to_disk(output_path)
```

##### **Sauvegarde sur serveur S3 : scalabilit√© et accessibilit√©**  

```python
final_dataset.save_to_disk(
    output_path,
    storage_options={"key": access_key, "secret": secret_key, "client_kwargs": {"endpoint_url": endpoint_url}},
)

print(f"Dataset saved to {output_path}")
```

üìå **Note :**  
`access_key`, `secret_key`, `endpoint_url` correspondent aux informations d‚Äôidentification et √† l‚ÄôURL de votre serveur S3.  


### **La m√©thode `map` : transformation et enrichissement de datasets**  

La m√©thode `map` est un outil puissant pour transformer et enrichir un datasets de fa√ßon rapide. La parallelisation est tr√®s bien g√©er√©√©e. Pour plus de details [ici] (https://huggingface.co/docs/datasets/v3.4.1/en/package_reference/main_classes#datasets.Dataset.map)

Elle applique une fonction √† chaque √©l√©ment du dataset, permettant des op√©rations comme :  

- Le nettoyage de donn√©e
- L‚Äôextraction de caract√©ristiques 
- La tokenisation de texte
- Et bien plus encore...

###### **Exemple d‚Äôutilisation de la m√©thode `map` :**  

```python
def ajouter_longueur(example):
    example["longueur"] = len(example["transcript"])
    return example

dataset_avec_longueur = dataset.map(ajouter_longueur)
```

Dans cet exemple, la fonction `ajouter_longueur` **calcule la longueur de la transcription** dans chaque √©l√©ment du dataset et l‚Äôajoute comme **une nouvelle colonne**.  

##### **Param√®tres cl√©s de la m√©thode `map` :**  

- `function` : La fonction √† appliquer √† chaque √©l√©ment.  
- `batched` : Si `True`, la fonction est appliqu√©e √† des **lots** d‚Äô√©l√©ments.  
- `batch_size` : La taille des lots (si `batched=True`).  
- `num_proc` : Le nombre de processus √† utiliser pour le parall√©lisme.  
- `remove_columns` : Les colonnes √† supprimer apr√®s l‚Äôapplication de la fonction.  



## 6. Choix du format audio pour l‚Äôapprentissage

Le choix du format est crucial pour obtenir des r√©sultats optimaux lors de l‚Äôentra√Ænement :

- **WAV (PCM 16-bit ou 32-bit float)**  
  ‚ûî Le meilleur choix pour l‚Äôentra√Ænement : non compress√© et sans perte de qualit√©, il est parfait pour l‚Äôanalyse fine avec des librairies comme **librosa**, **torchaudio** ou **tensorflow.audio**.

- **FLAC (compression sans perte)**  
  ‚ûî Une alternative int√©ressante pour √©conomiser de l‚Äôespace tout en conservant une qualit√© optimale. Support√© par plusieurs librairies de traitement audio.

- **MP3 (compression avec perte)**  
  ‚ûî √Ä √©viter pour l‚Äôentra√Ænement, car la compression avec perte √©limine certaines informations, notamment dans les hautes fr√©quences, ce qui peut impacter la pr√©cision des mod√®les de reconnaissance vocale ou de classification.


## 7. Agr√©gation de segments d‚Äôaudio dans un dataset

Souvent, un enregistrement est d√©coup√© en plusieurs segments pour en faciliter l‚Äôanalyse. Parfois, il est n√©cessaire de les concat√©ner en r√©introduisant un silence entre chaque segment. Voici un exemple de fonction de mapping pour r√©aliser cette agr√©gation :

```python
def mapper_function2(batch):
    silence_duration = 0.5  # 500 ms de silence
    sampling_rate = 48000
    silence_samples = int(silence_duration * sampling_rate)
    silence_array = np.zeros(silence_samples, dtype=np.float32)
    
    concatenated_audio = []
    for i, audio_segment in enumerate(batch["audio"]):
        concatenated_audio.extend(audio_segment["array"].tolist())
        if i < len(batch["audio"]) - 1:
            concatenated_audio.extend(silence_array.tolist())
    
    concatenated_audio = np.array(concatenated_audio, dtype=np.float32)
    return {
        "audio": [{"array": concatenated_audio, "sampling_rate": sampling_rate}],
        "transcript": [", ".join(batch["transcript"])],
        "page": [batch["page"][0]],
    }

agg_dataset = dataset.map(mapper_function2, batched=True, batch_size=4, remove_columns=list(dataset.features))
agg_dataset = agg_dataset.add_column("audio_sequence", list(range(1, len(agg_dataset) + 1)))
```

Cette √©tape permet de transformer plusieurs segments en une seule s√©quence continue, tout en ins√©rant intelligemment des p√©riodes de silence pour mieux s√©parer les diff√©rents passages.


## 8. √Ä l‚Äôint√©rieur d‚Äôun fichier audio : un tas d‚Äôarray !

Ici, on entre dans le vif du sujet : quand on charge un fichier audio, ce qu‚Äôon obtient, c‚Äôest une immense s√©rie d‚Äôarray de valeurs. Chaque array repr√©sente l‚Äôamplitude du signal √† un instant donn√©. C‚Äôest √† partir de ces donn√©es brutes que l‚Äôon peut :

- Extraire des caract√©ristiques du signal, comme le spectrogramme.
- D√©tecter et segmenter les silences ou rep√©rer les transitions.
- Transformer le signal en repr√©sentations visuelles ou num√©riques exploitables par nos mod√®les.

Pour donner un exemple, voici comment g√©n√©rer un spectrogramme avec **librosa** :

```python
import librosa
import librosa.display

# Charger l'audio (de pr√©f√©rence un fichier WAV pour une qualit√© optimale)
y, sr = librosa.load("path_to_audio_file.wav", sr=48000)
# Calculer le spectrogramme Mel
S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)

plt.figure(figsize=(10, 4))
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')
plt.title('Spectrogramme (Mel)')
plt.tight_layout()
plt.show()
```

Ce passage du fichier audio brut √† sa repr√©sentation en matrices est fondamental pour le traitement et l‚Äôanalyse via des techniques de machine learning. C‚Äôest un peu comme passer d‚Äôune image floue √† une version haute d√©finition gr√¢ce √† un algorithme de traitement d‚Äôimage !


## Conclusion

Le traitement des fichiers audio, de leur d√©coupe en segments jusqu‚Äô√† leur agr√©gation dans un dataset complet et riche en m√©tadonn√©es, est un parcours passionnant et technique. On part d‚Äôun simple signal ‚Äì un tas d‚Äôarray ‚Äì pour arriver √† une repr√©sentation exploitable par des mod√®les de deep learning, tout comme l‚Äôhistoire o√π Git m‚Äôa sauv√© d‚Äôune catastrophe, en nous rappelant que chaque outil, aussi discret soit-il, peut transformer notre mani√®re de travailler.

Dans les prochains articles, je d√©taillerai comment annoter ces donn√©es et entra√Æner des mod√®les pour la reconnaissance vocale ou la classification audio. En attendant, faites un tour sur mon GitHub et sur mon profil Hugging Face pour suivre l‚Äôaventure et n‚Äôh√©sitez pas √† partager vos propres exp√©riences. 


## References

https://huggingface.co/docs/hub/en/datasets-usage
https://huggingface.co/docs/hub/en/datasets-adding
https://huggingface.co/docs/hub/en/datasets-overview