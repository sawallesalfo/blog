{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Personal blog","text":""},{"location":"2024/08/24/all-things-start-with-git/","title":"All things start with git","text":""},{"location":"2024/08/24/all-things-start-with-git/#git-gitlab-et-github-pour-le-cicd","title":"Git, GitLab et GitHub pour le CI/CD","text":"<p>Dans cet article, nous allons explorer comment Git, GitLab, et GitHub sont utilis\u00e9s pour impl\u00e9menter des pipelines CI/CD (Int\u00e9gration Continue et D\u00e9ploiement Continu). Ces outils sont devenus essentiels pour automatiser et g\u00e9rer les processus de d\u00e9veloppement logiciel. Comment vous pouvez les utiliser pour am\u00e9liorer votre flux de travail.</p>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-git","title":"Qu'est-ce que Git ?","text":"<p>Git est un syst\u00e8me de gestion de version distribu\u00e9. Cela signifie qu'il permet \u00e0 plusieurs d\u00e9veloppeurs de travailler sur un projet de mani\u00e8re simultan\u00e9e sans se marcher sur les pieds. Voici quelques concepts de base de Git :</p> <ul> <li>Repository (D\u00e9p\u00f4t) : C'est un espace de stockage o\u00f9 l'historique de votre projet est enregistr\u00e9. Il contient tous les fichiers et l'historique des modifications.</li> <li>Branch (Branche) : Une branche est une version parall\u00e8le du code sur laquelle vous pouvez travailler ind\u00e9pendamment. Une branche peut \u00eatre fusionn\u00e9e avec la branche principale (<code>main</code> ou <code>master</code>) apr\u00e8s approbation.</li> <li>Commit : Un commit est un enregistrement de changements dans le d\u00e9p\u00f4t. Chaque commit a un identifiant unique qui permet de revenir en arri\u00e8re ou de fusionner des modifications.</li> <li>Merge : C'est l'action de fusionner les changements d'une branche dans une autre.</li> </ul>"},{"location":"2024/08/24/all-things-start-with-git/#comment-fonctionne-git","title":"Comment fonctionne Git ?","text":"<p>Voici une illustration simple du fonctionnement de Git avec un exemple de flux de travail :</p> <pre><code>graph TD;\n    A[Clone du d\u00e9p\u00f4t] --&gt; B[Cr\u00e9ation d'une branche];\n    B --&gt; C[Modification du code];\n    C --&gt; D[Commit des changements];\n    D --&gt; E[Fusion de la branche];\n    E --&gt; F[Push vers le d\u00e9p\u00f4t distant];</code></pre>"},{"location":"2024/08/24/all-things-start-with-git/#explication-du-processus-git","title":"Explication du Processus Git","text":"<ol> <li>Clone du D\u00e9p\u00f4t : Vous commencez par cloner un d\u00e9p\u00f4t existant depuis GitHub ou GitLab vers votre machine locale.</li> <li>Cr\u00e9ation d'une Branche : Vous cr\u00e9ez une nouvelle branche pour travailler sur une fonctionnalit\u00e9 ou une correction de bug.</li> <li>Modification du Code : Vous faites les changements n\u00e9cessaires dans votre code.</li> <li>Commit des Changements : Vous enregistrez vos modifications dans le d\u00e9p\u00f4t local avec un message de commit.</li> <li>Fusion de la Branche : Une fois les modifications pr\u00eates, vous fusionnez votre branche dans la branche principale.</li> <li>Push vers le D\u00e9p\u00f4t Distant : Enfin, vous poussez vos changements vers le d\u00e9p\u00f4t distant pour les partager avec les autres d\u00e9veloppeurs.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#quest-ce-que-gitlab-et-github","title":"Qu'est-ce que GitLab et GitHub ?","text":""},{"location":"2024/08/24/all-things-start-with-git/#github","title":"GitHub","text":"<p>GitHub est une plateforme de d\u00e9veloppement collaboratif qui repose sur Git. Elle est principalement utilis\u00e9e pour h\u00e9berger des d\u00e9p\u00f4ts Git et permet de collaborer sur des projets de mani\u00e8re transparente. GitHub offre \u00e9galement des fonctionnalit\u00e9s de CI/CD via GitHub Actions, qui permettent d'automatiser les tests, les builds, et les d\u00e9ploiements.</p>"},{"location":"2024/08/24/all-things-start-with-git/#gitlab","title":"GitLab","text":"<p>GitLab est une plateforme similaire \u00e0 GitHub, mais avec un ensemble d'outils encore plus complet pour le DevOps. GitLab CI/CD est une fonctionnalit\u00e9 int\u00e9gr\u00e9e qui permet de cr\u00e9er des pipelines pour automatiser les tests, les builds, et les d\u00e9ploiements directement depuis le d\u00e9p\u00f4t GitLab.</p>"},{"location":"2024/08/24/all-things-start-with-git/#fonctionnement-de-la-cicd-avec-gitlab-et-github","title":"Fonctionnement de la CI/CD avec GitLab et GitHub","text":"<ol> <li>GitHub Actions (CI/CD)</li> </ol> <p>GitHub Actions vous permet de cr\u00e9er des workflows pour automatiser les processus de d\u00e9veloppement. Ces workflows sont d\u00e9finis dans un fichier YAML au sein du d\u00e9p\u00f4t.</p> <p>Exemple de workflow pour GitHub Actions :</p> <pre><code>name: CI/CD Pipeline\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run tests\n        run: pytest\n</code></pre> <ol> <li>GitLab CI/CD</li> </ol> <p>GitLab CI/CD utilise un fichier <code>.gitlab-ci.yml</code> pour d\u00e9finir les pipelines. Ce fichier d\u00e9crit les \u00e9tapes que GitLab doit suivre pour tester, construire, et d\u00e9ployer le code.</p> <p>Exemple de pipeline pour GitLab CI/CD :</p> <pre><code>stages:\n  - test\n  - build\n  - deploy\n\ntest:\n  stage: test\n  script:\n    - pytest\n\nbuild:\n  stage: build\n  script:\n    - python setup.py sdist\n\ndeploy:\n  stage: deploy\n  script:\n    - scp dist/* user@server:/path/to/deploy/\n</code></pre> <ol> <li>Visualisation du Processus CI/CD</li> </ol> <p>Pour mieux comprendre le processus CI/CD avec Git, GitHub, et GitLab, voici une visualisation avec Mermaid :</p> <pre><code>graph LR\n    A[Push du Code] --&gt; B[Pipeline CI/CD D\u00e9marr\u00e9]\n    B --&gt; C[Tests]\n    C --&gt; D{Tests R\u00e9ussis?}\n    D --&gt;|Oui| E[Build de l'Application]\n    E --&gt; F[D\u00e9ploiement sur Environnement de ProD]\n    D --&gt;|Non| G[Retourne les Erreurs au D\u00e9veloppeur]</code></pre> <p>\u00c9tapes du Processus CI/CD</p> <ol> <li>Push du Code : Le d\u00e9veloppeur pousse son code vers GitLab ou GitHub.</li> <li>Pipeline CI/CD D\u00e9marr\u00e9 : Le push d\u00e9clenche automatiquement un pipeline CI/CD.</li> <li>Tests : Le code est test\u00e9 pour d\u00e9tecter les erreurs ou les bugs.</li> <li>Build : Si les tests r\u00e9ussissent, l'application est construite (compilation, packaging, etc.).</li> <li>D\u00e9ploiement : Enfin, l'application est d\u00e9ploy\u00e9e sur l'environnement de production. Si les tests \u00e9chouent, les erreurs sont retourn\u00e9es au d\u00e9veloppeur pour correction.</li> </ol>"},{"location":"2024/08/24/all-things-start-with-git/#conclusion","title":"Conclusion","text":"<p>Git, GitLab, et GitHub sont des outils puissants pour g\u00e9rer et automatiser le d\u00e9veloppement logiciel. En utilisant Git pour le contr\u00f4le de version et GitLab/GitHub pour le CI/CD, vous pouvez cr\u00e9er un flux de travail robuste qui assure que votre code est test\u00e9, valid\u00e9, et d\u00e9ploy\u00e9 automatiquement \u00e0 chaque changement. Ces pratiques vous permettent de livrer du code de qualit\u00e9 plus rapidement et plus efficacement.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/","title":"CI/CD pour les Data Scientists : Quand le Code Se Met \u00e0 Danser","text":"<p>La collaboration entre data scientists et d\u00e9veloppeurs peut parfois ressembler \u00e0 une partie de ping-pong chaotique : chacun fait rebondir des id\u00e9es et des bouts de code, mais rien ne semble vraiment s'assembler correctement. Heureusement, il existe une solution pour rendre cette danse collaborative plus harmonieuse : la CI/CD (Continuous Integration/Continuous Deployment). Oui, m\u00eame pour les data scientists\u202f! Alors, prenez vos notebooks, ajustez vos lunettes, et d\u00e9couvrons comment transformer cette pagaille en une symphonie bien orchestr\u00e9e :)</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#quest-ce-que-la-cicd","title":"Qu'est-ce que la CI/CD ?","text":"<p>Avant de plonger dans les d\u00e9tails, clarifions ce que signifie CI/CD, surtout pour ceux qui, parmi nous, passent plus de temps \u00e0 jongler des notebooks qu\u2019\u00e0 jongler avec des pipelines. Ou travaillent : generalement en nombre reduits au pr\u00e8s des metiers:</p> <ul> <li>Continuous Integration (CI): une pratique qui consiste \u00e0 int\u00e9grer r\u00e9guli\u00e8rement les modifications du code dans un d\u00e9p\u00f4t central, o\u00f9 elles sont automatiquement test\u00e9es. Imaginez un petit robot qui v\u00e9rifie si chaque ligne de code que vous ajoutez fonctionne bien avec le reste du code, comme un danseur de tango qui s'assure que chaque pas est en harmonie avec le rythme. L'id\u00e9e est de d\u00e9tecter rapidement les erreurs afin qu'elles ne s'accumulent pas comme une pile de vaisselle sale (vous savez, celle qu\u2019on promet de faire plus tard, mais qui finit par devenir un Everest in\u00e9branlable).</li> </ul> <ul> <li>Continuous Deployment (CD): c' est comme la cerise sur le g\u00e2teau. Ici, chaque modification valid\u00e9e (apr\u00e8s les tests de CI) est automatiquement d\u00e9ploy\u00e9e en production. Oui, vous avez bien entendu : plus besoin d'appuyer sur un bouton pour d\u00e9ployer, c'est comme si votre code se d\u00e9ployait tout seul, un peu comme une machine \u00e0 caf\u00e9 qui se pr\u00e9pare elle-m\u00eame une nouvelle tasse de caf\u00e9 d\u00e8s que vous avez termin\u00e9 la pr\u00e9c\u00e9dente. Le r\u00eave, non ?</li> </ul>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#cicd-pour-les-data-scientists-pourquoi","title":"CI/CD pour les Data Scientists : Pourquoi ?","text":"<p>Pendant longtemps, la cicd etait propre aux developpeurs : Bonne pratique de developpement (devops). Avec le developpement de la data science et la volont\u00e9 de maturer les projets data, on a donc commencer \u00e0 entendre parler de MLOPS. Disons que la CICD est une composante pour faire du MlOps.</p> <p>En tant que data scientist, vous pourriez vous demander : \"Pourquoi devrais-je me pr\u00e9occuper de tout ce bazar ? Mes notebooks fonctionnent tr\u00e8s bien tels quels !\" Certes, mais imaginez la sc\u00e8ne : vous travaillez sur un mod\u00e8le hyper complexe, vous l\u2019entra\u00eenez pendant des heures (ou des jours), et puis\u2026 Oups, un autre membre de l'\u00e9quipe modifie le code d'importation des donn\u00e9es, et votre magnifique mod\u00e8le ne fonctionne plus. Catastrophe. Ou plus simplement, vous voudriez suivre l'historique d'un code. Le code marchait -il avant? Difficilement de repondre \u00e0 ces questions \u00e0 priori sans CICD</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#les-enjeux-de-la-collaboration","title":"Les Enjeux de la Collaboration","text":"<p>La collaboration entre plusieurs data scientists (et d\u00e9veloppeurs) sur un m\u00eame projet peut vite devenir compliqu\u00e9e. Chacun a son style, ses m\u00e9thodes, et son code. Comme une recette de cuisine o\u00f9 chaque cuisinier ajoute ses propres ingr\u00e9dients sans se concerter avec les autres, le r\u00e9sultat peut \u00eatre\u2026 surprenant, pour ne pas dire immangeable. </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#la-solution-cicd-pour-data-scientists","title":"La Solution : CI/CD pour Data Scientists","text":"<p>Impl\u00e9menter une cha\u00eene CI/CD dans vos projets de data science permet d'assurer que :</p> <ol> <li> <p>Tous les changements sont test\u00e9s : Vous \u00e9vitez le fameux \"\u00e7a marche sur ma machine !\" en vous assurant que chaque modification est test\u00e9e dans un environnement standardis\u00e9.</p> </li> <li> <p>Le code est toujours pr\u00eat pour la production : Vous pouvez d\u00e9ployer vos mod\u00e8les en production rapidement et en toute confiance, sans avoir \u00e0 passer des jours \u00e0 les v\u00e9rifier manuellement.</p> </li> <li> <p>La documentation et le versionning sont automatiques : Chaque modification est document\u00e9e, et vous pouvez facilement revenir en arri\u00e8re en cas de probl\u00e8me (comme une machine \u00e0 remonter le temps pour votre code).</p> </li> <li> <p>Tout developpeur ou data scientist pourrait reprendre vos travaux sans perdre les cheveux.</p> </li> </ol> <p>C'est quoi git , gitlab ou github dans tout \u00e7a? Je vous invite \u00e0 faire un tour sur cet article au cas o\u00f9 vous n'etes pas tres familier avec des trois mots: github vs gitlab vs git</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-pour-un-projet-data-science","title":"Exemple pour un Projet Data Science","text":"<p>Dans ce guide, nous allons cr\u00e9er un pipeline CI/CD pour un projet de data science sur GitLab. Nous aborderons la structure du projet, la configuration du pipeline avec GitLab CI/CD, les tests, le d\u00e9ploiement d'une application Dash, et un bonus sur l'utilisation des \u201cgit hooks\u201d pour tester localement avant de pousser les changements. Avant tout parlons de </p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#structure-du-projet","title":"Structure du Projet","text":"<p>Voici une structure typique pour un projet de data science utilisant GitLab CI/CD :</p> <pre><code>mon_projet_data_science/\n\u2502\n\u251c\u2500\u2500 .gitlab-ci.yml        # Fichier de configuration pour le pipeline CI/CD\n\u251c\u2500\u2500 requirements.txt      # Fichier listant les d\u00e9pendances Python\n\u251c\u2500\u2500 README.md             # Documentation du projet\n\u251c\u2500\u2500 setup.py              # Script d'installation pour le projet\n\u2502\n\u251c\u2500\u2500 data/                 # R\u00e9pertoire contenant les donn\u00e9es\n\u2502   \u251c\u2500\u2500 raw/              # Donn\u00e9es brutes non trait\u00e9es\n\u2502   \u2514\u2500\u2500 processed/        # Donn\u00e9es pr\u00e9-trait\u00e9es\n\u2502\n\u251c\u2500\u2500 src/                  # R\u00e9pertoire du code source principal\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package Python\n\u2502   \u251c\u2500\u2500 data_loader.py    # Script pour charger et traiter les donn\u00e9es\n\u2502   \u251c\u2500\u2500 model.py          # D\u00e9finition du mod\u00e8le de machine learning\n\u2502   \u2514\u2500\u2500 train_model.py    # Script pour l'entra\u00eenement du mod\u00e8le\n\u2502\n\u251c\u2500\u2500 tests/                # R\u00e9pertoire contenant les tests\n\u2502   \u251c\u2500\u2500 __init__.py       # Fichier d'initialisation du package de tests\n\u2502   \u251c\u2500\u2500 test_data_loader.py  # Tests pour le chargement des donn\u00e9es\n\u2502   \u2514\u2500\u2500 test_model.py     # Tests pour le mod\u00e8le de machine learning\n\u2502\n\u2514\u2500\u2500 notebooks/            # R\u00e9pertoire pour les notebooks Jupyter\n    \u251c\u2500\u2500 exploration.ipynb # Notebook pour l'exploration des donn\u00e9es\n    \u2514\u2500\u2500 analysis.ipynb    # Notebook pour l'analyse des r\u00e9sultats\n</code></pre>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#exemple-avec-gitlab","title":"Exemple avec gitlab","text":"<p>Cela revient \u00e0 configurer le fichier <code>.gitlab-ci.yml</code> Voici un exemple de configuration pour le pipeline CI/CD :</p> <pre><code>stages:\n  - install\n  - test\n  - lint\n  - train\n  - deploy\n\nvariables:\n  VENV_PATH: .venv\n\nbefore_script:\n  - python3 -m venv $VENV_PATH\n  - source $VENV_PATH/bin/activate\n  - pip install --upgrade pip\n  - pip install -r requirements.txt\n\ninstall:\n  stage: install\n  script:\n    - pip install -r requirements.txt\n  cache:\n    paths:\n      - $VENV_PATH\n\ntest:\n  stage: test\n  script:\n    - pytest tests/\n\nlint:\n  stage: lint\n  script:\n    - flake8 .\n\ntrain:\n  stage: train\n  script:\n    - python src/train_model.py\n\ndeploy:\n  stage: deploy\n  script:\n    - echo \"D\u00e9ploiement de l'application Dash sur le serveur de production...\"\n    - scp -r * user@server:/path/to/deployment\n  only:\n    - main\n</code></pre> <p>Explications des \u00c9tapes du Pipeline :</p> <ol> <li>Install Stage : Installe les d\u00e9pendances Python d\u00e9finies dans <code>requirements.txt</code>.</li> <li>Test Stage : Ex\u00e9cute les tests unitaires avec <code>pytest</code> pour s'assurer que chaque composant fonctionne correctement.</li> <li>Lint Stage : Utilise <code>flake8</code> pour v\u00e9rifier la qualit\u00e9 du code et s'assurer qu'il respecte les bonnes pratiques de codage.</li> <li>Train Stage : Lance l'entra\u00eenement du mod\u00e8le de machine learning en ex\u00e9cutant le script <code>train_model.py</code>.</li> <li>Deploy Stage : D\u00e9ploie l'application Dash sur un serveur distant. Cette \u00e9tape est d\u00e9clench\u00e9e uniquement pour la branche <code>main</code>.</li> </ol>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#utiliser-des-git-hooks","title":"Utiliser des Git Hooks","text":"<p>Dans le cas o\u00f9 vous ne disposer pas de serveur distant pour lancer vos codes, vous pourriez utiliser  git hooks pour ex\u00e9cuter les tests locaux. Par exemple, un hook <code>pre-push</code> peut \u00eatre utilis\u00e9 pour ex\u00e9cuter les tests avant chaque <code>git push</code>.</p> <ol> <li> <p>Cr\u00e9er un Hook <code>pre-push</code> :</p> <p>Dans le r\u00e9pertoire <code>.git/hooks</code>, cr\u00e9ez un fichier nomm\u00e9 <code>pre-push</code> :</p> <p><pre><code>touch .git/hooks/pre-push\n</code></pre> 2. Rendre le Hook Ex\u00e9cutable : 3. Ajouter le Script pour Ex\u00e9cuter les Tests :</p> <p>Ouvrez le fichier <code>pre-push</code> et ajoutez le script suivant :</p> </li> </ol> <p><pre><code>#!/bin/bash\n\necho \"Ex\u00e9cution des tests locaux avant le push...\"\nsource .venv/bin/activate\npytest tests/\n\nif [ $? -ne 0 ]; then\n    echo \"\u00c9chec des tests. Annulation du push.\"\n    exit 1\nfi\n\necho \"Tests r\u00e9ussis. Push en cours...\"\n</code></pre> Avec ce hook en place, chaque tentative de git push ex\u00e9cutera les tests locaux. Si les tests \u00e9chouent, le push sera annul\u00e9, garantissant ainsi que seul le code valide est pouss\u00e9 vers le d\u00e9p\u00f4t distant.</p>"},{"location":"2024/08/25/cicd-pour-les-data-scientists--quand-le-code-se-met-%C3%A0-danser/#conclusion","title":"Conclusion","text":"<p>En somme, int\u00e9grer la CI/CD dans vos projets de data science est comme apprendre \u00e0 danser le tango avec vos coll\u00e8gues : c'est au d\u00e9but un peu maladroit, mais une fois que vous avez le rythme, vous ne pouvez plus vous en passer. Cela transforme votre fa\u00e7on de travailler, rend vos collaborations plus fluides, et garantit que vos mod\u00e8les sont toujours au top de leur forme.</p> <p>Alors, chers data scientists, pr\u00eats \u00e0 chausser vos chaussures de danse et \u00e0 adopter la CI/CD ? Parce qu\u2019une fois que vous y aurez go\u00fbt\u00e9, vous ne reviendrez jamais en arri\u00e8re. Promis, jur\u00e9.</p> <p>References : - https://martinfowler.com/articles/continuousIntegration.html</p> <ul> <li>https://martinfowler.com/books/duvall.html</li> </ul>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/","title":"Utilisation d'alias - gestions des variables d'environnement -Windows terminal","text":"<p>Dans cet article, je vais partager avec vous quelques astuces qui peuvent grandement am\u00e9liorer votre efficacit\u00e9 en d\u00e9veloppement : l'utilisation d'alias, la gestion des variables d'environnement, et l'usage du nouveau terminal de Windows. En tant que d\u00e9veloppeur ou data scientist, vous savez que de nombreuses t\u00e2ches r\u00e9p\u00e9titives peuvent devenir sources de frustration. Par exemple, r\u00e9p\u00e9ter sans cesse <code>export LLM_KEY=\"macle\"</code> pour que votre code r\u00e9cup\u00e8re la cl\u00e9 via <code>os.getenv()</code> peut vite devenir fastidieux.</p> <p>Voyons comment cr\u00e9er un fichier d'environnement pour centraliser les variables n\u00e9cessaires et un alias pour acc\u00e9der rapidement \u00e0 ce r\u00e9pertoire et ex\u00e9cuter votre script.</p>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#1-preparer-votre-terminal","title":"1. Pr\u00e9parer votre terminal","text":"<p>Avant de commencer, assurez-vous d'avoir Windows Terminal install\u00e9 sur votre machine si vous travaillez sous Windows. Ce terminal moderne permet d'acc\u00e9der \u00e0 des shells comme PowerShell, l'Invite de Commande, et des environnements Linux via le Windows Subsystem for Linux (WSL). Il offre une exp\u00e9rience utilisateur proche de celle d'un shell Bash sous Linux, avec une gestion intuitive des onglets pour \u00e9viter la multiplication des fen\u00eatres.</p> <p>Conseil : Windows Terminal est bien plus agr\u00e9able que CMD ou m\u00eame l'invite de commande WSL standard. Sa gestion des onglets permet de travailler sur plusieurs fen\u00eatres en parall\u00e8le dans une seule interface. Plus besoin de jongler entre plusieurs fen\u00eatres ! voici \u00e0 quoi il ressemble :  Bon, j'arr\u00e8te de faire du marketing gratuitement pour microsoft. :)</p> <p>T\u00e9l\u00e9charger Windows Terminal</p>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#2-creer-un-fichier-envsh","title":"2. Cr\u00e9er un fichier <code>env.sh</code>","text":"<p>Pour centraliser vos variables d'environnement, cr\u00e9ez un fichier <code>env.sh</code> qui pourra \u00eatre appel\u00e9 \u00e0 chaque lancement de projet. Si vous avez un \u00e9diteur en ligne de commande comme <code>nano</code> ou <code>vim</code> :</p> <pre><code>nano env.sh\n</code></pre> <p>Sinon, cr\u00e9ez simplement le fichier depuis un \u00e9diteur de texte comme Notepad ou Visual Studio Code, et ajoutez-y le contenu suivant :</p> <pre><code>#!/bin/bash\nexport MY_VARIABLE=\"Valeur\"\nexport API_KEY=\"VotreCl\u00e9API\"\n</code></pre> <p>Enregistrez le fichier sous le nom <code>env.sh</code> et rendez-le ex\u00e9cutable (si vous \u00eates dans un terminal compatible avec Bash) :</p> <pre><code>chmod +x env.sh\n</code></pre>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#3-configurer-un-alias-dans-bash","title":"3. Configurer un alias dans Bash","text":"<p>Cr\u00e9ez un alias qui vous permettra de changer de r\u00e9pertoire et de charger vos variables d'environnement en une seule commande.</p> <ol> <li> <p>\u00c9ditez votre fichier de configuration Bash (ou cr\u00e9ez-en un si n\u00e9cessaire) :</p> <pre><code>nano ~/.bashrc\n</code></pre> </li> <li> <p>Ajoutez l'alias en bas du fichier, pour faciliter l'acc\u00e8s au dossier et le chargement des variables d'environnement. Ici, je cr\u00e9e un alias local vers mon projet car je trouve fastidieux de taper <code>cd</code> \u00e0 chaque fois pour me rendre dans le dossier :</p> <pre><code>alias workspace='cd ~/Desktop/\"Project X\" &amp;&amp; source env.sh'\n</code></pre> </li> </ol> <p>Si vous n'avez pas <code>nano</code> ou <code>vim</code>, modifiez simplement le fichier <code>.bashrc</code> dans un \u00e9diteur de texte comme Notepad ou Visual Studio Code. Le fichier <code>.bashrc</code> se trouve g\u00e9n\u00e9ralement dans votre r\u00e9pertoire personnel, aux emplacements suivants : - Sous Linux et macOS : <code>~/.bashrc</code> - Sous Windows avec WSL : <code>/home/nom_utilisateur/.bashrc</code></p>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#4-appliquer-les-modifications","title":"4. Appliquer les modifications","text":"<p>Pour que les modifications soient prises en compte, rechargez le fichier de configuration :</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Ou, tout simplement, fermez et relancez un nouveau terminal.</p>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#5-utilisation-pratique","title":"5. Utilisation pratique","text":"<p>D\u00e9sormais, \u00e0 chaque fois que vous souhaitez lancer votre projet, il vous suffit de taper :</p> <pre><code>workspace\n</code></pre> <p>Cette commande vous am\u00e8nera dans le r\u00e9pertoire <code>Project X</code> et ex\u00e9cutera le fichier <code>env.sh</code>, chargeant ainsi vos variables d'environnement.</p> <p>Ensuite, pour lancer un script Python, tapez simplement :</p> <pre><code>python votre_script.py\n</code></pre> <p>V\u00e9rifiez que <code>votre_script.py</code> se trouve bien dans le r\u00e9pertoire <code>Project X</code> pour qu'il puisse s'ex\u00e9cuter correctement.</p>"},{"location":"2024/10/18/utilisation-dalias---gestions-des-variables-denvironnement--windows-terminal/#conclusion","title":"Conclusion","text":"<p>L'utilisation d'alias Bash est une pratique simple mais puissante pour simplifier votre flux de travail et r\u00e9duire le risque d'erreurs. En quelques commandes, vous pouvez structurer efficacement vos projets et vous concentrer sur l'essentiel. Par exemple, si une commande comme <code>kubectl logs nom_du_pod</code> vous p\u00e8se, vous pouvez utiliser un alias <code>k logs nom_du_pod</code> pour simplifier vos appels. Ce petit changement peut sembler anodin, mais combin\u00e9 \u00e0 d'autres t\u00e2ches, il am\u00e9liore consid\u00e9rablement l'efficacit\u00e9 et rend les sessions terminal plus fluides.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/","title":"Apache Arrow pour l'optimisation du traitement des donn\u00e9es","text":"<p>La gestion des donn\u00e9es en m\u00e9moire ressemble parfois \u00e0 un tatonnement. Les  data engineers, scientists et analysts se retrouvent souvent \u00e0 jongler entre diff\u00e9rents formats de donn\u00e9es, calculs intensifs et besoins de performance. Jusqu\u2019\u00e0 r\u00e9cemment, nous \u00e9tions limit\u00e9s par des outils et formats con\u00e7us pour des volumes et des vitesses bien inf\u00e9rieurs \u00e0 ceux d\u2019aujourd'hui. C\u2019est l\u00e0 qu\u2019Apache Arrow entre en sc\u00e8ne, tel un champion pr\u00eat \u00e0 transformer ce marathon en un sprint ma\u00eetris\u00e9. Dans ce billet de blog, on ne va parler que de donn\u00e9es. Comment python g\u00e8re les dataframes en backend?</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#representation-tabulaire-des-donnees","title":"Repr\u00e9sentation tabulaire des donn\u00e9es","text":""},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#theoriquement","title":"Th\u00e9oriquement","text":"<p>On commence par un petit voyage dans la mani\u00e8re dont les donn\u00e9es sont stock\u00e9es. Imaginez que vous avez une \u00e9norme table remplie de donn\u00e9es :</p> <p></p> <p>Il existe deux fa\u00e7ons principales de l\u2019organiser en m\u00e9moire dans un logiciel :</p> <ul> <li> <p>Row-wise : ici, les donn\u00e9es sont stock\u00e9es ligne par ligne. Chaque ligne repr\u00e9sente un enregistrement complet. C\u2019est comme si vous lisiez un livre page par page, pratique pour \u00e9crire rapidement des donn\u00e9es.</p> <p></p> </li> </ul> <ul> <li> <p>Columnar : dans ce format, les donn\u00e9es sont stock\u00e9es par colonne. Chaque colonne contient les valeurs d\u2019un m\u00eame attribut, ce qui est tr\u00e8s efficace pour les op\u00e9rations de lecture et d'analyse, notamment lors du filtrage ou de l'agr\u00e9gation.</p> <p></p> </li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#en-pratique-stockage-sur-disque","title":"En pratique : stockage sur disque","text":"<p>La mani\u00e8re dont ces formats sont impl\u00e9ment\u00e9s peut avoir un impact majeur sur les performances d'analyse. Prenons l'exemple d'un data scientist travaillant sur de grands ensembles de donn\u00e9es.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#formats-row-wise","title":"Formats Row-wise","text":"<p>Les formats row-wise sont souvent utilis\u00e9s dans des bases de donn\u00e9es transactionnelles comme PostgreSQL ou MySQL. Ils permettent un acc\u00e8s rapide aux enregistrements complets, id\u00e9aux pour des op\u00e9rations telles que les insertions et mises \u00e0 jour. Toutefois, pour les analyses complexes, ce format peut ralentir les choses, car il est moins efficace pour les op\u00e9rations d'agr\u00e9gation ou de filtrage sur des colonnes sp\u00e9cifiques. Par exemple, interroger une base de donn\u00e9es pour des statistiques sur une colonne exige la lecture de toutes les lignes, ce qui entra\u00eene une surcharge.</p> <p>Les formats row-wise incluent \u00e9galement des formats comme CSV, Excel, et JSON :</p> <ul> <li>CSV (Comma-Separated Values) : Ce format, tr\u00e8s r\u00e9pandu, stocke les donn\u00e9es sous forme de texte brut. Bien qu'il soit pratique pour l'\u00e9change de donn\u00e9es, il se r\u00e9v\u00e8le souvent inefficace pour les analyses complexes de grands ensembles de donn\u00e9es, en raison de sa structure plate et non optimis\u00e9e.</li> </ul> <ul> <li>JSON (JavaScript Object Notation) : Structur\u00e9 de mani\u00e8re row-wise, JSON est id\u00e9al pour l'\u00e9change de donn\u00e9es, mais reste inefficace pour les analyses complexes.</li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#formats-columnar","title":"Formats Columnar","text":"<p>Les formats columnar sont optimis\u00e9s pour les performances lors des analyses de donn\u00e9es. Ils permettent des op\u00e9rations rapides sur les colonnes, notamment pour le filtrage et l'agr\u00e9gation. Les formats optimis\u00e9s incluent Apache Parquet, ORC, et Feather.</p> <ul> <li>Parquet : Un format de fichier compress\u00e9 et optimis\u00e9 pour le stockage de grands volumes de donn\u00e9es. Il est particuli\u00e8rement efficace pour les analyses car il stocke les donn\u00e9es par colonne et compresse les valeurs similaires, r\u00e9duisant ainsi l'espace de stockage et am\u00e9liorant les performances d'E/S.</li> </ul> <ul> <li>ORC (Optimized Row Columnar) : Similaire \u00e0 Parquet, ORC offre des performances de lecture rapides et une compression efficace, souvent utilis\u00e9 dans les environnements Hadoop.</li> </ul> <ul> <li>Feather : Format simplifi\u00e9 pour les \u00e9changes rapides de donn\u00e9es entre langages, comme Python et R. Il permet un acc\u00e8s extr\u00eamement rapide et r\u00e9duit le temps d'E/S gr\u00e2ce \u00e0 des buffers efficaces.</li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#backends-arrow-vs-numpy-array","title":"Backends Arrow vs NumPy Array","text":"<p>Lorsque nous travaillons sur des donn\u00e9es en Python, que ce soit pour des calculs ou du stockage, nous utilisons des biblioth\u00e8ques comme Pandas, NumPy, Polars ou PyArrow. Mais comment ces donn\u00e9es sont-elles g\u00e9r\u00e9es en backend ? </p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#gestion-des-formats-de-donnees","title":"Gestion des formats de donn\u00e9es","text":"<p>L'image ci-dessus illustre comment Apache Arrow s'est impos\u00e9 comme un standard pour le traitement de donn\u00e9es en m\u00e9moire. Autrefois, chaque outil devait r\u00e9aliser des conversions pour lire diff\u00e9rents formats de donn\u00e9es (CSV, Parquet, etc.), ce qui entra\u00eenait des pertes de temps et une inefficacit\u00e9 des ressources. Aujourd'hui, Arrow centralise le stockage en m\u00e9moire, permettant aux outils d'acc\u00e9der directement aux donn\u00e9es sans conversion interm\u00e9diaire.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#integration-progressive-du-backend-pyarrow","title":"Int\u00e9gration progressive du backend PyArrow","text":"<p>Par d\u00e9faut, Pandas utilise NumPy comme backend :</p> <pre><code>pd.read_csv('data.csv')\n</code></pre> <p>Depuis Pandas 2.0, il est d\u00e9sormais possible d'utiliser Apache Arrow comme backend pour am\u00e9liorer les performances, notamment pour les types de donn\u00e9es complexes comme les cha\u00eenes de caract\u00e8res et les dates.</p> <p>Voici comment lire un fichier CSV avec le backend PyArrow :</p> <pre><code>pd.read_csv(\"data.csv\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n</code></pre> <p>Sur un fichier de test, cette m\u00e9thode a \u00e9t\u00e9 32 fois plus rapide que l'ex\u00e9cution standard avec NumPy.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#pourquoi","title":"Pourquoi ?","text":"<p>Pour comprendre la raison, r\u00e9sumons rapidement comment Pandas fonctionne. L'id\u00e9e g\u00e9n\u00e9rale est qu'avant de pouvoir faire quoi que ce soit dans Pandas, il est n\u00e9cessaire de charger en m\u00e9moire les donn\u00e9es d'int\u00e9r\u00eat (en utilisant des m\u00e9thodes comme read_csv, read_sql, read_parquet, etc.). Lors du chargement des donn\u00e9es en m\u00e9moire, il est n\u00e9cessaire de d\u00e9cider comment ces donn\u00e9es seront stock\u00e9es. Pour les simples donn\u00e9es comme les entiers ou les flottants, cela n'est g\u00e9n\u00e9ralement pas si compliqu\u00e9, car la repr\u00e9sentation d'un seul \u00e9l\u00e9ment est principalement standard, et nous avons juste besoin de tableaux du nombre d'\u00e9l\u00e9ments dans nos donn\u00e9es. Mais pour d'autres types (comme les cha\u00eenes, les dates et heures, les cat\u00e9gories, etc.),</p> <p>Python est capable de repr\u00e9senter presque tout, mais les structures de donn\u00e9es de Python (listes, dictionnaires, tuples, etc.) sont tr\u00e8s lentes et ne peuvent pas \u00eatre utilis\u00e9es. Ainsi, la repr\u00e9sentation des donn\u00e9es n'est ni en Python ni standard, et une impl\u00e9mentation doit \u00eatre r\u00e9alis\u00e9e via des extensions Python, g\u00e9n\u00e9ralement impl\u00e9ment\u00e9es en C (ou en C++, Rust, et autres). Pendant de nombreuses ann\u00e9es, l'extension principale pour repr\u00e9senter des tableaux et effectuer des op\u00e9rations sur eux de mani\u00e8re rapide a \u00e9t\u00e9 NumPy. Et c'est sur cette base que Pandas a initialement \u00e9t\u00e9 construit.</p> <p>Apache Arrow, quant \u00e0 lui, a \u00e9merg\u00e9 comme un standard pour le traitement en m\u00e9moire. Il permet de centraliser les donn\u00e9es, \u00e9liminant la n\u00e9cessit\u00e9 de conversions co\u00fbteuses entre diff\u00e9rents formats de stockage. Cela am\u00e9liore consid\u00e9rablement les performances des analyses.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#fonctionnement-des-backends-arrow-et-numpy","title":"Fonctionnement des backends Arrow et NumPy","text":"<p>Pour illustrer les diff\u00e9rences entre NumPy et Apache Arrow, g\u00e9n\u00e9rons des donn\u00e9es similaires \u00e0 celles que nous avons utilis\u00e9es pr\u00e9c\u00e9demment. </p> <p>Imaginons que nous avons une colonne avec les cha\u00eenes suivantes :</p> <ul> <li><code>\"numpy\"</code></li> </ul> <ul> <li><code>\"data\"</code></li> </ul> <ul> <li><code>\"analysis\"</code></li> </ul> <ul> <li><code>null</code></li> </ul> <ul> <li><code>\"performance\"</code></li> </ul>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#numpy-backend","title":"NumPy Backend","text":"<p>Avec NumPy, on stockerait ces cha\u00eenes en utilisant un tableau de taille fixe, souvent moins optimal pour les cha\u00eenes de caract\u00e8res.</p> <ul> <li>Structure :<ul> <li>Tableau de cha\u00eenes de longueur maximale, par exemple 15 caract\u00e8res.</li> <li>Chaque cha\u00eene occupe 15 espaces, m\u00eame si elle est plus courte.</li> </ul> </li> </ul> <p>Repr\u00e9sentation hypoth\u00e9tique du tableau NumPy</p> Cha\u00eene Stockage <code>\"numpy\"</code> <code>\"numpy         \"</code> <code>\"data\"</code> <code>\"data         \"</code> <code>\"analysis\"</code> <code>\"analysis    \"</code> <code>null</code> <code>\"\"</code> (ou espace vide) <code>\"performance\"</code> <code>\"performance \"</code>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#apache-arrow-backend","title":"Apache Arrow Backend","text":"<p>Avec Arrow, ces cha\u00eenes sont stock\u00e9es en utilisant plusieurs buffers (un buffer est une zone m\u00e9moire temporaire utilis\u00e9e pour stocker des donn\u00e9es en attente de traitement ou de transfert).</p> <ul> <li>Validity Bitmap Buffer : <code>00011101</code></li> <li>Offsets Buffer : <code>0 5 9 17 17 28</code></li> <li>Values Buffer : <code>numpydataanalysisperformance</code></li> </ul> <p>Repr\u00e9sentation Arrow</p> Composant Valeurs Validity Bitmap Buffer <code>00011101</code> Offsets Buffer <code>0 5 9 17 17 28</code> Values Buffer <code>numpydataanalysisperformance</code> <ul> <li>Validity Bitmap Buffer : Indique l'absence du 4\u00e8me \u00e9l\u00e9ment (<code>null</code>).</li> <li>Offsets Buffer : Montre les d\u00e9buts et fins de chaque cha\u00eene dans le Values Buffer.</li> <li>Values Buffer : Contient les donn\u00e9es r\u00e9elles, stock\u00e9es de mani\u00e8re compacte.</li> </ul> <p>Cela permet une gestion plus efficace de la m\u00e9moire et am\u00e9liore les temps de traitement, notamment pour des op\u00e9rations complexes sur de gros ensembles de donn\u00e9es.</p>"},{"location":"2024/09/21/apache-arrow-pour-loptimisation-du-traitement-des-donn%C3%A9es/#conclusion","title":"Conclusion","text":"<p>Gr\u00e2ce \u00e0 Apache Arrow, les op\u00e9rations de lecture et de calcul sont optimis\u00e9es, quel que soit le format de stockage. Le passage aux formats columnar est un atout majeur, surtout pour le traitement de grandes quantit\u00e9s de donn\u00e9es.</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/","title":"Traitement des audios pour la cr\u00e9ation de datasets audio","text":"<p>\u00c7a fait un moment que je n\u2019ai pas publi\u00e9, et c\u2019est surtout parce que j\u2019ai \u00e9t\u00e9 absorb\u00e9 par mon projet open source de cr\u00e9ation de datasets en Moore et l\u2019entra\u00eenement de mod\u00e8les locaux. Beaucoup de choses ont \u00e9t\u00e9 r\u00e9alis\u00e9es en coulisses, et j\u2019ai d\u00e9cid\u00e9 de publier un article par mois pour vous tenir au courant. Pour plus de d\u00e9tails, n\u2019h\u00e9sitez pas \u00e0 faire un tour sur mon GitHub ici ou \u00e0 consulter mon profil Hugging Face  ici.</p> <p>Aujourd\u2019hui, je vais vous raconter comment j\u2019ai abord\u00e9 le traitement de fichiers audio, depuis leur chargement jusqu\u2019\u00e0 leur agr\u00e9gation dans un dataset, en passant par la segmentation des audios. On va voir ensemble comment un simple fichier audio se transforme en un tas d\u2019array, pr\u00eat \u00e0 \u00eatre exploit\u00e9 pour du machine learning. C\u2019est parti !</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#1-de-laudio-aux-series-temporelles","title":"1. De l\u2019audio aux s\u00e9ries temporelles","text":"<p>Quand j\u2019\u00e9tais \u00e0 l\u2019\u00e9cole de statistique, on m\u2019apprenait surtout les s\u00e9ries temporelles sous forme de donn\u00e9es num\u00e9riques. Mais d\u00e8s mes premiers pas en industrie, j\u2019ai r\u00e9alis\u00e9 qu\u2019un fichier audio n\u2019\u00e9tait rien d\u2019autre qu\u2019une s\u00e9rie temporelle, une succession de valeurs qui fluctuent dans le temps. En fait, d\u00e9biter des paroles, c\u2019est exactement comme analyser les signaux d\u2019un capteur\u202f! Chaque chiffre dans l\u2019array repr\u00e9sente l\u2019amplitude du signal \u00e0 un instant donn\u00e9, et c\u2019est en les traitant qu\u2019on peut extraire la musique ou la parole cach\u00e9e dans l\u2019audio.</p> <p>Pour les plus matheux,  Un signal audio est math\u00e9matiquement une s\u00e9rie temporelle continue, qui repr\u00e9sente l\u2019\u00e9volution d\u2019une onde sonore en fonction du temps. Un signal audio peut \u00eatre d\u00e9fini comme une fonction d\u00e9pendant du temps :  </p> <p>\\(S(t) : \\mathbb{R}^+ \\to \\mathbb{R}\\)</p> <p>o\u00f9 : - \\(S(t)\\) repr\u00e9sente l\u2019amplitude du signal sonore \u00e0 l\u2019instant \\(t\\), - \\(t\\) est le temps en secondes.  </p> <p>Un signal audio est une s\u00e9rie temporelle, car c\u2019est une suite de valeurs mesur\u00e9es \u00e0 diff\u00e9rents instants. Lorsqu\u2019un son est enregistr\u00e9, il est discr\u00e9tis\u00e9 en une s\u00e9rie temporelle discr\u00e8te selon une fr\u00e9quence d\u2019\u00e9chantillonnage $ f_s $ :</p> <p>\\(S[n] = S(nT_s), \\quad n \\in \\mathbb{N}\\) o\u00f9 : - \\(S[n]\\) est l\u2019amplitude du signal \u00e0 l\u2019instant $ n $, - \\(T_s = \\frac{1}{f_s}\\) est la p\u00e9riode d\u2019\u00e9chantillonnage, - \\(f_s\\) est la fr\u00e9quence d\u2019\u00e9chantillonnage (exemple : 16 kHz signifie 16 000 points par seconde).  </p> <p>Ainsi, un fichier audio num\u00e9rique (comme un enregistrement vocal) est fondamentalement une s\u00e9rie temporelle de valeurs d\u2019amplitude, tout comme le PIB mesur\u00e9 chaque ann\u00e9e est une s\u00e9rie temporelle \u00e9conomique.</p> <p>Tr\u00e8ve de bavardage,  on se lance dans le processing d'audio avec Python.</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#2-pre-requis-ffmpeg","title":"2. Pr\u00e9-requis : FFmpeg","text":"<p>Avant de commencer, assurez-vous d\u2019avoir FFmpeg install\u00e9. Cet outil est indispensable pour manipuler vos fichiers audio sur votre pc avaec python</p> <ul> <li> <p>Sur Linux (Docker par exemple) :</p> <pre><code>RUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends ffmpeg\n</code></pre> </li> </ul> <ul> <li> <p>Sur Windows :</p> <p>Utilisez <code>winget</code> : <pre><code>winget install ffmpeg\n</code></pre> Sinon, suivez ce guide complet : Installation de FFmpeg sur Windows.</p> </li> </ul>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#3-chargement-et-visualisation-dun-fichier-audio","title":"3. Chargement et visualisation d\u2019un fichier audio","text":"<p>Pour commencer, on utilise pydub pour charger le fichier :</p> <pre><code>from pydub import AudioSegment\n\nfile_path = \"./nwt_01_Ge_MM_03.mp3\"\naudio = AudioSegment.from_file(file_path)\n</code></pre> <p>Ensuite, il est super utile de visualiser le signal audio en d\u00e9cibels (dBFS) pour rep\u00e9rer les silences :</p> <p><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nsegment_ms = 100  # D\u00e9coupage en segments de 100 ms\nsegments = [audio[i:i+segment_ms] for i in range(0, len(audio), segment_ms)]\ndbfs_values = [segment.dBFS for segment in segments]\ntimes = np.arange(len(dbfs_values)) * (segment_ms / 1000)\n\nmin_dbfs = -80  # Pour remplacer les valeurs -inf des silences\ndbfs_values = [max(db, min_dbfs) if db != float('-inf') else min_dbfs for db in dbfs_values]\n\nplt.figure(figsize=(15, 6))\nplt.plot(times, dbfs_values)\nplt.xlabel('Temps (secondes)')\nplt.ylabel('Niveau sonore (dBFS)')\nplt.title('Variation du niveau sonore en dBFS')\nplt.grid(True)\nplt.axhline(y=-35, color='r', linestyle='--', label='Seuil de silence (-35 dBFS)')\nplt.legend()\nplt.show()\n</code></pre>  Comme on le voit clairement, il s'agit bel et bien d'une serie temporelle. </p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#4-detection-et-segmentation-des-silences","title":"4. D\u00e9tection et segmentation des silences","text":"<p>Quand on fait du deep learning, on a souvent des limites d'entr\u00e9e. Par exemple Whisper a besoin d'audios  de max 30 secondes pour donner de bons resultat en fine tuning. Comme g\u00e9n\u00e9ralement, nous avons de longs fichiers audios, la solution revient \u00e0 decouper le signal en p\u00e9tit segments.  Une des fa\u00e7on les plus simple est de les decouper en fonction des silences pour garder des verbatims qui ont du context.</p> <p>L\u2019identification des silences permet de d\u00e9couper automatiquement un audio en segments exploitables. G\u00e9n\u00e9ralement, le seuil se situe autour de -40 dB, mais une analyse visuelle vous permettra d\u2019ajuster ce param\u00e8tre au mieux. Par exemple :</p> <pre><code>start, segments = 0, []\nsegment_folder = \"segments/\"\nfor i, (silence_start, silence_end) in enumerate(silences):\n    segment = audio[start:silence_start]\n    filename = f\"{segment_folder}segment_{i+1}.wav\"\n    segment.export(filename, format=\"wav\")\n    print(f\"Segment saved: {filename}\")\n    start = silence_end\n    segments.append(filename)\n</code></pre>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#5-creation-dun-dataset-audio-sur-hugging-face","title":"5. Cr\u00e9ation d\u2019un dataset audio sur Hugging Face !","text":""},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#51-la-reference-des-datasets","title":"5.1 La r\u00e9f\u00e9rence des datasets","text":"<p>Avant l\u2019av\u00e8nement de plateformes comme Hugging Face, la constitution de datasets audio \u00e9tait souvent un processus artisanal. Il fallait assembler manuellement les fichiers, les annoter avec des scripts personnalis\u00e9s et g\u00e9rer les m\u00e9tadonn\u00e9es de fa\u00e7on dispers\u00e9e. Les chercheurs utilisaient des fichiers encod\u00e9es en base64, des r\u00e9pertoires organis\u00e9s \u00e0 la main ou des formats propri\u00e9taires.  </p> <p>Avec la mont\u00e9e en puissance du deep learning, il est devenu indispensable de partager des jeux de donn\u00e9es standardis\u00e9s. Hugging Face s\u2019est impos\u00e9 comme la r\u00e9f\u00e9rence pour la gestion et le partage de datasets, notamment dans le domaine de l\u2019audio. Gr\u00e2ce \u00e0 leur librairie datasets, il est d\u00e9sormais possible de :  </p> <ul> <li>Centraliser et standardiser vos donn\u00e9es audio, facilitant ainsi leur int\u00e9gration dans diff\u00e9rents mod\u00e8les.  </li> <li>Explorer des champs riches : Au-del\u00e0 de l\u2019audio brut, vous pouvez ajouter des transcriptions, des informations sur la dur\u00e9e, le locuteur, le contexte de l\u2019enregistrement et bien plus.  </li> <li>Collaborer et partager : La communaut\u00e9 Hugging Face vous permet de b\u00e9n\u00e9ficier de jeux de donn\u00e9es d\u00e9j\u00e0 construits et de contribuer \u00e0 une base de connaissances collective, essentielle pour faire avancer la recherche.  </li> </ul>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#52-creation-dun-dataset-audio","title":"5.2 Cr\u00e9ation d\u2019un dataset audio","text":"<pre><code>from datasets import Dataset, Audio\n\n# Supposons que vous avez d\u00e9j\u00e0 une liste de chemins audio et leurs transcriptions associ\u00e9es\ndata = {\n    \"audio\": audio_paths,          # Chemins vers vos fichiers audio\n    \"transcript\": transcripts,     # Transcriptions textuelles\n    \"duration\": durations,         # Dur\u00e9e de chaque enregistrement (optionnel)\n    \"speaker\": speakers,           # Informations sur le locuteur (optionnel)\n    \"recording_date\": dates        # Date d'enregistrement (optionnel)\n}\n\n# Cr\u00e9ation du dataset\ndataset = Dataset.from_dict(data)\n\n# Conversion de la colonne audio au format Audio de Hugging Face\ndataset = dataset.cast_column(\"audio\", Audio())\n\n# Ajout d\u2019un champ s\u00e9quentiel pour le suivi des enregistrements\ndataset = dataset.add_column(\"audio_sequence\", list(range(1, len(dataset) + 1)))\n\nprint(dataset)\n</code></pre> <p>Adopter Hugging Face, c\u2019est b\u00e9n\u00e9ficier d\u2019un cadre flexible qui r\u00e9volutionne la mani\u00e8re de g\u00e9rer les donn\u00e9es audio. Aujourd\u2019hui, c\u2019est devenu la r\u00e9f\u00e9rence pour quiconque veut explorer et exploiter au mieux les potentialit\u00e9s du deep learning appliqu\u00e9 \u00e0 l\u2019audio. D'ailleurs les datasets sur le hub sont versionn\u00e9s. Donc plus de soucis de revenir en arri\u00e8re.</p> <p>Pour terminer, pour visualiser ou \u00e9couter l'audio depuis un notebook, il faut utiliser le package <code>IPyhton</code>  tout simplement </p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#53-confidentialite-et-protections","title":"5.3 Confidentialit\u00e9 et protections","text":"<p>Le Hub Hugging Face est une mine d\u2019or pour les datasets, mais certains sont prot\u00e9g\u00e9s par des autorisations d\u2019acc\u00e8s. Cela signifie que leurs propri\u00e9taires contr\u00f4lent qui peut les t\u00e9l\u00e9charger.G\u00e9n\u00e9ralement, il suffit d'accepter les conditions d'utilisation  de l'auteur. Hugging Face assure ainsi la confidentialit\u00e9 et la conformit\u00e9. Imaginer que vous avez cr\u00e9er votre dataset et que son cout de cr\u00e9ation vaut 5000 euros. Naturellement, vous ne publierer pas en open source  par defaut. Vous decidez de qui peut avoir acc\u00e8s \u00e0 vos donn\u00e9es. Pour plus de details, consuter l'article sur la gestion des  droits des datasets</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#54-chargement-dun-dataset-public","title":"5.4 Chargement d\u2019un dataset public","text":"<p>Si le dataset est public, vous pouvez le charger directement sans token.  </p> <pre><code>from datasets import load_dataset\n\nds = load_dataset(\"glaiveai/reasoning-v1-20m\", split=\"train\")\n</code></pre>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#55-chargment-dun-dataset-dataset-protege","title":"5.5 Chargment d'un dataset dataset prot\u00e9g\u00e9","text":"<p>Pour charger un dataset prot\u00e9g\u00e9, vous devez fournir un token d\u2019acc\u00e8s. Ce token prouve que vous avez l\u2019autorisation de t\u00e9l\u00e9charger les donn\u00e9es.  </p> <pre><code>from datasets import DownloadConfig, load_dataset\nimport os\n\nDATA_FILE = \"sawadogosalif/MooreFRCollections_BibleOnlyText\"\ndataset = load_dataset(DATA_FILE, split=\"train\", download_config=DownloadConfig(token=os.environ[\"HF_TOKEN\"]))\n</code></pre> <p><code>os.environ[\"HF_TOKEN\"]</code> r\u00e9cup\u00e8re votre token d\u2019acc\u00e8s depuis les variables d\u2019environnement. Assurez-vous de configurer cette variable avec votre token Hugging Face.  </p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#56-sauvegarde-des-datasets","title":"5.6 Sauvegarde des datasets","text":"<p>Hugging Face offre une flexibilit\u00e9 de stockage. Vous pouvez sauvegarder vos datasets localement ou sur des serveurs cloud comme S3.  J'oubliais, la gestion du cache des datasets hugginface est juste insane . Je vous laisse tester \u00e7a.</p> <p>Pour la sauvegarde sur ton pc, il suffit d'utiliser  </p> <p><pre><code>final_dataset.save_to_disk(output_path)\n</code></pre> Quant \u00e0 la sauvegarde sur un bucket distant, il faudrait vous munir des <code>storage_options</code></p> <pre><code>final_dataset.save_to_disk(\n    output_path,\n    storage_options={\"key\": access_key, \"secret\": secret_key, \"client_kwargs\": {\"endpoint_url\": endpoint_url}},\n)\n\nprint(f\"Dataset saved to {output_path}\")\n</code></pre> <p><code>access_key</code>, <code>secret_key</code>, <code>endpoint_url</code> correspondent aux informations d\u2019identification et \u00e0 l\u2019URL de votre serveur S3.  </p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#57-la-methode-map","title":"5.7 La m\u00e9thode <code>map</code>","text":"<p>La m\u00e9thode <code>map</code> est un outil puissant pour transformer et enrichir un datasets de fa\u00e7on rapide. La parallelisation est tr\u00e8s bien g\u00e9er\u00e9\u00e9e. </p> <p>Elle applique une fonction \u00e0 chaque \u00e9l\u00e9ment du dataset, permettant des op\u00e9rations comme :  </p> <ul> <li>Le nettoyage de donn\u00e9e</li> <li>L\u2019extraction de caract\u00e9ristiques </li> <li>La tokenisation de texte</li> <li>Et bien plus encore...</li> </ul> <p>Plus concr\u00e8tement, voici un exemple</p> <pre><code>def ajouter_longueur(example):\n    example[\"longueur\"] = len(example[\"transcript\"])\n    return example\n\ndataset_avec_longueur = dataset.map(ajouter_longueur)\n</code></pre> <p>Dans cet exemple, la fonction <code>ajouter_longueur</code> calcule la longueur de la transcription dans chaque \u00e9l\u00e9ment du dataset et l\u2019ajoute comme une nouvelle colonne.  </p> <p>Les param\u00e8tre cl\u00e9s de map sont nombreuses;. voici les plus importants. </p> <ul> <li><code>function</code> : La fonction \u00e0 appliquer \u00e0 chaque \u00e9l\u00e9ment.  </li> <li><code>batched</code> : Si <code>True</code>, la fonction est appliqu\u00e9e \u00e0 des lots d\u2019\u00e9l\u00e9ments.  </li> <li><code>batch_size</code> : La taille des lots (si <code>batched=True</code>).  </li> <li><code>num_proc</code> : Le nombre de processus \u00e0 utiliser pour le parall\u00e9lisme.  </li> <li><code>remove_columns</code> : Les colonnes \u00e0 supprimer apr\u00e8s l\u2019application de la fonction.  </li> </ul> <p>Pour plus de details [ici] (https://huggingface.co/docs/datasets/v3.4.1/en/package_reference/main_classes#datasets.Dataset.map)</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#6-format-audio-pour-training","title":"6. Format audio pour training","text":"<p>Le choix du format est crucial pour obtenir des r\u00e9sultats optimaux lors de l\u2019entra\u00eenement :</p> <ul> <li>WAV (PCM 16-bit ou 32-bit float)   \u2794 Le meilleur choix pour l\u2019entra\u00eenement : non compress\u00e9 et sans perte de qualit\u00e9, il est parfait pour l\u2019analyse fine avec des librairies comme librosa, torchaudio ou tensorflow.audio.</li> </ul> <ul> <li>FLAC (compression sans perte)   \u2794 Une alternative int\u00e9ressante pour \u00e9conomiser de l\u2019espace tout en conservant une qualit\u00e9 optimale. Support\u00e9 par plusieurs librairies de traitement audio.</li> </ul> <ul> <li>MP3 (compression avec perte)   \u2794 \u00c0 \u00e9viter pour l\u2019entra\u00eenement, car la compression avec perte \u00e9limine certaines informations, notamment dans les hautes fr\u00e9quences, ce qui peut impacter la pr\u00e9cision des mod\u00e8les de reconnaissance vocale ou de classification.</li> </ul>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#7-agregation-de-segments-daudio-dans-un-dataset","title":"7. Agr\u00e9gation de segments d\u2019audio dans un dataset","text":"<p>Souvent, un enregistrement est d\u00e9coup\u00e9 en plusieurs segments pour en faciliter l\u2019analyse. Parfois, il est n\u00e9cessaire de les concat\u00e9ner en r\u00e9introduisant un silence entre chaque segment. Voici un exemple de fonction de mapping pour r\u00e9aliser cette agr\u00e9gation :</p> <pre><code>def mapper_function2(batch):\n    silence_duration = 0.5  # 500 ms de silence\n    sampling_rate = 48000\n    silence_samples = int(silence_duration * sampling_rate)\n    silence_array = np.zeros(silence_samples, dtype=np.float32)\n\n    concatenated_audio = []\n    for i, audio_segment in enumerate(batch[\"audio\"]):\n        concatenated_audio.extend(audio_segment[\"array\"].tolist())\n        if i &lt; len(batch[\"audio\"]) - 1:\n            concatenated_audio.extend(silence_array.tolist())\n\n    concatenated_audio = np.array(concatenated_audio, dtype=np.float32)\n    return {\n        \"audio\": [{\"array\": concatenated_audio, \"sampling_rate\": sampling_rate}],\n        \"transcript\": [\", \".join(batch[\"transcript\"])],\n        \"page\": [batch[\"page\"][0]],\n    }\n\nagg_dataset = dataset.map(mapper_function2, batched=True, batch_size=4, remove_columns=list(dataset.features))\nagg_dataset = agg_dataset.add_column(\"audio_sequence\", list(range(1, len(agg_dataset) + 1)))\n</code></pre> <p>Cette \u00e9tape permet de transformer plusieurs segments en une seule s\u00e9quence continue, tout en ins\u00e9rant intelligemment des p\u00e9riodes de silence pour mieux s\u00e9parer les diff\u00e9rents passages.</p>"},{"location":"2025/03/23/traitement-des-audios-pour-la-cr%C3%A9ation-de-datasets-audio/#conclusion","title":"Conclusion","text":"<p>Le traitement des fichiers audio, de leur d\u00e9coupe en segments jusqu\u2019\u00e0 leur agr\u00e9gation dans un dataset complet et riche en m\u00e9tadonn\u00e9es, est un parcours passionnant et technique. Dans les prochains articles, je d\u00e9taillerai comment annoter ces donn\u00e9es et entra\u00eener des mod\u00e8les pour la reconnaissance vocale ou la classification audio. </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/","title":"Construire et publier son propre package Open Source Python","text":"<p>Quand on d\u00e9bute avec Python pour l\u2019analyse de donn\u00e9es, on commence souvent par installer des packages comme NumPy ou Pandas apr\u00e8s avoir configur\u00e9 Python. Au fil du temps, lorsqu\u2019on g\u00e8re plusieurs projets, on se retrouve avec une \"bo\u00eete \u00e0 outils\" compos\u00e9e de fonctions r\u00e9currentes que l\u2019on transporte de projet en projet.  </p> <p>Mais, au lieu de copier et coller du code, pourquoi ne pas structurer ce travail dans un package Python ? Cela facilite le partage et l\u2019utilisation, que ce soit par vos coll\u00e8gues ou la communaut\u00e9.  </p> <p>Je me souviens qu\u2019\u00e0 l\u2019\u00e9poque o\u00f9 j\u2019\u00e9tais en \u00e9cole d\u2019ing\u00e9nieur, chaque fois que j\u2019installais un package avec <code>pip install</code>, je me demandais comment il \u00e9tait possible de cr\u00e9er ces outils accessibles \u00e0 tous. Voir les informations d\u2019un package avec une commande comme <code>pip show</code> me fascinait (image en bas).     C\u2019est cette curiosit\u00e9 qui m\u2019a pouss\u00e9 \u00e0 explorer le fonctionnement interne des packages Python et \u00e0 apprendre comment les concevoir moi-m\u00eame.  </p> <p>Dans cet article, nous allons d\u00e9couvrir les \u00e9tapes et les bonnes pratiques pour construire et publier votre propre package Python, de votre ordinateur local jusqu\u2019\u00e0 PyPI (Python Package Index).</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#i-pourquoi-creer-un-package","title":"I. Pourquoi cr\u00e9er un package ?","text":"<p>Si vous \u00eates passionn\u00e9 de data science, vous avez probablement l\u2019habitude de cr\u00e9er des fonctions que vous stockez dans des notebooks ou des fichiers <code>.py</code>. Cependant, au fur et \u00e0 mesure que votre code devient complexe, il devient plus logique et efficace de regrouper ces fonctions dans un package.  </p> <p>Imaginez que vous avez d\u00e9velopp\u00e9 un algorithme performant. Plut\u00f4t que de l\u2019envoyer \u00e0 vos coll\u00e8gues sous forme de fichiers, vous pourriez publier un package que tout le monde peut installer facilement avec <code>pip</code>. Et dans l\u2019esprit de l\u2019open source, vous pourriez m\u00eame partager ce travail avec la communaut\u00e9 mondiale.  </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#ii-les-etapes-de-lopen-source-packaging","title":"II. Les \u00e9tapes de l'open source packaging","text":"<p>Un package Python est une collection de code r\u00e9utilisable, comme NumPy ou Pandas, qui aide les d\u00e9veloppeurs \u00e0 r\u00e9soudre des probl\u00e8mes sp\u00e9cifiques. Publier un package open source signifie que ce code est accessible gratuitement \u00e0 tous, sous r\u00e9serve d\u2019une licence (par exemple, MIT ou GPL).  </p> <p>Cela peut sembler compliqu\u00e9 au d\u00e9part, mais les \u00e9tapes sont simples si elles sont bien suivies. Voici une vue d\u2019ensemble pour cr\u00e9er et partager un package open source : Les \u00e9tapes pour construire un package Python**  </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#a-creez-une-structure-claire","title":"a. Cr\u00e9ez une structure claire","text":"<p>Voici un exemple de structure typique pour un package :  </p> <pre><code>my_package/\n\u251c\u2500\u2500 my_package/\n\u2502   \u251c\u2500\u2500 __init__.py      # Fichier qui rend le dossier utilisable comme un package\n\u2502   \u251c\u2500\u2500 module1.py       # Contient une fonction ou classe\n\u2502   \u2514\u2500\u2500 module2.py\n\u251c\u2500\u2500 tests/               # Tests unitaires\n\u2502   \u251c\u2500\u2500 test_module1.py\n\u251c\u2500\u2500 setup.py             # Fichier de configuration pour installer le package\n\u251c\u2500\u2500 README.md            # Description de votre package\n\u251c\u2500\u2500 LICENSE              # Type de licence choisie\n\u2514\u2500\u2500 requirements.txt     # D\u00e9pendances n\u00e9cessaires\n</code></pre>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#b-du-code-reutilisable","title":"b. Du code r\u00e9utilisable","text":"<p>Placez vos fonctions et classes dans des fichiers <code>.py</code> bien organis\u00e9s. Documentez-les avec des docstrings et ajoutez des exemples d\u2019utilisation.  </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#c-des-tests-unitaires","title":"c. Des tests unitaires","text":"<p>Les tests permettent de garantir la fiabilit\u00e9 de votre package. Par exemple, avec pytest : Comme dirait un de mes coll\u00e8gue, si ton projet a des tests bien \u00e9crit, on a meme pas besoin de la documentation pour comprendre le projet.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#d-un-fichier-setuppy","title":"d. Un fichier <code>setup.py</code>","text":"<p>Ce fichier d\u00e9crit votre package et ses d\u00e9pendances. Exemple : <pre><code>from setuptools import setup, find_packages\n\nsetup(\n    name=\"my_package\",\n    version=\"0.1\",\n    description=\"Un package simple pour la data science\",\n    author=\"Votre Nom\",\n    license=\"MIT\",\n    packages=find_packages(),\n    install_requires=[\"numpy\", \"pandas\"],\n)\n</code></pre> Si vous rappeler de l'image en introductiona avec le pip , show c est ce fameux fichier qui est a l'origine.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#e-choisissez-une-licence","title":"e. Choisissez une licence","text":"<p>ell prot\u00e8ge vos droits tout en autorisant la communaut\u00e9 \u00e0 utiliser votre travail.  Ajoutez un fichier <code>LICENSE</code> \u00e0 la racine du projet avec le texte d\u2019une licence adapt\u00e9e : - MIT : Permissive, simple et populaire. - GPL : Plus restrictive, impose que tout code d\u00e9riv\u00e9 reste open source. - Apache 2.0 : Similaire \u00e0 MIT avec des clauses sp\u00e9cifiques pour les brevets.  </p> <p>Utilisez ChooseALicense pour trouver la licence qui correspond \u00e0 votre projet parce qu'il y en beaucoup.  Pour etre honnete , je ne connais que 5 parmis toutes ces licencses</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#f-un-fichier-readmemd","title":"f. Un fichier <code>README.md</code>","text":"<p>Votre README doit inclure : - Une description du package. - Les instructions d\u2019installation. - Des exemples d\u2019utilisation.  </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#g-publier-votre-package","title":"g. Publier votre package","text":"<ul> <li>Avant de publier, installez et testez votre package en local : <pre><code>pip install .\n</code></pre> Pour  Publier sur PyPI** </li> <li>Cr\u00e9er un compte si vous avez pas  et recuperer vos tokens</li> <li>. Pr\u00e9parez votre package : <pre><code>python setup.py sdist\n</code></pre>    Cela g\u00e9n\u00e8re un dossier <code>dist/</code> contenant un fichier <code>.tar.gz</code>.  </li> </ul> <ul> <li>**Installez Twine ** : <pre><code>pip install twine\n</code></pre></li> <li>Uploadez sur PyPI : <pre><code>twine upload dist/*\n</code></pre></li> </ul> <ul> <li>V\u00e9rifiez votre package sur PyPI.  </li> </ul>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#h-heberger-votre-code","title":"h. H\u00e9berger votre code","text":"<p>Rendre le code accessible et visible en ligne pour que les gens soit avis\u00e9 de comment fonctionne le pcakges GitHub est id\u00e9al pour : - H\u00e9berger le code source. - Permettre la collaboration. - Ajouter une documentation d\u00e9taill\u00e9e.  </p> Plateforme Pourquoi choisir ? PyPI Distribution simple via <code>pip install</code>. GitHub H\u00e9bergement et collaboration. Anaconda Cloud Partage pour la science des donn\u00e9es via <code>conda</code>. GitLab Gestion int\u00e9gr\u00e9e avec CI/CD."},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#iii-pratique-et-best-pratices","title":"III. Pratique et Best pratices","text":"<p>La construction d\u2019un package Python va bien au-del\u00e0 du simple regroupement de fichiers Python. Un package bien structur\u00e9 et conforme aux standards facilite son adoption, sa maintenance, et sa distribution, que ce soit en interne ou via des plateformes publiques comme PyPI. Les bonnes pratiques permettent de : 1. Standardiser la structure pour une meilleure lisibilit\u00e9 et \u00e9volutivit\u00e9. 2. Automatiser les t\u00e2ches courantes, comme le build, les tests ou la publication. 3. Am\u00e9liorer la documentation pour les utilisateurs et d\u00e9veloppeurs tiers. 4. R\u00e9duire les erreurs en testant localement avant toute publication. 5. Gagner du temps avec des processus clairs et reproductibles.  </p> <p>Ces quelques bonnes  vous assure une approche progressive et bien document\u00e9e pour construire, documenter et publier votre package Python de mani\u00e8re professionnelle.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#1-la-meilleure-structuration","title":"1. La meilleure structuration","text":"<p>Apr\u00e8s avoir d\u00e9velopp\u00e9 plusieurs packages en entreprise et gr\u00e2ce \u00e0 mes retours d\u2019exp\u00e9rience, l\u2019id\u00e9e est d\u2019automatiser le fichier <code>setup.py</code> le plus t\u00f4t possible tout en utilisant un fichier <code>package_metadata.py</code> pour centraliser les m\u00e9tadonn\u00e9es.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#structure","title":"Structure","text":"<p>Voici une structure id\u00e9ale pour un package Python bien organis\u00e9 : </p> <pre><code>mon_package/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 mon_package/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 mon_module.py\n\u2502   \u2502   \u251c\u2500\u2500 package_metadata.py  # M\u00e9tadonn\u00e9es centralis\u00e9es\n\u2502   \u2502   \u251c\u2500\u2500 sub_package/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 sub_module.py\n\u2502   \u2502   \u251c\u2500\u2500 sub_package2/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 sub_module2.py\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_mon_module.py\n\u2502   \u251c\u2500\u2500 test_sub_package.py\n\u2502   \u251c\u2500\u2500 test_sub_package2.py\n\u2502\n\u251c\u2500\u2500 setup.py  # Automatis\u00e9 avec m\u00e9tadonn\u00e9es\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 HISTORY.md  # Changelog pour les versions\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 MANIFEST.in  # Inclusion des fichiers non Python\n</code></pre>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#fichier-setuppy-optimal","title":"Fichier <code>setup.py</code> optimal","text":"<p>Ce fichier utilise les m\u00e9tadonn\u00e9es centralis\u00e9es et facilite la documentation automatique sur PyPI.</p> <pre><code>#!/usr/bin/env python\n\n\"\"\"The setup script.\"\"\"\n\nfrom setuptools import setup, find_packages\nimport pathlib\nimport runpy\nimport pkg_resources\n\nSRC_PATH = \"src\"\n\n# R\u00e9cup\u00e9ration des m\u00e9tadonn\u00e9es\nmetadata_path = next(pathlib.Path(SRC_PATH).glob(\"*/package_metadata.py\"))\nmetadata = runpy.run_path(metadata_path)\n\nauthor = metadata[\"__author__\"]\nemail = metadata[\"__email__\"]\ndoc = metadata[\"__doc__\"]\nname = metadata[\"__name__\"]\nurl = metadata[\"__url__\"]\nversion = metadata[\"__version__\"]\n\n# Lecture des fichiers requis pour PyPI\nwith open(\"README.md\") as readme_file:\n    readme = readme_file.read()\n\nwith open(\"HISTORY.md\") as history_file:\n    history = history_file.read()\n\nwith open(\"requirements.txt\") as requirements_file:\n    requirements = [\n        str(requirement)\n        for requirement in pkg_resources.parse_requirements(requirements_file)\n    ]\n\nsetup_requirements = [\n    \"pytest-runner\",\n]\n\ntest_requirements = [\n    \"pytest&gt;=3\",\n]\n\nsetup(\n    name=name,\n    url=url,\n    keywords=name,\n    version=version,\n    zip_safe=False,\n    packages=find_packages(SRC_PATH),  # Packages dans src/\n    package_dir={\"\": SRC_PATH},  # Racine des packages : src/\n    long_description=readme + \"\\n\\n\" + history,\n    long_description_content_type=\"text/markdown\",\n    author=author,\n    author_email=email,\n    description=doc,\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\"&gt;=3.8\",\n    test_suite=\"tests\",\n    tests_require=test_requirements,\n    setup_requires=setup_requirements,\n    install_requires=requirements,\n)\n</code></pre>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#fichier-manifestin","title":"Fichier <code>MANIFEST.in</code>","text":"<p>Ce fichier garantit que des fichiers essentiels comme <code>README.md</code> ou <code>HISTORY.md</code> ou requirement.txt sont inclus dans la distribution. Sans lui, certains fichiers pourraient \u00eatre ignor\u00e9s lors du build. D'ailleurs , le build echoue avec le tools build sans manifest.</p> <p>Exemple : <pre><code>include README.md\ninclude HISTORY.md\ninclude LICENSE\ninclude requirements.txt\n</code></pre></p> <p>Vous pouvez aussi inclure des fichiers sp\u00e9cifiques comme des images ou des assets n\u00e9cessaires </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#resultat-sur-pypi","title":"R\u00e9sultat sur PyPI","text":"<p>Une fois toutes les \u00e9tapes suivies, votre package sera bien document\u00e9 sur PyPI. Voici un aper\u00e7u de la documentation g\u00e9n\u00e9r\u00e9e :  </p> <p></p> <p>Gr\u00e2ce \u00e0 un <code>README.md</code> coupl\u00e9 setup.py bien r\u00e9dig\u00e9s et au fichier <code>HISTORY.md</code>, vous obtenez une pr\u00e9sentation claire et professionnelle de votre package.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#2-build-en-local","title":"2. Build en local","text":"<p>Construire un package localement permet de s'assurer qu'il est pr\u00eat avant publication.</p> <ol> <li> <p>Installer les d\u00e9pendances pour le build : <pre><code>python -m pip install --upgrade pip\npython -m pip install build\n</code></pre></p> </li> <li> <p>Construire les distributions : <pre><code>python -m build\n</code></pre></p> </li> <li> <p>Installer localement pour tester : <pre><code>pip install dist/mon_package-0.0.3-py3-none-any.whl\n</code></pre></p> </li> </ol>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#3-le-fichier-historymd","title":"3. Le fichier <code>HISTORY.md</code>","text":"<p>Un bon changelog permet de suivre les \u00e9volutions du package. Exemple :  </p> <pre><code># Changelog\n\n## [0.0.3] - 2024-11-25\n### Added\n- Fonctionnalit\u00e9 de scraping.\n\n### Fixed\n- Correction d\u2019un bug de mise \u00e0 jour du cache.\n\n## [0.0.2] - 2024-11-20\n### Added\n- Initialisation du projet.\n</code></pre> <p>Ajoutez ce fichier dans <code>MANIFEST.in</code> pour qu\u2019il soit inclus dans la distribution.</p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#4-cicd-pour-pypi","title":"4. CICD pour PyPI","text":"<p>Voici un exemple d\u2019automatisation avec GitHub Actions :  </p> <pre><code>name: Publish to PyPI\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.8\"\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          python -m pip install build\n      - name: Build package\n        run: python -m build\n      - name: Publish to PyPI\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          password: ${{ secrets.PYPI_API_TOKEN }}\n</code></pre>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#5-gestion-des-versions","title":"5. Gestion des versions","text":"<p>Suivez les conventions Semantic Versioning :  </p> <ul> <li>1.0.0 : Version majeure (rupture de compatibilit\u00e9).  </li> <li>1.1.0 : Version mineure (nouvelles fonctionnalit\u00e9s).  </li> <li>1.1.1 : Patch (corrections de bugs).  </li> </ul> <p>Ainsi si vous faites de changement importante dans le package, il faut passer aux versions superieurse. c'est ce qui explique pourquoi pandas est pass\u00e9  en version 2 en  Avril 2023 (changement important sur dtype_backend).</p> <p>Tenez \u00e0 jour votre fichier <code>HISTORY.md</code> \u00e0 chaque nouvelle version. Cela garantit une documentation claire des \u00e9volutions. Regarder les notes de release de pandas par ici. N'est ce pas merveilleux?</p> <p>D'ailleurs , il faut \u00e0 chaque fois modifier la version dans <code>package_metadata.py</code> sinon vos Push ne marcheront pas. Vos obtiendez des errerus de type :  </p> <p>Sur github, vous pouvez cr\u00e9er vos released \u00e9galement mais bon cela est equivalent \u00e0 l'histrique des version deja disponble sur pypi.  </p>"},{"location":"2024/11/10/construire-et-publier-son-propre-package-open-source-python/#conclusion","title":"Conclusion","text":"<p>En suivant ces bonnes pratiques, vous avez d\u00e9sormais toutes les cl\u00e9s pour cr\u00e9er et maintenir un package Python bien structur\u00e9. Que ce soit pour un usage interne avec des outils comme Artifactory ou Nexus, ou pour publier sur PyPI, chaque \u00e9tape compte pour garantir la qualit\u00e9 et la s\u00e9curit\u00e9 de vos projets.</p> <p>Un package bien construit facilite son int\u00e9gration continue, ses tests et sa mise \u00e0 jour. N'oubliez pas l'importance du fichier <code>MANIFEST.in</code> pour inclure tous vos fichiers n\u00e9cessaires et de tenir \u00e0 jour votre <code>History.md</code> pour vos changelogs.</p> <p>Avec un peu de rigueur, vous pouvez construire des packages solides et les partager en toute confiance. Alors, pr\u00eat \u00e0 passer \u00e0 l'action et \u00e0 d\u00e9ployer votre package ? </p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/","title":"Faire son Blog avec MkDocs et github actions","text":"<p>Dans ce billet de blog, j'essayerai de retracer  cr\u00e9er et d\u00e9ployer un blog en utilisant MkDocs, un outil qui facilite la cr\u00e9ation de belles documentations gr\u00e2ce \u00e0 des fichiers <code>markdown</code>. Nous allons couvrir chaque \u00e9tape pour t'aider \u00e0 mettre ton blog en ligne.</p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#ce-dont-tu-as-besoin","title":"Ce dont tu as besoin","text":"<ol> <li>Compte GitHub : Tu auras besoin d\u2019un compte GitHub pour stocker et d\u00e9ployer ton blog.</li> <li>Connaissances de Base : Une familiarit\u00e9 avec GitHub, Docker et quelques bases de la ligne de commande sera utile.</li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etape-1-configuration-de-ton-blog","title":"\u00c9tape 1 : Configuration de ton blog","text":"<ol> <li> <p>Cr\u00e9er un r\u00e9pertoire gitHub :</p> <ul> <li>Va sur GitHub et connecte-toi.</li> <li>Clique sur New pour cr\u00e9er un nouveau r\u00e9pertoire.</li> <li>Nomme ton r\u00e9pertoire, par exemple <code>mon-blog</code>.</li> <li>Choisis Public ou Private selon ta pr\u00e9f\u00e9rence.</li> <li>Clique sur Create repository.</li> </ul> </li> <li> <p>Installer MkDocs localement :</p> <ul> <li>Ouvre ton terminal ou la ligne de commande.</li> <li>Installe MkDocs avec pip :    <pre><code>pip install mkdocs\n</code></pre></li> </ul> </li> <li> <p>Configurer ton blog :</p> <ul> <li>Navigue jusqu\u2019au dossier o\u00f9 tu souhaites cr\u00e9er ton blog.</li> <li>Ex\u00e9cute :    <pre><code>mkdocs new mon-blog\n</code></pre></li> <li>Cela cr\u00e9e un dossier nomm\u00e9 <code>mon-blog</code> avec les fichiers de base pour ton blog.</li> </ul> </li> <li> <p>Personnaliser Ton Blog :</p> <ul> <li>Ouvre le fichier <code>mkdocs.yml</code> dans le dossier de ton blog. C\u2019est ici que tu d\u00e9finis le nom et le th\u00e8me de ton blog.</li> <li> <p>Modifie le fichier <code>mkdocs.yml</code> pour qu\u2019il ressemble \u00e0 ceci :</p> <pre><code>site_name: Mon Blog\ntheme:\n  name: material\nnav:\n  - Accueil: index.md\n</code></pre> </li> </ul> <ul> <li>Ajoute du contenu en modifiant <code>index.md</code> ou en cr\u00e9ant de nouveaux fichiers Markdown dans le dossier <code>docs</code>.</li> </ul> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etape-2-construire-et-deployer-avec-docker","title":"\u00c9tape 2 : Construire et d\u00e9ployer avec docker","text":"<ol> <li> <p>Cr\u00e9er une Image Docker :</p> <ul> <li>R\u00e9dige un Dockerfile pour inclure MkDocs et les outils n\u00e9cessaires.</li> <li> <p>Voici un Dockerfile de base :</p> <pre><code># Utiliser l'image Python 3.12\nFROM python:3.12-slim\n\n# D\u00e9finir le r\u00e9pertoire de travail\nWORKDIR /app\n\n# Installer MkDocs et les plugins\nRUN pip install mkdocs mkdocs-material ghp-import\n\n# Copier les fichiers du blog dans l'image Docker\nCOPY . /app\n\n# D\u00e9finir la commande pour construire MkDocs\nCMD [\"mkdocs\", \"build\", \"--verbose\", \"--site-dir\", \"site\"]\n</code></pre> </li> </ul> <ul> <li>Construis l'image Docker avec :    <pre><code>docker build -t mon-blog-image .\n</code></pre></li> </ul> </li> <li> <p>D\u00e9ployer ton blog :</p> <ul> <li> <p>Cr\u00e9e un fichier workflow GitHub Actions pour automatiser le d\u00e9ploiement. Sauvegarde le fichier suivant sous <code>.github/workflows/deploy.yml</code> dans ton r\u00e9pertoire :</p> <pre><code>name: Build and Deploy MkDocs Site\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n  workflow_dispatch:\n\nenv:\n  IMAGE_NAME: mon-blog-image\n  IMAGE_TAG: latest\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Build MkDocs site\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} mkdocs build --verbose --site-dir site\n\n      - name: Deploy to GitHub Pages\n        if: github.ref == 'refs/heads/main'\n        run: |\n          docker run --rm -v ${{ github.workspace }}:/app -w /app ${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }} /bin/bash -c \"\n            ghp-import -n -p -f site -r https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git -b gh-pages\"\n\n      - name: Clean up Docker resources\n        run: docker system prune -f\n</code></pre> </li> </ul> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#etapes-finales","title":"\u00c9tapes finales","text":"<ol> <li> <p>Pousser tes modifications :</p> <ul> <li>Commit et pousse tes modifications vers GitHub :    <pre><code>git add .\ngit commit -m \"Configuration du site MkDocs\"\ngit push origin main\n</code></pre></li> </ul> </li> <li> <p>Assurer les permissions correctes pour le d\u00e9ploiement :</p> <ul> <li>Pour \u00e9viter les probl\u00e8mes de permission avec GitHub Actions, assure-toi que le <code>GITHUB_TOKEN</code> dispose des permissions n\u00e9cessaires.</li> <li>V\u00e9rifie les param\u00e8tres du r\u00e9pertoire :<ul> <li>Va sur ton r\u00e9pertoire GitHub.</li> <li>Navigue vers Settings &gt; Actions &gt; General.</li> <li>Sous Workflow permissions, assure-toi que Read and write permissions sont s\u00e9lectionn\u00e9es.</li> </ul> </li> </ul> </li> <li> <p>V\u00e9rifier le d\u00e9ploiement :</p> <ul> <li>Va sur ton r\u00e9pertoire GitHub.</li> <li>Navigue vers Settings &gt; Pages.</li> <li>Assure-toi que la source est d\u00e9finie sur la branche <code>gh-pages</code>.</li> </ul> <p>Ton blog devrait maintenant \u00eatre en ligne ! Visite l\u2019URL fournie dans les param\u00e8tres de GitHub Pages pour voir ton site.</p> </li> </ol>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#conclusion","title":"Conclusion","text":"<p>F\u00e9licitations ! Tu as construit et d\u00e9ploy\u00e9 un blog en utilisant MkDocs et Docker. Ce guide vise \u00e0 simplifier le processus pour que tu puisses facilement partager ton contenu en ligne. Bon blogging !</p> <p>Pour un exemple complet, jette un \u0153il \u00e0 mon projet : R\u00e9pertoire GitHub.</p>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#references","title":"R\u00e9f\u00e9rences:","text":"<ul> <li>https://squidfunk.github.io/mkdocs-material/getting-started/</li> </ul>"},{"location":"2024/08/18/faire-son-blog-avec-mkdocs-et-github-actions/#maj","title":"MAJ","text":"<ul> <li>08/09/2024 : Traduire en fran\u00e7ais</li> <li>14/09/2024 : Ajout de r\u00e9f\u00e9rences</li> </ul>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/","title":"Faites du Dask plut\u00f4t que du Spark si vous avez juste de grosses tables de donn\u00e9es","text":"<p>Dans le monde actuel des donn\u00e9es, traiter de grands volumes n\u00e9cessite des solutions performantes et \u00e9volutives. Le calcul distribu\u00e9 permet de g\u00e9rer efficacement ces donn\u00e9es en les r\u00e9partissant sur plusieurs machines/cores/workers (selon votre cas). Cependant, toutes les entreprises n'ont pas besoin d'une infrastructure lourde et co\u00fbteuse telle qu'un cluster Spark. Dask, un framework Python, est une alternative l\u00e9g\u00e8re et flexible, particuli\u00e8rement adapt\u00e9e aux environnements d\u00e9j\u00e0 bas\u00e9s sur Python.</p> <p>Ce billet de blog explore pourquoi Dask est une excellente option pour les data scientists qui manipulent de gros volumes de donn\u00e9es, sans pour autant entrer dans la cat\u00e9gorie du Big Data.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#prerequis","title":"Pr\u00e9requis","text":"<p>Si vous ne les avez pas d\u00e9j\u00e0 install\u00e9s, vous aurez besoin des paquets suivants : <pre><code>pip install dask[distributed]\npip install \"bokeh&gt;=3.1.0\"  # Pour visualiser le tableau de bord du cluster\n</code></pre></p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#1-donnees-volumineuses-et-le-besoin-de-calcul-distribue","title":"1. Donn\u00e9es volumineuses et le besoin de calcul distribu\u00e9","text":"<p>Lorsque le volume de donn\u00e9es devient trop important pour \u00eatre trait\u00e9 en m\u00e9moire sur une seule machine, les performances diminuent rapidement. Des t\u00e2ches courantes comme l'entra\u00eenement de mod\u00e8les de machine learning, l'analyse de donn\u00e9es, ou encore la cr\u00e9ation de visualisations interactives peuvent \u00eatre ralenties. \u00c0 ce stade, le besoin de calcul distribu\u00e9 devient critique. Le principe est simple : r\u00e9partir la charge de travail sur plusieurs machines pour parall\u00e9liser les calculs, et ainsi r\u00e9duire consid\u00e9rablement le temps de traitement. Lorsqu'on ne poss\u00e8de pas de serveur, il s'agit de parall\u00e9liser sur le nombre de c\u0153urs du PC.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#2-deux-frameworks-de-calcul-distribue-spark-et-dask","title":"2. Deux frameworks de calcul distribu\u00e9 : Spark et Dask","text":"<p>Pour r\u00e9pondre \u00e0 ces d\u00e9fis, deux frameworks majeurs sont souvent utilis\u00e9s : Spark et Dask. Alors que Spark a gagn\u00e9 en popularit\u00e9 dans les infrastructures Big Data, Dask est particuli\u00e8rement appr\u00e9ci\u00e9 dans les environnements Python. Voici un tableau comparatif pour mieux comprendre leurs diff\u00e9rences :</p> Caract\u00e9ristiques Spark Dask Langage de programmation Majoritairement Scala et Java Python API de haut niveau DataFrame API Dask DataFrame Scalabilit\u00e9 Hautement scalable, con\u00e7u pour le Big Data Scalabilit\u00e9 adapt\u00e9e aux workflows Python Type de cluster Clusters distribu\u00e9s (Hadoop, Kubernetes) Clusters distribu\u00e9s ou local sur une machine \u00c9cosyst\u00e8me Int\u00e9gr\u00e9 avec des outils Big Data (HDFS, Hive) Int\u00e9gr\u00e9 avec NumPy, Pandas, Scikit-learn Simplicit\u00e9 d'installation Plus complexe, n\u00e9cessite souvent un cluster Hadoop Plus simple, peut \u00eatre ex\u00e9cut\u00e9 localement Machine Learning Biblioth\u00e8que int\u00e9gr\u00e9e (MLlib) Int\u00e9gr\u00e9 avec Scikit-learn, XGBoost, etc. Taille des donn\u00e9es Con\u00e7u pour des p\u00e9taoctets Convient pour des volumes moyens \u00e0 grands Maturit\u00e9 Plus mature, utilis\u00e9 \u00e0 grande \u00e9chelle Moins mature, mais en pleine expansion Co\u00fbt Tr\u00e8s cher \u00e0 mettre en place un cluster spark Pas besoin de cluster d\u00e9di\u00e9 pour le faire tourner efficacement"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#3-attention-tout-nest-pas-du-big-data","title":"3. Attention, tout n'est pas du Big Data","text":"<p>Il ne faut pas confondre donn\u00e9es volumineuses et Big Data. Beaucoup d'entreprises ont de gros ensembles de donn\u00e9es, mais cela ne suffit pas pour dire qu'elles font du Big Data. Pour m\u00e9riter ce titre, il faut r\u00e9pondre aux crit\u00e8res des 5V :</p> <ul> <li>Volume : Une quantit\u00e9 massive de donn\u00e9es.</li> <li>V\u00e9locit\u00e9 : La vitesse \u00e0 laquelle ces donn\u00e9es sont g\u00e9n\u00e9r\u00e9es.</li> <li>Vari\u00e9t\u00e9 : Diff\u00e9rents types de donn\u00e9es (structur\u00e9es, non structur\u00e9es, semi-structur\u00e9es).</li> <li>V\u00e9racit\u00e9 : La qualit\u00e9 et la fiabilit\u00e9 des donn\u00e9es.</li> <li>Valeur : Les insights et b\u00e9n\u00e9fices que l'on peut tirer de ces donn\u00e9es.</li> </ul> <p>Les g\u00e9ants comme Google, Amazon, et Netflix op\u00e8rent v\u00e9ritablement dans le domaine du Big Data. Ils g\u00e8rent des volumes immenses, des flux continus de donn\u00e9es, et disposent d'une infrastructure optimis\u00e9e pour tout ce traitement de masse. Cependant, si une entreprise a quelques dizaines de millions de lignes avec une vingtaine de colonnes, elle n\u2019a pas n\u00e9cessairement besoin d\u2019un cluster Spark. Souvent, une solution comme Dask, plus l\u00e9g\u00e8re et flexible, est beaucoup mieux adapt\u00e9e \u00e0 ses besoins.</p> <p>Vous avez compris d\u00e9j\u00e0. Dans ce billet de blog, je ne veux pas vous convaincre d'utiliser Spark. Je vous presenterai plut\u00f4t Dask. Ce dernier est compos\u00e9 de plusieurs API :</p> <ul> <li>Arrays (s\u2019appuie sur NumPy)</li> <li>DataFrames (repose sur Pandas)</li> <li>Bag (suit map/filter/groupby/reduce)</li> <li>Dask-ML (fonctionne avec Scikit-Learn)</li> <li>Delayed (couvre de mani\u00e8re g\u00e9n\u00e9rale Python)</li> </ul> <p>Puisque nous sommes en data science, on parlera de dataframe et de Dask-ML. Pour plus de r\u00e9f\u00e9rences, consultez les liens dans les r\u00e9f\u00e9rences.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#4-le-cluster-dask","title":"4. Le Cluster Dask","text":"<p>Dask est un outil hyper flexible qui s'adapte aussi bien sur ta machine locale que sur un cluster distribu\u00e9. Cela signifie que tu peux commencer avec une configuration l\u00e9g\u00e8re et \u00e9voluer au fur et \u00e0 mesure que tes besoins augmentent. Le noyau de sa magie r\u00e9side dans le Dask Scheduler, qui g\u00e8re l\u2019ex\u00e9cution des calculs de mani\u00e8re dynamique, m\u00eame sur plusieurs machines.</p> <p>Les clusters Dask sont faciles \u00e0 d\u00e9ployer sur des services comme Kubernetes, AWS, ou m\u00eame avec un simple SSH sur des machines distantes. Compar\u00e9 \u00e0 des solutions plus lourdes comme Spark, cette flexibilit\u00e9 aide \u00e0 r\u00e9duire les co\u00fbts d\u2019infrastructure. Dans mon projet, je l'ex\u00e9cute en local, mais ce n\u2019est pas une obligation. Cela permet de suivre de mani\u00e8re efficace les ressources utilis\u00e9es.</p> <p>Lorsque tu cr\u00e9es ton client Dask, un lien vers le tableau de bord s'affiche. Je te conseille de le garder ouvert d\u2019un c\u00f4t\u00e9 de ton \u00e9cran pendant que tu travailles sur ton notebook, m\u00eame si c\u2019est parfois un peu compliqu\u00e9 de jongler avec les fen\u00eatres. C\u2019est extr\u00eamement utile lorsque tu commences \u00e0 apprendre \u00e0 utiliser Dask.</p> <p>Voici comment cela fonctionne en local : </p> <p>On peut voir comment le client est configur\u00e9 et o\u00f9 le dashboard tourne. Je recommande d\u2019y jeter un \u0153il pour voir comment les workers ex\u00e9cutent ton code en coulisses. </p> <p>Encore une fois, ce n\u2019est pas obligatoire, mais c\u2019est surtout pratique pour suivre les t\u00e2ches, particuli\u00e8rement lorsqu'on veut monitorer des processus ETL.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#5-composant-dataframes-dask","title":"5. Composant DataFrames Dask","text":"<p>Le Dask DataFrame est la version parall\u00e8le du c\u00e9l\u00e8bre Pandas DataFrame. Donc, si tu ma\u00eetrises d\u00e9j\u00e0 Pandas, tu n'auras besoin que de petits ajustements pour te mettre \u00e0 Dask. Ce qui est cool, c\u2019est qu\u2019il est con\u00e7u pour g\u00e9rer des datasets plus gros que ce que Pandas peut traiter en m\u00e9moire, tout en gardant une interface famili\u00e8re.</p> <p>Pandas charge tout en m\u00e9moire, ce qui n\u2019est pas toujours id\u00e9al. Dask, en revanche, divise tes donn\u00e9es en partitions, trait\u00e9es en parall\u00e8le. R\u00e9sultat, tu peux travailler avec des datasets plus volumineux que ta RAM sans trop de stress, tout en continuant d'utiliser des op\u00e9rations classiques comme groupby, merge, ou apply. Un DataFrame Dask est essentiellement un ensemble de DataFrames Pandas.</p> <p></p> <p>Une Petite Histoire...</p> <p>Pour l'anecdote, une amie statisticienne se battait avec un fichier CSV de 7 gigas sur son PC avec 64 gigas de RAM. Elle souhaitait pr\u00e9parer ses donn\u00e9es pour de la mod\u00e9lisation statistique, mais le traitement prenait 4 heures. Avec Dask, nous avons pu r\u00e9duire ce temps \u00e0... 19 minutes ! Elle \u00e9tait abasourdie. La cl\u00e9 \u00e9tait d'utiliser Dask pour son ETL, puis de sauvegarder sa table interm\u00e9diaire en format Parquet. Avec suffisamment de RAM et de c\u0153urs, nous avons pu ex\u00e9cuter tout cela directement sur son PC.</p> <p>Regarde mon code ci-dessous : </p> <p>Comme tu vois, c\u2019est du Pandas classique, non ? La seule diff\u00e9rence, c\u2019est que les r\u00e9sultats ne s'affichent qu'apr\u00e8s avoir appel\u00e9 la m\u00e9thode <code>.compute()</code>. Pourquoi ? Parce que Dask fonctionne en mode \"lazy execution\". Il pr\u00e9pare toutes les op\u00e9rations, et c\u2019est seulement quand tu appelles <code>.compute()</code> qu\u2019il ex\u00e9cute tout d'un coup, optimisant ainsi le processus.</p> <p>Ce calcul \u00e9tait dix fois plus rapide qu\u2019avec du Pandas classique. Impressionnant, non ? Si tu veux en savoir plus, jette un \u0153il ici : Dask DataFrame Examples.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#6-composant-dask-ml","title":"6. Composant Dask-ML","text":"<p>Dask ne propose pas de biblioth\u00e8que int\u00e9gr\u00e9e comme MLlib dans Spark, mais il s\u2019int\u00e8gre parfaitement avec les biblioth\u00e8ques populaires de machine learning de l\u2019\u00e9cosyst\u00e8me Python, telles que Scikit-learn, XGBoost, et TensorFlow. Gr\u00e2ce au module Dask-ML, tu peux \u00e9tendre l\u2019entra\u00eenement de mod\u00e8les sur plusieurs machines, ce qui facilite le traitement de datasets volumineux sans forc\u00e9ment recourir \u00e0 des solutions lourdes.</p> <p>Dans un environnement classique, Scikit-learn utilise <code>joblib</code> pour le parall\u00e9lisme sur une seule machine. Cela permet d'entra\u00eener la plupart des estimateurs (ceux qui acceptent un param\u00e8tre <code>n_jobs</code>) en utilisant tous les c\u0153urs de ton ordinateur portable ou de ta station de travail. Mais avec Dask, tu peux aller plus loin.</p> <p>Dask te permet de parall\u00e9liser ton code Scikit-learn sur un cluster sans avoir \u00e0 modifier significativement ton code existant. Par exemple, des fonctions comme GridSearchCV ou RandomizedSearchCV, tr\u00e8s utilis\u00e9es pour la recherche d\u2019hyperparam\u00e8tres, peuvent \u00eatre parall\u00e9lis\u00e9es avec Dask. R\u00e9sultat : tu gagnes en efficacit\u00e9 tout en gardant le m\u00eame code que celui que tu ex\u00e9cuterais localement.</p> <p>Voici un exemple avec XGBoost et Dask : <pre><code>import dask_xgboost as dxgb\nimport dask.dataframe as dd\nfrom dask_ml.model_selection import train_test_split\n\nparams = {'objective': 'binary:logistic', 'max_depth': 4, \n          'eta': 0.01, 'subsample': 0.5, 'min_child_weight': 0.5}\n\ndf = dd.read_csv('data*.csv')\n\nX = df.drop('target', axis=1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nbst = dxgb.train(client, params, X_train, y_train, num_boost_round=10)\n\ny_hat = dxgb.predict(client, bst, X_test).persist()\n</code></pre></p> <p>Ce qui est int\u00e9ressant, c\u2019est que tout ce code peut tourner sur un cluster distribu\u00e9, ce qui booste la vitesse et la performance lorsque tu traites de gros datasets. Plus besoin de se battre contre les limites d'une seule machine.</p> <p>Dask-ML offre aussi des extensions pour distribuer les mod\u00e8les sur plusieurs machines. Que tu utilises Scikit-learn, TensorFlow, ou XGBoost, tu peux profiter de cette int\u00e9gration fluide pour acc\u00e9l\u00e9rer ton travail de data science. La flexibilit\u00e9 est vraiment l\u2019atout majeur de Dask, surtout si tu \u00e9volues d\u00e9j\u00e0 dans un environnement Python.</p> <p>Pour plus de d\u00e9tails et d'exemples sur l'utilisation de Dask avec le machine learning, consulte le guide officiel : Dask XGBoost Example.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#conclusion","title":"Conclusion","text":"<p>En conclusion, Dask est un excellent choix pour les entreprises souhaitant int\u00e9grer le calcul distribu\u00e9 dans leur workflow de data science, tout en restant dans un \u00e9cosyst\u00e8me Python. Pour celles qui ne font pas du v\u00e9ritable Big Data et qui n'ont pas besoin de l'infrastructure massive d'un cluster Spark, Dask repr\u00e9sente une solution plus l\u00e9g\u00e8re, facile \u00e0 d\u00e9ployer, et suffisamment puissante pour traiter de grandes quantit\u00e9s de donn\u00e9es dans un environnement distribu\u00e9.</p>"},{"location":"2024/09/28/faites-du-dask-plut%C3%B4t-que-du-spark-si-vous-avez-juste-de-grosses-tables-de-donn%C3%A9es/#references","title":"R\u00e9f\u00e9rences","text":"<ul> <li>Dask Overview</li> <li>Parallel Computing with Dask</li> <li>Dask XGBoost Example</li> <li>From Pandas to Dask</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/","title":"Encodage efficace des variables cat\u00e9gorielles pour du ML","text":"<p>Les variables cat\u00e9gorielles, on les croise partout dans nos datasets, mais les algorithmes de machine learning, eux, pr\u00e9f\u00e8rent les chiffres. Dans ce billet, nous allons explorer plusieurs techniques d'encodage pour transformer ces variables, le tout agr\u00e9ment\u00e9 d'explications claires, de formulations math\u00e9matiques, et quelques exemples pratiques. Nous aborderons aussi les avantages et les limites de chaque technique, que ce soit les \"Classic Encoders\", le \"Contrast Encoder\", ou les \"Bayesian Encoders\".</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#prerequis","title":"Pr\u00e9requis","text":"<p>Assurez-vous d'avoir pandas, scikit-learn, et category_encoders install\u00e9s.</p> <p>Pour illustrer nos exemples, voici un petit jeu de donn\u00e9es :</p> x1 x2 x3 x4 y 5.6 3.4 1.5 B 0 7.8 2.7 6.9 A 1 4.9 3.1 1.5 A 0 6.4 3.2 5.3 C 1 5.1 3.8 1.6 A 0 6.0 2.9 4.5 B 0 6.5 3.0 5.8 C 1 5.3 3.7 1.5 A 0 7.1 3.0 5.9 B 0 5.9 3.0 5.1 C 1 <p>Pour mod\u00e9liser cette table de donn\u00e9es, il nous faut transformer la colonne x4 en variable num\u00e9rique. En gros, nous allons discuter des diff\u00e9rentes approches qui s'offrent \u00e0 nous.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#i-classic-encoders","title":"I. Classic Encoders","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-label-encoding","title":"1. Label Encoding","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description","title":"Description","text":"<p>L'encodage par \u00e9tiquette attribue un entier unique \u00e0 chaque cat\u00e9gorie d'une variable cat\u00e9gorielle. Cependant, attention ! Cette m\u00e9thode peut insuffler une notion d'ordre qui pourrait \u00eatre inappropri\u00e9e pour des donn\u00e9es purement nominales.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement","title":"Math\u00e9matiquement","text":"<p>Pour des cat\u00e9gories \\(C_1, C_2, \\ldots, C_n\\), l'encodage se fait comme suit : $$ \\text{Valeur Encod\u00e9e} = \\text{index}(C_i) \\quad \\text{pour} \\; i = 1, 2, \\ldots, n $$ o\u00f9 \\(\\text{index}(C_i)\\) repr\u00e9sente un entier unique associ\u00e9 \u00e0 chaque cat\u00e9gorie.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement","title":"Pratiquement","text":"<p>Voici un petit extrait de code :</p> <p><pre><code>from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndata[\"x4_LabelEncoder\"] = encoder.fit_transform(data[\"x4\"])\ndata.head()\n</code></pre> </p> <p>N\u2019oublions pas l\u2019inconv\u00e9nient : cela peut introduire des valeurs qui n\u2019ont pas de sens statistique.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-ordinal-encoder","title":"2. Ordinal Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_1","title":"Description","text":"<p>Il arrive que certaines cat\u00e9gories aient un sens d'ordre. Dans ce cas, un Label Encoder ne sera pas tr\u00e8s utile et pourrait m\u00eame causer des dommages dans les donn\u00e9es. L'encodage ordinal attribue aussi un entier unique \u00e0 chaque cat\u00e9gorie, mais cela se fait lorsque les cat\u00e9gories ont un ordre naturel. Pensez \u00e0 des cat\u00e9gories telles que faible, moyen, et \u00e9lev\u00e9 ; cet ordre doit \u00eatre respect\u00e9.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_1","title":"Math\u00e9matiquement","text":"<p>Pour des cat\u00e9gories ordonn\u00e9es \\(C_1, C_2, \\ldots, C_n\\) o\u00f9 l'ordre naturel est \\(C_1 &lt; C_2 &lt; \\ldots &lt; C_n\\), l'encodage ordinal se fait par :  $$  \\text{Valeur Encod\u00e9e} = \\text{position}(C_i) \\quad \\text{pour} \\; i = 1, 2, \\ldots, n  $$ </p> <p>o\u00f9 : </p> <ul> <li>\\(\\text{position}(C_i)\\) repr\u00e9sente la position ordinale de la cat\u00e9gorie \\(C_i\\) dans l'ordre naturel. Si \\(C_1\\) est la premi\u00e8re, alors \\(\\text{position}(C_1) = 1\\) et ainsi de suite.</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_1","title":"Pratiquement","text":"<p>Pour une variable cat\u00e9gorielle x4 repr\u00e9sentant \"Niveau de risque\", avec les cat\u00e9gories suivantes :</p> <ul> <li>C \u2192 Faible </li> <li>B \u2192 Moyen </li> <li>A \u2192 \u00c9lev\u00e9 </li> </ul> <p>Si l'ordre naturel est Faible &lt; Moyen &lt; \u00c9lev\u00e9, alors l'encodage ordinal sera : </p> <ul> <li>C \u2192 0 </li> <li>B \u2192 1 </li> <li>A \u2192 2</li> </ul> <p><pre><code>from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories=[['C', 'B', 'A']])\ndata[\"x4_OrdinalEncoder\"] = encoder.fit_transform(data[[\"x4\"]])\ndata.head()\n</code></pre> </p> <p>Cette m\u00e9thode pr\u00e9serve l'ordre des cat\u00e9gories, crucial pour certaines analyses statistiques. En revanche, il faut s'assurer que cet ordre est bien d\u00e9fini dans le code <code>OrdinalEncoder(categories=[['C', 'B', 'A']])</code>.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#3-one-hot-encoder","title":"3. One-Hot Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_2","title":"Description","text":"<p>Ce proc\u00e9d\u00e9 cr\u00e9e des colonnes binaires, ou indicatrices, pour chaque cat\u00e9gorie. Pour chaque observation, la colonne correspondant \u00e0 la cat\u00e9gorie pr\u00e9sente prend la valeur 1, et les autres sont \u00e0 0.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_2","title":"Math\u00e9matiquement","text":"<p>Soit \\(C = \\{ C_1, C_2, ..., C_n \\}\\) les cat\u00e9gories d'une variable. Une observation appartenant \u00e0 \\(C_i\\) est repr\u00e9sent\u00e9e par : $$ \\mathbf{x} = [0, 0, \\ldots, 1, \\ldots, 0] \\quad \\text{o\u00f9 la position } i \\text{ est \u00e0 1} $$</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_2","title":"Pratiquement","text":"<p>Il existe plusieurs outils, mais restons simples avec la m\u00e9thode <code>get_dummies</code> de pandas, que je trouve bien pratique.</p> <pre><code>import pandas as pd\ndata = pd.get_dummies(data, columns=[\"x4\"], drop_first=False, prefix=\"x4_OneHotEncoder\", dtype=int)\ndata.head()\n</code></pre> <p></p> <p>On peut jouer avec pas mal de param\u00e8tres. Pour les mod\u00e8les statistiques, on a souvent tendance \u00e0 fixer <code>drop_first=True</code> afin d'\u00e9viter le probl\u00e8me de colin\u00e9arit\u00e9 parfaite. Vous l'avez vu, on a transform\u00e9 la variable x4 en plusieurs nouvelles caract\u00e9ristiques. Cela peut poser probl\u00e8me si on a un grand nombre de cat\u00e9gories, ce qui pourrait mener \u00e0 des matrices creuses. Dans une situation de ML training, cela peut entra\u00eener du surapprentissage. Parfois, une s\u00e9lection de caract\u00e9ristiques devient alors in\u00e9vitable.</p> <p>https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#autres-encodeurs","title":"Autres encodeurs","text":"<p>Je vous encourage aussi \u00e0 jeter un \u0153il sur deux encodeurs int\u00e9ressants : - Hashing encoder  - Count encoder</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#ii-contrast-encoder","title":"II. Contrast Encoder","text":"<p>Les encodeurs de contraste transforment les variables cat\u00e9gorielles en format num\u00e9rique en cr\u00e9ant des codes de contraste qui permettent aux algorithmes d'interpr\u00e9ter efficacement les resultats des mod\u00e8les de regression. Voici quelques m\u00e9thodes courantes pour encoder les contrastes :</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-sum-encoder","title":"1. Sum Encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_3","title":"Description","text":"<p>Cette m\u00e9thode encode les variables de mani\u00e8re \u00e0 ce que la somme des vecteurs encod\u00e9s soit \u00e9gale \u00e0 z\u00e9ro, \u00e9vitant ainsi la multicolin\u00e9arit\u00e9. Dans un mod\u00e8le One Hot Encoding, on doit supprimer une cat\u00e9gorie et la garder comme r\u00e9f\u00e9rence. Ainsi :</p> <ul> <li>Dans ce mod\u00e8le, l'intercept repr\u00e9sente la moyenne de la condition de r\u00e9f\u00e9rence.</li> </ul> <ul> <li>Les coefficients repr\u00e9sentent les effets simples, c'est-\u00e0-dire la diff\u00e9rence entre une condition particuli\u00e8re et la condition de r\u00e9f\u00e9rence.</li> </ul> <p>Cela n'est pas toujours du go\u00fbt des statisticiens ! Ils ont donc introduit l'encodage par somme. Dans les mod\u00e8les de r\u00e9gression :</p> <ul> <li>L'intercept repr\u00e9sente la moyenne g\u00e9n\u00e9rale du target \u00e0 travers toutes les conditions.</li> </ul> <ul> <li>Les coefficients des cat\u00e9gories sont alors interpr\u00e9t\u00e9s comme la variation de la moyenne du target pour chaque cat\u00e9gorie par rapport \u00e0 cette moyenne g\u00e9n\u00e9rale.</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_3","title":"Math\u00e9matiquement","text":"<p>Pour des cat\u00e9gories \\(C = \\{ C_1, C_2, \\ldots, C_n \\}\\), si nous choisissons \\(C_k\\) comme cat\u00e9gorie de r\u00e9f\u00e9rence, une observation appartenant \u00e0 \\(C_i\\) (o\u00f9 $ i \\neq k $) se repr\u00e9sente par :</p> \\[ \\mathbf{x} = \\begin{bmatrix} 1 &amp; 0 &amp; \\ldots &amp; -1 &amp; -1 \\end{bmatrix} \\] <p>o\u00f9 les valeurs sont : -  \\(1\\) pour la cat\u00e9gorie \\(C_1\\)  -  \\(1\\) pour la cat\u00e9gorie \\(C_2\\)  -  \\(-1\\) pour la cat\u00e9gorie de r\u00e9f\u00e9rence \\(C_k\\)  -  \\(0\\) pour les autres cat\u00e9gories.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_3","title":"Pratiquement","text":"<p>Pour appliquer l'encodage Sum avec Pandas, on pourrait le faire directement, mais je vous conseille d'utiliser le package category_encoders, notamment la classe SumEncoder.</p> <p><pre><code>from category_encoders.sum_coding import SumEncoder\nSE_encoder = SumEncoder(drop_invariant=True)\nSE_encoder.fit_transform(data).head()\n</code></pre> </p> <p>Premi\u00e8re remarque : il n\u2019y a pas de cat\u00e9gorie de r\u00e9f\u00e9rence, car par d\u00e9faut, c\u2019est la derni\u00e8re par ordre alphab\u00e9tique. On ne peut pas choisir la cat\u00e9gorie de r\u00e9f\u00e9rence directement ici, mais une fois que l\u2019on a compris le principe, on peut s\u2019en charger par nous-m\u00eames. Pour obtenir les coefficients de la cat\u00e9gorie de r\u00e9f\u00e9rence, il suffit de prendre -1 et -1 pour <code>x4_0</code> et <code>x4_1</code>.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-helmert-coding","title":"2. Helmert Coding","text":"<p>Pour plus de d\u00e9tails, consultez ce lien.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#iii-bayesian-target-encoders","title":"III. Bayesian Target Encoders","text":"<p>Les methodes class\u00e9es comme Bayesiennes  sont des  technique utile pour encoder les variables cat\u00e9gorielles en tenant compte de la distribution du target. Cette approche int\u00e8gre des informations a priori sur la variable cible, ce qui la rend particuli\u00e8rement efficace pour am\u00e9liorer la performance des mod\u00e8les d'apprentissage automatique. Leurs Caract\u00e9ristiques cl\u00e9s sont les suivantes.</p> <ol> <li> <p>Cadre Bay\u00e9sien : Cette m\u00e9thode recourt \u00e0 l\u2019approche bay\u00e9sienne pour estimer la moyenne du target pour chaque cat\u00e9gorie tout en consid\u00e9rant les informations provenant de l'ensemble de donn\u00e9es global. Cela aide \u00e0 att\u00e9nuer les soucis li\u00e9s au surajustement, surtout quand les cat\u00e9gories ont peu d'observations.</p> </li> <li> <p>R\u00e9duction (Shrinkage) : L'encodage cible bay\u00e9sien applique une technique de r\u00e9duction, o\u00f9 la moyenne de la cat\u00e9gorie est ajust\u00e9e vers la moyenne g\u00e9n\u00e9rale du target, rendant l'encoding plus robuste.</p> </li> <li> <p>Gestion des Donn\u00e9es Manquantes : Cette m\u00e9thode s\u2019accommode bien des donn\u00e9es manquantes dans la caract\u00e9ristique cat\u00e9gorique en fournissant une estimation significative bas\u00e9e sur les donn\u00e9es accessibles.</p> </li> <li> <p>Cas d'Utilisation : L'encoding cible bay\u00e9sien est particuli\u00e8rement pr\u00e9conis\u00e9 pour les variables cat\u00e9gorielles \u00e0 forte cardinalit\u00e9, o\u00f9 l'encoding one-hot entra\u00eenerait trop de caract\u00e9ristiques.</p> </li> </ol> <p>Avant d\u2019explorer les formules, voici quelques notations cruciales :</p> <ul> <li>\\(y\\) et \\(y^+\\) : Le nombre total d'observations et le nombre total d'observations positives (o\u00f9 \\(y = 1\\)).</li> <li>\\(x_i, y_i\\) : La valeur de la cat\u00e9gorie et du target pour l'observation $ i $.</li> <li>\\(n\\) et \\(n^+\\) : Le nombre d'observations et le nombre d'observations positives pour une valeur sp\u00e9cifique d'une colonne cat\u00e9gorielle.</li> <li>\\(a\\) : Un hyperparam\u00e8tre de r\u00e9gularisation.</li> <li>\\(prior\\) : La valeur moyenne du target sur l'ensemble du dataset.</li> <li>\\(x^k_i\\) est la valeur encod\u00e9e pour l'observation \\(i\\) de la cat\u00e9gorie \\(k\\)</li> </ul>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#1-target-encoder","title":"1. Target encoder","text":"<p>Le target encoder est une technique de transformation de variables cat\u00e9gorielles fond\u00e9e sur la variable cible, souvent utilis\u00e9e dans les mod\u00e8les de machine learning supervis\u00e9. L'id\u00e9e est de remplacer chaque cat\u00e9gorie par une valeur calcul\u00e9e \u00e0 partir de la moyenne du target, avec un m\u00e9canisme de lissage pour pr\u00e9venir le surajustement.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_4","title":"Math\u00e9matiquement","text":"<ol> <li> <p>Calcul du Param\u00e8tre de lissage (\\(s\\))</p> <p>Le param\u00e8tre de lissage est utilis\u00e9 pour \u00e9quilibrer la contribution entre la moyenne g\u00e9n\u00e9rale (prior) et la moyenne par cat\u00e9gorie :  $$  s = \\frac{1}{1 + \\exp\\left(-\\frac{n - mdl}{a}\\right)}  $$</p> <p>o\u00f9 :     - \\(mdl\\) est la valeur minimale de donn\u00e9es par feuille,</p> </li> <li> <p>Calcul de la valeur encod\u00e9e ($ \\hat{x}^k $)</p> <p>La valeur encod\u00e9e pour chaque cat\u00e9gorie $ k $ est donn\u00e9e par :  $$  \\hat{x}^k = prior \\cdot (1 - s) + s \\cdot \\frac{n^+}{n}  $$</p> <p>o\u00f9 :     - \\(s\\) est le param\u00e8tre de lissage calcul\u00e9,     - \\(\\frac{n^+}{n}\\) est la moyenne des cibles positives pour la cat\u00e9gorie \\(k\\).</p> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_4","title":"Pratiquement","text":"<p>On utilisera encore le package category_encoders, avec les valeurs par d\u00e9faut : </p> <p><pre><code>from category_encoders import TargetEncoder\nencoder = TargetEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p> Avantages Inconv\u00e9nient Capture la relation avec le target : Directement int\u00e9gr\u00e9e, permettant d'am\u00e9liorer la performance. Surajustement potentiel : Surtout pour les cat\u00e9gories avec peu de donn\u00e9es. R\u00e9duit la dimensionnalit\u00e9 : \u00c9vite une explosion de dimensions compar\u00e9 \u00e0 l'encoding one-hot. Le m\u00e9canisme de r\u00e9gularisation aide, mais n\u00e9cessite un bon ajustement des hyperparam\u00e8tres pour \u00e9viter le surapprentissage. G\u00e8re les cat\u00e9gories rares : Le lissage minimise le risque de surajustement pour les valeurs peu fr\u00e9quentes. Facile \u00e0 interpr\u00e9ter : Les valeurs encod\u00e9es refl\u00e8tent des probabilit\u00e9s moyennes pond\u00e9r\u00e9es, simplifiant l'analyse."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#2-m-estimate-coding","title":"2. M-Estimate coding","text":"<p>M-Estimate encoder est une version simplifi\u00e9e du target encoder qui a un seul param\u00e8tre de lissage, ce qui facilite sa mise en place et son ajustement. Con\u00e7u pour estimer la probabilit\u00e9 d'appartenance \u00e0 une cat\u00e9gorie en utilisant une moyenne pond\u00e9r\u00e9e.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_4","title":"Description","text":"<p>M-Estimate coding est une technique simplifi\u00e9e qui utilise un seul param\u00e8tre de lissage, facilitant son adaptation. Il est destin\u00e9 \u00e0 estimer la probabilit\u00e9 d'appartenance \u00e0 une cat\u00e9gorie en s'appuyant sur une moyenne pond\u00e9r\u00e9e.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_5","title":"Math\u00e9matiquement","text":"<p>La formule de  M-Estimate coding est :</p> \\[ \\hat{x}^k = \\frac{n^+ + \\text{prior} \\cdot m}{n + m} \\] <p>o\u00f9 :    - \\(m\\) : param\u00e8tre de lissage.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_5","title":"Pratiquement","text":"<p>Voici comment on peut impl\u00e9menter ce type d'encoding en Python :</p> <p><pre><code>from category_encoders import MEstimateEncoder\nencoder = MEstimateEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p> Avantages Inconv\u00e9nients Simple et efficace : Un seul param\u00e8tre de lissage \u00e0 ajuster. R\u00e9gularisation limit\u00e9e : Moins flexible que le target encoder classique. R\u00e9duction du surajustement : Le param\u00e8tre $ m $ stabilise les valeurs, r\u00e9duisant l'impact des cat\u00e9gories rares. Pas id\u00e9al pour les cibles cat\u00e9gorielles multiples : Un wrapper polynomial est n\u00e9cessaire, complexifiant la m\u00e9thode. Performance \u00e9lev\u00e9e : Pratique \u00e0 impl\u00e9menter et efficace pour les cibles binaires et continues."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#3-leave-one-out-encoder","title":"3. Leave-One-Out encoder","text":"<p>Le Leave-One-Out encoder (LOO) est une autre m\u00e9thode tir\u00e9e de target encoder, mais avec une variation importante pour minimiser la fuite d'information.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_5","title":"Description","text":"<p>L'id\u00e9e est de calculer la moyenne du target pour chaque cat\u00e9gorie, mais sans inclure l'observation actuelle. Cela aide \u00e0 limiter la fuite d'information puisque la valeur cible de l'observation en cours n'est pas int\u00e9gr\u00e9e dans sa propre transformation.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_6","title":"Math\u00e9matiquement","text":"<p>Le calcul se fait en deux etapes.</p> <ol> <li> <p>Calcul de la moyenne du target en excluant l'observation actuelle</p> <p>Pour chaque observation $ i $ appartenant \u00e0 la cat\u00e9gorie $ k $, la moyenne est calcul\u00e9e sans l\u2019observation en cours par :  $$  x^k_i = \\frac{\\sum_{j \\neq i} (y_j \\cdot (x_j == k)) - y_i}{\\sum_{j \\neq i} (x_j == k)}  $$</p> <p>En excluant $ y_i $, on \u00e9vite que le mod\u00e8le \"voit\" sa propre valeur cible, ce qui r\u00e9duit le risque de surapprentissage.</p> </li> <li> <p>Encodage des donn\u00e9es de test</p> <p>Pour les donn\u00e9es de test, chaque cat\u00e9gorie est remplac\u00e9e par la moyenne du target calcul\u00e9e sur l'ensemble des donn\u00e9es d'entra\u00eenement :  $$  x^k = \\frac{\\sum y_j \\cdot (x_j == k)}{\\sum (x_j == k)}  $$</p> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_6","title":"Pratiquement","text":"<p><pre><code>import pandas as pd\nfrom category_encoders import LeaveOneOutEncoder\nencoder = LeaveOneOutEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"])\n</code></pre> </p> <p>D\u00e9composons le calcul pour chaque observation dans la colonne <code>x4</code> :</p> <p>Nous allons expliquer les calculs dans le tableau ci-dessous pour les trois premi\u00e8res lignes : Calculer la moyenne du target pour chaque cat\u00e9gorie en excluant l'observation actuelle. La valeur trouv\u00e9e est la nouvelle valeur.</p> Index Cat\u00e9gorie Cibles (sans l'observation actuelle) Moyenne du target 0 B [0, 0] \\(\\frac{0 + 0}{2} = 0\\) 1 A [0, 0, 0] \\(\\frac{0 + 0 + 0}{3} = 0\\) 2 A [1, 0, 0] \\(\\frac{1 + 0 + 0}{3} \\approx 0.33\\) <p>Si tu as besoin de modifications ou d'ajouts, n'h\u00e9site pas \u00e0 me le faire savoir ! Chaque observation est maintenant encod\u00e9e avec la moyenne des cibles des autres observations de la m\u00eame cat\u00e9gorie.</p> Avantages Inconv\u00e9nients R\u00e9duction de la fuite d'information : Sa m\u00e9thode minimise les risques de biais. Complexit\u00e9 : Peut \u00eatre co\u00fbteux en calculs sur de grands ensembles de donn\u00e9es. Capture des relations complexes : Comme target encoder, utile pour des relations non lin\u00e9aires. Variabilit\u00e9 : Peut introduire de la variance avec de petites cat\u00e9gories, n\u00e9cessitant une r\u00e9gularisation suppl\u00e9mentaire."},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#5-james-stein-encoder","title":"5. James-Stein encoder","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_6","title":"Description","text":"<p>James-Stein encoder est  bas\u00e9 sur le target. Son id\u00e9e fondatrice est d'estimer la moyenne du target pour une cat\u00e9gorie donn\u00e9e \\(k\\) selon la formule suivante :</p> \\[ JS_i = (1-B) \\cdot \\text{mean}(y_i) + B \\cdot \\text{mean}(y) \\] <p>o\u00f9 :     - \\(JS_i\\) est l\u2019estimation pour la cat\u00e9gorie \\(C_i\\)</p> <p>- \\(\\text{mean}(y_i)\\) est la moyenne des valeurs cibles pour la cat\u00e9gorie \\(C_i\\)</p> <p>- \\(\\text{mean}(y)\\) est la moyenne g\u00e9n\u00e9rale des cibles</p> <p>- \\(B\\) est un poids calcul\u00e9 qui \u00e9quilibre l\u2019influence de la moyenne conditionnelle et de la moyenne globale.</p> <p>Cela semble tr\u00e8s sens\u00e9. Nous cherchons une estimation qui se situe entre la moyenne de l'\u00e9chantillon (risquant d'\u00eatre extr\u00eame) et la moyenne globale.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_7","title":"Math\u00e9matiquement","text":"<p>Le poids $ B $ est d\u00e9fini par :</p> \\[ B = \\frac{\\text{var}(y_i)}{\\text{var}(y_i) + \\text{var}(y)} \\] <p>On se demande quel devrait \u00eatre ce poids. Si on accorde trop de poids \u00e0 la moyenne conditionnelle, on risque le surajustement, tandis qu'en privil\u00e9giant la moyenne globale, on peut sous-ajuster. Une approche canonique en apprentissage machine serait de passer par une validation crois\u00e9e. Cependant, Charles Stein a propos\u00e9 une solution en forme ferm\u00e9e. L'id\u00e9e : ajuster la qualit\u00e9 des estimations selon la variance.</p> <p>Cet estimateur est limit\u00e9 aux distributions normales, ce qui ne convient pas \u00e0 toutes les t\u00e2ches de classification. Ainsi, on retrouve :</p> \\[ SE^2 = \\frac{\\text{var}(y)}{\\text{count}(y)} \\] <p>Un d\u00e9fi majeur est que nous ne connaissons pas \\(\\text{var}(y)\\). Il nous faudra donc estimer ces variances. Voici quelques solutions :</p> <ol> <li> <p>Mod\u00e8le Pooled : Si toutes les observations sont semblables et prennent un nombre commun d'observations pour chaque valeur.</p> </li> <li> <p>Mod\u00e8le Ind\u00e9pendant : Si les comptes d'observation diff\u00e8rent, il est plus judicieux de remplacer les variances par des erreurs standard, p\u00e9nalisant ainsi les petites observations :</p> </li> </ol> \\[ SE^2 = \\frac{\\text{var}(y)}{\\text{count}(y)} \\]"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#application-pour-la-classification-binaire","title":"Application pour la classification binaire","text":"<p>Cet estimateur a une limitation pratique dans les mod\u00e8les de classification binaire, o\u00f9 les cibles ne sont que \\(0\\) ou \\(1\\). Pour l'appliquer, on doit convertir lamoyenne du target dans l'intervalle born\u00e9 \\(&lt;0,1&gt;\\) en rempla\u00e7ant \\(\\text{mean}(y)\\) par le ratio des cotes logarithmique :</p> \\[ \\text{log-odds\\_ratio}_i = \\log\\left(\\frac{\\text{mean}(y_i)}{\\text{mean}(y_{\\text{not} \\, i})}\\right) \\] <p>Cela s'appelle mod\u00e8le binaire. C\u2019est d\u00e9licat d'estimer les param\u00e8tres de ce mod\u00e8le, et parfois cela \u00e9choue. Il est souvent plus judicieux de recourir \u00e0 un mod\u00e8le b\u00eata, souvent plus stable malgr\u00e9 une pr\u00e9cision l\u00e9g\u00e8rement inf\u00e9rieure.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_7","title":"Pratiquement","text":"<p>Pour utiliser l'encodeur James-Stein, allez-y avec la classe <code>JamesSteinEncoder</code> de <code>category_encoders</code> :</p> <p><pre><code>import pandas as pd\nfrom category_encoders import JamesSteinEncoder\n\nencoder = JamesSteinEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"])\n</code></pre> </p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#4-catboost-encoding","title":"4. CatBoost Encoding","text":""},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#description_7","title":"Description","text":"<p>Il s'agit  d'une m\u00e9thode d'encodage bas\u00e9e sur la cible, d\u00e9velopp\u00e9e \u00e0 l'origine pour \u00eatre utilis\u00e9e avec l'algorithme CatBoost, mais qui est applicable \u00e0 d'autres mod\u00e8les. Cet encodeur utilise une m\u00e9thode particuli\u00e8re pour \u00e9viter la fuite d'information tout en exploitant les relations entre les cat\u00e9gories et ld target y.</p> <p>L'id\u00e9e principale est d'utiliser les informations du target de mani\u00e8re ordonn\u00e9e. Plut\u00f4t que de d\u00e9terminer la moyenne du target pour chaque cat\u00e9gorie sur l'ensemble des donn\u00e9es (qui peut introduire des fuites), CatBoost effectue une mise \u00e0 jour de l'encodage de mani\u00e8re s\u00e9quentielle.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#mathematiquement_8","title":"Math\u00e9matiquement","text":"<p>Le calcul se fait en deux etapes.</p> <ol> <li> <p>Ordre des observations</p> <ul> <li>L'algorithme parcourt les donn\u00e9es de mani\u00e8re ordonn\u00e9e.</li> <li>L'encodage pour chaque observation est bas\u00e9 sur les informations des observations pr\u00e9c\u00e9dentes seulement, emp\u00eachant ainsi la valeur du target actuelle d'affecter son propre encodage.</li> </ul> </li> <li> <p>Calcul progressif de la moyenne du target</p> <ul> <li>Pour chaque observation \\(i\\) dans la cat\u00e9gorie $ k $, la moyenne du target est calcul\u00e9e avec les observations pr\u00e9c\u00e9dentes. La formule est :  $$  x^k_i = \\frac{\\sum_{j &lt; i} (y_j \\cdot (x_j == k)) + \\text{prior} \\cdot \\alpha}{\\sum_{j &lt; i} (x_j == k) + \\alpha}  $$</li> </ul> </li> <li> <p>Encodage des donn\u00e9es de test</p> <ul> <li>Pour les donn\u00e9es de test, l'encodage est bas\u00e9 sur les moyennes calcul\u00e9es \u00e0 partir des donn\u00e9es d'entra\u00eenement, sans fuite d'information.</li> </ul> </li> </ol>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pourquoi-catboost-encoder-est-il-efficace","title":"Pourquoi CatBoost encoder est-il Efficace ?","text":"<p>CatBoost encoder r\u00e9duit efficacement la fuite d'information gr\u00e2ce \u00e0 sa m\u00e9thode de calcul s\u00e9quentiel. Voici quelques atouts : - S\u00e9quentiel et Progressif : En n'utilisant que les observations pr\u00e9c\u00e9dentes, il \u00e9vite que la valeur actuelle influence son encodage. - R\u00e9gularisation : L'ajout d'un terme de r\u00e9gularisation permet de contr\u00f4ler la variance.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#pratiquement_8","title":"Pratiquement","text":"<p><pre><code>from category_encoders import CatBoostEncoder\nencoder = CatBoostEncoder()\nencoder.fit_transform(data.drop(columns=[\"y\"]), data[\"y\"]).head()\n</code></pre> </p> <p>Et voil\u00e0 ! Pour appliquer l'encodeur CatBoost \u00e0 la variable cat\u00e9gorielle <code>x4</code>, nous avons vu le calcul \u00e9tape par \u00e9tape. </p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#conclusion","title":"Conclusion","text":"<p>Le choix de la meilleure m\u00e9thode d\u00e9pendra de votre cas d'utilisation et de la cardinalit\u00e9 des cat\u00e9gories. Est-on \u00e0 la recherche d'un mod\u00e8le explicatif ou pr\u00e9dictif ?  Dans le billet de blog de la semaine prochaine, je vais essayer de mesurer les performances de ces m\u00e9thodes avec un mod\u00e8le simple et voir qui s\u2019en sort le mieux.</p>"},{"location":"2024/10/05/encodage-efficace-des-variables-cat%C3%A9gorielles-pour-du-ml/#references","title":"R\u00e9f\u00e9rences","text":"<p>doc officiel de category_encoder</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/","title":"Auto ML et Interpr\u00e9tabilit\u00e9 avec FLAML et SHAP","text":"<p>En machine learning, construire des mod\u00e8les performants ne suffit plus : l'interpr\u00e9tabilit\u00e9 est devenue indispensable, surtout dans des domaines r\u00e9glement\u00e9s. Comprendre les d\u00e9cisions de nos mod\u00e8les est essentiel pour une IA de confiance, un des axes majeurs de l'IA Act de l'Union Europ\u00e9enne.</p> <p>Cependant, allier performance et transparence prend du temps, surtout lorsque l\u2019on doit optimiser les hyperparam\u00e8tres et choisir les bons algorithmes. C\u2019est l\u00e0 que FLAML (Fast Lightweight AutoML) entre en sc\u00e8ne pour rendre ce processus fluide en automatisant la recherche de mod\u00e8les et de param\u00e8tres optimaux.</p> <p>Dans cet article, explorons comment FLAML permet de concilier efficacit\u00e9 et interpr\u00e9tabilit\u00e9, en int\u00e9grant les valeurs SHAP (SHapley Additive exPlanations). Cette combinaison rend nos mod\u00e8les plus compr\u00e9hensibles, sans compromis sur la performance.</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#exigences","title":"Exigences","text":"<p>Avant d'explorer FLAML, assurez-vous d'avoir toutes les biblioth\u00e8ques n\u00e9cessaires. Pas de panique, voici la commande pour tout installer d'un coup :</p> <pre><code>pip install flaml[automl]==2.3.3 plotly==5.24.1 loguru==0.7.2 category_encoders==2.6.4 shap matplotlib\n</code></pre> <p>Avec tout \u00e7a, nous sommes pr\u00eats \u00e0 d\u00e9marrer notre aventure avec FLAML et SHAP !</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#1-presentation-de-flaml","title":"1. Pr\u00e9sentation de FLAML","text":"<p>FLAML est une biblioth\u00e8que AutoML d\u00e9velopp\u00e9e par Microsoft, con\u00e7ue pour vous faire gagner un temps fou dans la configuration de vos mod\u00e8les de machine learning. Son interface est tellement intuitive qu'elle permet d\u2019optimiser rapidement les hyperparam\u00e8tres tout en restant flexible pour diff\u00e9rents types de probl\u00e8mes.</p> <p>Les Caract\u00e9ristiques Principales de FLAML:</p> <ul> <li>Optimisation Automatique des Hyperparam\u00e8tres : Fini les essais exhaustifs ! FLAML d\u00e9ploie des algorithmes de recherche avanc\u00e9s pour trouver les meilleurs hyperparam\u00e8tres sans vous faire perdre de temps.</li> <li>Adaptabilit\u00e9 : Que ce soit XGBoost ou LightGBM, FLAML supporte de nombreux mod\u00e8les connus et permet m\u00eame d\u2019int\u00e9grer vos propres mod\u00e8les.</li> <li>Support Multi-T\u00e2ches : Que vous soyez dans la classification ou la r\u00e9gression, FLAML s'adapte \u00e0 vos besoins.</li> <li>Efficacit\u00e9 en Temps et Co\u00fbt : Optimis\u00e9 pour minimiser le temps d'ex\u00e9cution et les ressources, c\u2019est l\u2019outil id\u00e9al pour des exp\u00e9rimentations rapides et efficaces.</li> </ul> <p>Avec tout cela, FLAML se pr\u00e9sente comme un alli\u00e9 de choix pour vos projets de machine learning !</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#2-interpretabilite-avec-shap","title":"2. Interpr\u00e9tabilit\u00e9 avec SHAP","text":"<p>Comprendre pourquoi un mod\u00e8le fait certaines pr\u00e9dictions est essentiel, surtout dans les secteurs avec des attentes r\u00e9glementaires. Les valeurs SHAP, bas\u00e9es sur la th\u00e9orie des jeux, offrent une m\u00e9thode robuste pour rendre le processus d\u00e9cisionnel algorithmique bien plus transparent. Elles permettent de d\u00e9cortiquer l'impact de chaque variable, tout en tenant compte des interactions complexes. Dans la jungle des frameworks d'explication, les valeurs SHAP se distinguent par leur reconnaissance et leur logique.</p> <p>Voici quelques atouts des valeurs SHAP :</p> <ul> <li>Fondement Th\u00e9orique : Leur solidit\u00e9 repose sur la th\u00e9orie des jeux, leur procurant une robustesse ind\u00e9niable.</li> <li>Quantification de l'Impact : Chaque valeur SHAP r\u00e9v\u00e8le comment une variable influence la pr\u00e9diction, facilitant la compr\u00e9hension de ce qui affecte le plus votre mod\u00e8le.</li> <li>Visualisations Intuitives : Les graphiques tels que les diagrammes en barres ou en dispersion rendent l\u2019interpr\u00e9tation des r\u00e9sultats accessible, m\u00eame pour les non-experts.</li> </ul> <p>En int\u00e9grant SHAP, vos mod\u00e8les deviennent presque transparents, offrant une vision claire sur vos donn\u00e9es et vos mod\u00e8les.</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#3-automl-en-un-coup-dil","title":"3. AutoML en Un Coup d'\u0152il","text":"<p>Pour illustrer ce que FLAML peut faire, voyons un exemple simple utilisant FLAML pour pr\u00e9dire les prix des maisons en Californie :</p> <pre><code>from flaml.ml import sklearn_metric_loss_score\nimport pandas as pd\nfrom flaml import AutoML\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport logging\n\nlogger = logging.getLogger()\n\nautoml = AutoML()\nsettings = {\n    \"time_budget\": 20,  # Temps total d'ex\u00e9cution en secondes\n    \"metric\": 'r2',  # M\u00e9trique principale pour la r\u00e9gression\n    \"estimator_list\": ['lgbm'],  # S\u00e9lection de mod\u00e8les ML\n    \"task\": 'regression',  # Type de t\u00e2che    \n    \"log_file_name\": 'houses_experiment.log',  # Log de FLAML\n    \"seed\": 2024,    \n}\n\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(data.data), data.target, test_size=0.2, random_state=42)\n\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\ny_pred = automl.predict(X_test)\n\ndef calculate_metrics(y_pred, y_test):\n    metrics = {\n        'r2': round(1 - sklearn_metric_loss_score('r2', y_pred, y_test), 4),\n        'mse': round(sklearn_metric_loss_score('mse', y_pred, y_test), 4),\n        'mae': round(sklearn_metric_loss_score('mae', y_pred, y_test), 4)\n    }\n    return metrics\n\nlogger.info(calculate_metrics(y_pred, y_test))\n</code></pre> <p>Dans ce code, le param\u00e8tre <code>time_budget</code> d\u00e9finit le temps allou\u00e9 pour explorer diff\u00e9rentes configurations de mod\u00e8les. Ajustez-le en fonction de vos besoins \u2013 en g\u00e9n\u00e9ral, 180 secondes fonctionnent bien pour les premiers essais.   Comme vous pouvez le voir, la sortie est tr\u00e8s verbeuse, et tant mieux, car cela permet de voir comment le framework a fonctionn\u00e9 pour s\u00e9lectionner les bons hyperparam\u00e8tres. Apr\u00e8s l'ajustement, nous utilisons des m\u00e9triques telles que R\u00b2, MSE et MAE pour \u00e9valuer les performances du mod\u00e8le.</p> <p>Dans ce code, le param\u00e8tre <code>time_budget</code> d\u00e9finit le temps allou\u00e9 pour explorer diff\u00e9rentes configurations de mod\u00e8les. Ajustez-le en fonction de vos besoins \u2013 en g\u00e9n\u00e9ral, 180 secondes fonctionne bien pour les premi\u00e8re it\u00e9rations (dans le cas de donn\u00e9es tabulaires avec un nombre de lignes  inferieur \u00e0 10 millions mlemeavec une centaine de dvariables on est dans la categorie small)</p> <p>En g\u00e9n\u00e9ral, un budget de 180 secondes fonctionne bien pour les premi\u00e8res it\u00e9rations, surtout dans le cas de donn\u00e9es tabulaires contenant moins de 10 millions de lignes. M\u00eame avec une centaine de variables, on reste dans la cat\u00e9gorie \"small\".</p> <p>Le choix de <code>time_budget</code> repose sur l'exp\u00e9rience. J'ai trouv\u00e9 une discussion int\u00e9ressante sur ce sujet : choix de time_budget. Selon les r\u00e9sultats de l'article de FLAML, le temps n\u00e9cessaire pour atteindre ou surpasser les meilleures performances rapport\u00e9es dans le benchmark AutoML peut \u00eatre consid\u00e9rablement r\u00e9duit avec FLAML. Voici les temps recommand\u00e9s pour les diff\u00e9rentes cat\u00e9gories de datasets :</p> <ul> <li>\u2018small\u2019 : 1m - 10m</li> <li>\u2018medium\u2019 : 10m - 1h</li> <li>\u2018large\u2019 : 1h - 4h</li> </ul>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#4-ajouter-un-learner-personnalise","title":"4. Ajouter un Learner Personnalis\u00e9","text":"<p>Un des atouts de FLAML est la possibilit\u00e9 d'int\u00e9grer des learners personnalis\u00e9s, ce qui permet d\u2019ajouter des fonctions de perte sp\u00e9cifiques ou de nouveaux mod\u00e8les. Voici comment proc\u00e9der :</p> <p><pre><code>import numpy as np \n\ndef my_loss_obj(y_true, y_pred):\n    c = 0.5\n    residual = y_pred - y_true\n    grad = c * residual / (np.abs(residual) + c)\n    hess = c ** 2 / (np.abs(residual) + c) ** 2\n\n    # grad et hess pour RMSE\n    grad_rmse = residual\n    hess_rmse = 1.0\n\n    # grad et hess pour MAE\n    grad_mae = np.array(residual)\n    grad_mae[grad_mae &gt; 0] = 1.\n    grad_mae[grad_mae &lt;= 0] = -1.\n    hess_mae = 1.0\n\n    coef = [0.4, 0.3, 0.3]\n    return coef[0] * grad + coef[1] * grad_rmse + coef[2] * grad_mae, \\\n        coef[0] * hess + coef[1] * hess_rmse + coef[2] * hess_mae\n\nfrom flaml.automl.model import LGBMEstimator\n\nclass MyLGBM(LGBMEstimator):\n    def __init__(self, **config):\n        super().__init__(objective=my_loss_obj, **config)\n\nautoml.add_learner(learner_name='my_lgbm', learner_class=MyLGBM)\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\n\nlogger.info(calculate_metrics(y_pred, y_test))\n</code></pre> </p> <p>Ici, la fonction <code>my_loss_obj</code> combine diff\u00e9rentes approches pour minimiser la perte en jouant sur RMSE et MAE. La classe <code>MyLGBM</code> h\u00e9rite d'un estimateur LGBM, tout en int\u00e9grant notre fonction de perte personnalis\u00e9e.</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#5-une-classe-automl-prete-a-lemploi","title":"5. Une Classe AutoML Pr\u00eate \u00e0 l'Emploi","text":"<p>Pour gagner encore plus de temps, construisons une classe qui va rationaliser tout le flux de travail, du pr\u00e9traitement \u00e0 la s\u00e9lection de caract\u00e9ristiques, en passant par le r\u00e9glage des hyperparam\u00e8tres.</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#genericautoml","title":"GenericAutoML","text":"<p>Cette classe va adopter les concepts suivants :</p> <ul> <li>Encodage Cat\u00e9goriel : Offrir diff\u00e9rentes strat\u00e9gies d\u2019encodage pour transformer facilement les variables cat\u00e9gorielles.</li> </ul> <ul> <li>S\u00e9lection de Variables : Utiliser des techniques pour filtrer automatiquement les caract\u00e9ristiques les plus pertinentes.</li> </ul> <ul> <li>Stratification Personnalis\u00e9e : G\u00e9rer la validation crois\u00e9e pour maintenir un \u00e9quilibre de classe si une colonne sp\u00e9cifique est choisie.</li> </ul> <ul> <li>Interpr\u00e9tabilit\u00e9 avec les valeurs SHAP.</li> </ul> <pre><code>from flaml import AutoML\nfrom sklearn.base import BaseEstimator, RegressorMixin\nimport plotly.express as px\nimport shap\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel\nfrom category_encoders import JamesSteinEncoder, CatBoostEncoder, LeaveOneOutEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom loguru import logger\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nclass CustomStratifiedKFold(RepeatedStratifiedKFold):\n    def __init__(self, X, stratify_column=None, n_splits=5, n_repeats=10, random_state=None):\n        \"\"\"\n        Initialize the custom stratified splitter.\n\n        Parameters:\n        - X: DataFrame containing the features and the stratify column.\n        - stratify_column: Column name to use for stratification (str).\n        - n_splits: Number of splits (folds).\n        - n_repeats: Number of repetitions of the splits.\n        - random_state: Random state for reproducibility.\n        \"\"\"\n        super().__init__(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n\n        self.stratify_column = stratify_column\n        if stratify_column:\n            # Initialize LabelEncoder if stratify_column is provided\n            self.le = LabelEncoder()\n            self.le.fit(X[stratify_column])\n\n    def split(self, X, y):\n        \"\"\"\n        Perform stratified splits based on the specified stratify column.\n\n        Parameters:\n        - X: The input features (DataFrame).\n        - y: Target labels (not used for stratification here, but required for the split method).\n\n        Returns:\n        - Generator of train-test indices for each fold.\n        \"\"\"\n        # Encode the stratify column if it exists\n        stratify_data = self.le.transform(X[self.stratify_column])\n        # Perform the stratified split using the stratify data\n        kfs = super().split(X, stratify_data)\n        return kfs\n\n\n\nclass AutoMLRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, settings=None, stratify_column=None, encoder_type=\"james_stein\", feature_selector=\"variance\"):\n        \"\"\"\n        Initialize the AutoMLRegressor with user settings, encoding options, and feature selection method.\n        \"\"\"\n        self.default_settings = {\n            \"time_budget\": 30,\n            \"task\": \"regression\",\n            \"estimator_list\": [\"xgboost\", \"rf\", \"lgbm\", \"extra_tree\"],\n        }\n        self.settings = self._merge_settings(self.default_settings, settings or {})\n        self.stratify_column = stratify_column\n        self.encoder_type = encoder_type\n        self.feature_selector = feature_selector\n        self.automl = AutoML()\n        self.best_estimator_ = None\n        self.shap_values_ = None\n        self.categorical_columns = None\n        self.features_selected = None\n        self.cat_encoder = None\n    def _merge_settings(self, default_settings, user_settings):\n        return {**default_settings, **user_settings}\n\n    def _get_categorical_subset(self, df):\n        \"\"\"\n        Select categorical columns based on dtype or a provided list.\n        \"\"\"\n        if self.categorical_columns is not None:\n            missing_cols = set(self.categorical_columns) - set(df.columns)\n            if missing_cols:\n                logger.warning(f\"Missing columns in the DataFrame: {', '.join(missing_cols)}\")\n            return df[self.categorical_columns]\n        categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n        if len(categorical_columns) == 0:\n            logger.info(\"No categorical columns detected.\")\n        return df[categorical_columns] if len(categorical_columns) &gt; 0 else pd.DataFrame()\n\n    def _encode_categorical_features(self, X, y):\n        \"\"\"\n        Encode categorical features using the selected encoding method.\n        \"\"\"\n        X_cat = self._get_categorical_subset(X)\n        if not X_cat.empty:\n            if self.encoder_type == \"dummy\":\n                X_cat = pd.get_dummies(X_cat, drop_first=True)\n            elif self.encoder_type == \"james_stein\":\n                self.cat_encoder = JamesSteinEncoder(handle_unknown='return_nan', handle_missing='return_nan')\n                X_cat = self.cat_encoder.fit_transform(X_cat, y)\n            elif self.encoder_type == \"catboost\":\n                self.cat_encoder  = CatBoostEncoder(handle_unknown='return_nan', handle_missing='return_nan')\n                X_cat = self.cat_encoder.fit_transform(X_cat, y)\n\n            elif self.encoder_type == \"leave_one_out\":\n                self.cat_encoder = LeaveOneOutEncoder(handle_unknown='return_nan', handle_missing='return_nan')\n                X_cat = self.cat_encoder.fit_transform(X_cat, y)\n            else:\n                raise ValueError(f\"Unknown encoder_type: {self.encoder_type}\")\n            X = X.drop(columns=X_cat.columns, errors='ignore')\n            X = pd.concat([X, X_cat], axis=1)\n        else:\n            logger.info(\"No categorical columns detected in the DataFrame.\")\n        return X\n\n    def _feature_selection(self, X, y):\n        \"\"\"\n        Perform feature selection using the chosen method.\n        \"\"\"\n        total_features = len(X.columns)\n        if self.feature_selector == \"variance\":\n            logger.info(\"Applying VarianceThreshold for feature selection.\")\n            selector = VarianceThreshold(threshold=0.0)\n            selector.fit_transform(X)\n        elif self.feature_selector == \"tree_model\":\n            logger.info(\"Applying SelectFromModel Tree based feature selection.\")\n            reg = ExtraTreesRegressor(random_state=0)\n            reg.fit(X, y)\n            selector = SelectFromModel(reg, prefit=True)\n        removed_features = total_features - selector.get_support().sum()\n        logger.info(f\"Removing {removed_features} features out of {total_features}: {X.columns[~selector.get_support()]}\")\n        self.features_selected = X.columns[selector.get_support()].tolist()\n        return self.features_selected\n\n    def fit(self, X, y, n_splits=3, n_repeats=3, random_state=42):\n        \"\"\"\n        Fit the AutoML model on the training data.\n        \"\"\"\n        self.settings[\"metric\"] = self.settings.get(\"metric\", self.huber_loss)\n        X = self._encode_categorical_features(X, y)\n        selected_features = self._feature_selection(X, y)\n        X = X[selected_features]\n\n        # Reset indices\n        X.reset_index(drop=True, inplace=True)\n        y.reset_index(drop=True, inplace=True)\n\n        # Define splitter based on stratify_column\n        if self.stratify_column is not None:\n            splitter = CustomStratifiedKFold(X, self.stratify_column, n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n        else:\n            splitter = \"auto\"  # Default behavior if no stratify_column is provided\n        self.automl.fit(X_train=X, y_train=y, **self.settings, seed=random_state, split_type=splitter)\n        self.best_estimator_ = self.automl.model.estimator\n        return self\n\n    def predict(self, X):\n        features = self._transform_input_to_features(X)\n        return self.automl.predict(features)\n\n    def score(self, X, y):\n        \"\"\"\n        Calculate the huber loss function for scoring.\n        \"\"\"\n        y_pred = self.predict(X)\n        return -self.huber_loss(X, y, estimator=self.best_estimator_)[0]\n\n    def plot(self, y_true, y_pred):\n        \"\"\"\n        Plot Actual vs Predicted values as a scatter plot.\n        \"\"\"\n        fig = px.scatter(x=y_true, y=y_pred,\n                         labels={'x': 'Actual Values', 'y': 'Predicted Values'},\n                         title='Actual vs Predicted Values')\n        fig.add_shape(type='line',\n                      x0=min(y_true), y0=min(y_true),\n                      x1=max(y_true), y1=max(y_true),\n                      line=dict(color='blue', dash='dash'))\n        fig.show()\n\n    def compute_shap_values(self, X):\n        \"\"\"\n        Compute SHAP values for the provided data.\n        \"\"\"\n        features = self._transform_input_to_features(X)\n        explainer = shap.TreeExplainer(self.best_estimator_, features, feature_perturbation=\"tree_path_dependent\")\n        self.shap_values_ = explainer(features)\n\n    def plot_shap(self, plot_type=\"feature_importance\", max_display=10):\n        \"\"\"\n        Generate SHAP plots for feature importance or impact.\n        \"\"\"\n        if self.shap_values_ is None:\n            raise ValueError(\"SHAP values not computed yet. Call compute_shap_values first.\")\n        if plot_type == \"feature_importance\":\n            shap.plots.bar(self.shap_values_, max_display=max_display)\n        elif plot_type == \"feature_impact\":\n            shap.plots.beeswarm(self.shap_values_)\n        else:\n            raise ValueError(\"Invalid plot_type. Choose 'feature_importance' or 'feature_impact'.\")\n\n    def _transform_input_to_features(self, X):\n        \"\"\"\n        Applies the same transformations to input data as were applied during training.\n        \"\"\"\n        # Categorical Encoding\n        X_cat = self._get_categorical_subset(X)\n        if not X_cat.empty:\n            if self.encoder_type == \"dummy\":\n                X_cat = pd.get_dummies(X, drop_first=True)\n            else :\n                X_cat = self.cat_encoder.transform(X_cat)\n\n            X = X.drop(columns=X_cat.columns, errors='ignore')\n            X = pd.concat([X, X_cat], axis=1)\n\n\n        # Select previously chosen features\n        if self.features_selected:\n            X = X[self.features_selected]\n\n        return X\n\n    def huber_loss(self, X_val, y_val, estimator, labels=None,\n                    X_train=None, y_train=None, weight_val=None, weight_train=None,\n                    config=None, groups_val=None, groups_train=None):\n\n        delta = self.settings.get(\"delta\", 1.0)\n        y_pred = estimator.predict(X_val)\n        error = y_val - y_pred\n        is_small_error = np.abs(error) &lt;= delta\n        squared_loss = 0.5 * error ** 2\n        linear_loss = delta * (np.abs(error) - 0.5 * delta)\n        huber_loss_value = np.mean(np.where(is_small_error, squared_loss, linear_loss))\n        return huber_loss_value, {\"huber_loss\": huber_loss_value}\n</code></pre>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#illustration","title":"Illustration","text":"<p>Pour voir cette classe \u00e0 l\u2019\u0153uvre, ex\u00e9cutons-la avec table <code>house_prices</code> de sklearn :</p> <pre><code>if __name__ == \"__main__\":\n    data = fetch_openml(name=\"house_prices\", as_frame=True)\n    X = data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    custom_settings = {\n        \"time_budget\": 10,\n        \"estimator_list\": [\"xgboost\", \"rf\", \"lgbm\", \"extra_tree\"],\n        \"log_file_name\": \"automl.log\",\n        \"n_jobs\": -1\n    }\n\n    automl_regressor = AutoMLRegressor(settings=custom_settings, encoder_type=\"catboost\", feature_selector=\"variance\", stratify_column=\"MSZoning\")\n    automl_regressor.fit(X_train, y_train)\n\n    print(\"Meilleur Mod\u00e8le :\", automl_regressor.best_estimator_)\n\n    automl_regressor.compute_shap_values(X_test)\n    automl_regressor.plot_shap(plot_type=\"feature_importance\", max_display=10)\n    automl_regressor.plot_shap(plot_type=\"feature_impact\", max_display=10)\n    y_pred = automl_regressor.predict(X_test)\n    automl_regressor.plot(y_test, y_pred)\n</code></pre> <p> </p> <p>Voici quelques corrections et suggestions d'am\u00e9lioration pour rendre le texte plus fluide et clair :</p> <p>D\u00e8s le d\u00e9but de l'output, on peut voir que la m\u00e9trique utilis\u00e9e est bien personnalis\u00e9e, et on a recours \u00e0 une stratification dans la cross-validation. La m\u00e9trique par d\u00e9faut que j\u2019ai d\u00e9finie dans la classe est <code>huber_loss</code>, car elle est robuste face aux valeurs aberrantes. L\u2019id\u00e9e est que, si aucune m\u00e9trique n\u2019est fournie, on utilise <code>huber_loss</code> par d\u00e9faut.</p> <p>Les graphiques que vous voyez illustrent l'explicabilit\u00e9 avec SHAP (SHapley Additive exPlanations). Les barres repr\u00e9sentent l'importance des caract\u00e9ristiques, tandis que le graphique qui suit met en \u00e9vidence l'impact des variables. Gr\u00e2ce \u00e0 ce visuel, on peut d\u00e9duire si les valeurs de X influencent Y de mani\u00e8re positive ou n\u00e9gative, par exemple.</p> <p>Je pense d'ailleurs que je viens d'avoir une id\u00e9e d'article : expliquer les bases de SHAP et comment interpr\u00e9ter ces visualisations :)</p> <p>Le dernier graphique montre simplement l'ajustement de notre mod\u00e8le, et apparemment, tout semble en ordre !</p> <p></p> <p>On peut varier la liste des estimateurs pour inclure d\u2019autres mod\u00e8les, mais ce qui est montr\u00e9 ici est largement suffisant pour d\u00e9marrer. Et si vous voulez aller plus loin et int\u00e9grer des learners sp\u00e9cifiques, l'exemple donn\u00e9 plus t\u00f4t s'adapte parfaitement !</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#conclusion","title":"Conclusion","text":"<p>Cet article vous a fait d\u00e9couvrir comment FLAML et SHAP peuvent travailler main dans la main pour construire des mod\u00e8les de machine learning \u00e0 la fois performants et interpr\u00e9tables. FLAML facilite et acc\u00e9l\u00e8re l\u2019optimisation, tandis que SHAP fournit des outils pour expliquer vos pr\u00e9dictions.</p> <p>N\u2019h\u00e9sitez pas \u00e0 exp\u00e9rimenter ces approches dans vos projets et partagez vos retours ! Ces techniques sont des atouts pr\u00e9cieux pour renforcer la confiance dans vos mod\u00e8les pr\u00e9dictifs. Les exemples de code que j\u2019ai pr\u00e9sent\u00e9s sont pr\u00eats \u00e0 \u00eatre utilis\u00e9s, alors, lancez-vous et explorez sans limites !</p>"},{"location":"2024/10/27/auto-ml-et-interpr%C3%A9tabilit%C3%A9-avec-flaml-et-shap/#references","title":"R\u00e9f\u00e9rences","text":"<ul> <li>Documentation FLAML</li> <li>Notebook sur FLAML</li> <li>Documentation sur les valeurs SHAP</li> <li>Mon article sur l'encodage efficace des variables cat\u00e9gorielles pour du ML</li> </ul>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/","title":"Le Jour o\u00f9 Git m\u2019a Sauv\u00e9","text":"<p>On a tous une anecdote avec Git, non ? Celle o\u00f9 on se dit que ce petit outil, discret mais essentiel, nous a \u00e9vit\u00e9 une catastrophe. Eh bien, laisse-moi te raconter la mienne.  </p> <p>C\u2019\u00e9tait une journ\u00e9e normale de boulot. Apr\u00e8s avoir boss\u00e9 sur un package sur de l'IAGen que je d\u00e9veloppais depuis un bon moment, j\u2019ai fait mon commit du jour juste avant de plier bagage et de rentrer chez moi. Rien d\u2019anormal jusque-l\u00e0. Mais le lendemain matin, pris d\u2019une envie soudaine de nettoyage, j\u2019ai d\u00e9cid\u00e9 de faire du tri dans mon workspace. Le z\u00e8le, c\u2019est bien\u2026 jusqu\u2019\u00e0 ce que tu r\u00e9alises que tu viens de supprimer par m\u00e9garde tout le r\u00e9pertoire de ton package IAGen en d\u00e9veloppement. Oui, tout.  </p>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/#panique-matinale","title":"Panique matinale","text":"<p>Mon premier r\u00e9flexe\u202f: \"Oh non, tout est foutu\u202f!\". Mais apr\u00e8s une seconde (ou une dizaine de minutes) \u00e0 regarder mon \u00e9cran comme si un miracle allait se produire, j\u2019ai r\u00e9alis\u00e9 une chose\u202f: Git est mon ami. C\u2019est l\u00e0 que j\u2019ai d\u00e9cid\u00e9 d\u2019utiliser les outils de Git pour remonter dans le temps et r\u00e9cup\u00e9rer mon pr\u00e9cieux travail.  </p>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/#git-reflog-a-la-rescousse","title":"Git Reflog \u00e0 la rescousse","text":"<p>La premi\u00e8re commande magique, c\u2019est <code>git reflog</code>. Si tu ne la connais pas, dis-toi qu\u2019elle enregistre tous les mouvements de ton <code>HEAD</code>, m\u00eame ceux que tu ne vois pas dans l\u2019historique habituel des commits.  </p> <p>Voici comment \u00e7a marche :  </p> <pre><code>git reflog\n</code></pre> <p>Cette commande m\u2019a affich\u00e9 une liste de tout ce que j\u2019avais fait r\u00e9cemment sur la branche\u202f:  </p> <pre><code>6 (HEAD -&gt; main) HEAD@{0}: reset: moving to HEAD~1\n75914b7 HEAD@{1}: commit: :rocket: Test de reformulation/full_text_search\n4212ce6 HEAD@{2}: commit: :see_no_evil: Ignorer pickle\n...  \n</code></pre> <p>Le commit que je cherchais s\u2019appelait <code>:rocket: Test de reformulation/full_text_search</code>, et il \u00e9tait \u00e0 <code>HEAD@{1}</code>. Sauv\u00e9, non ? Pas encore.  </p>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/#restaurer-le-commit","title":"Restaurer le Commit","text":"<p>Une fois que j\u2019ai trouv\u00e9 le commit, il fallait revenir \u00e0 cet \u00e9tat. Voici les trois options que Git offre pour reset :  </p> <ol> <li> <p>Si tu veux garder les modifications en attente (staged) : <pre><code>git reset --soft 75914b7\n</code></pre></p> </li> <li> <p>Si tu veux garder les fichiers modifi\u00e9s sans les pr\u00e9parer (unstaged) : <pre><code>git reset 75914b7\n</code></pre></p> </li> <li> <p>Si tu veux carr\u00e9ment tout \u00e9craser et revenir \u00e0 cet \u00e9tat pr\u00e9cis : <pre><code>git reset --hard 75914b7\n</code></pre></p> </li> </ol> <p>Moi, dans ma panique, j\u2019ai jou\u00e9 la carte de la s\u00e9curit\u00e9 avec <code>--soft</code>. On ne sait jamais\u202f!  </p>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/#une-lecon-apprise","title":"Une Le\u00e7on apprise","text":"<p>Apr\u00e8s avoir tout r\u00e9cup\u00e9r\u00e9 et v\u00e9rifi\u00e9 que tout fonctionnait bien, j\u2019ai pouss\u00e9 les changements avec un petit <code>git push --force</code>. C\u2019\u00e9tait comme si rien ne s\u2019\u00e9tait pass\u00e9.  </p> <p>Morale de l\u2019histoire : Git n\u2019oublie jamais. M\u00eame quand toi, tu fais des b\u00eatises. Depuis ce jour, je fais toujours des commits r\u00e9guliers et je v\u00e9rifie deux fois avant de supprimer quoi que ce soit. Et toi, quelle est ta success story avec Git\u202f?  </p>"},{"location":"2024/11/04/le-jour-o%C3%B9-git-ma-sauv%C3%A9/#mes-conseils-pour-eviter-le-stress","title":"Mes Conseils pour \u00e9viter le stress","text":"<ol> <li>Commite souvent\u202f: Plus il y a de checkpoints, mieux c\u2019est.  </li> <li>Apprends \u00e0 utiliser <code>git reflog</code> et <code>git reset</code>\u202f: Ces outils te sauveront la mise un jour.  </li> <li>Fais une sauvegarde locale r\u00e9guli\u00e8re\u202f: Si Git ne te suffit pas, un petit script de sauvegarde ne fait jamais de mal.  </li> </ol> <p>Git, c\u2019est un peu comme un ami fid\u00e8le : il te couvre toujours, m\u00eame quand tu fais n\u2019importe quoi. \ud83d\ude09  </p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/","title":"IA g\u00e9n\u00e9rative : une \u00c9volution plut\u00f4t que R\u00e9volution","text":"<p>Aujourd\u2019hui, on plonge dans l\u2019univers fascinant de l\u2019IA g\u00e9n\u00e9rative. On va retracer les grandes \u00e9tapes qui ont marqu\u00e9 son \u00e9volution. Pas de panique, on ne va pas s\u2019embourber dans une liste exhaustive \u2013 on va plut\u00f4t se concentrer sur les moments cl\u00e9s qui ont fa\u00e7onn\u00e9 l\u2019IA telle qu\u2019on la conna\u00eet aujourd'hui. J\u2019avais depuis un moment envie de faire le point sur les avanc\u00e9es dans ce domaine, mais avec le rythme effr\u00e9n\u00e9 des innovations, je ne savais pas comment illustrer cela jusqu'\u00e0 ce que je tombe sur les travaux de l'Innovation Makers Alliance.</p> <p>Comme vous l'avez bien remarqu\u00e9, j'adore sch\u00e9matiser avant de d\u00e9crire. Donc voici un sch\u00e9ma chronologique qui r\u00e9sume tout. </p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1950-1960-les-premiers-pas-avec-le-bag-of-words","title":"1950-1960 : Les Premiers Pas avec le \"Bag of Words\"","text":"<p>L'histoire de l'IA commence dans les ann\u00e9es 1950 avec le mod\u00e8le \"Bag of Words\". Cette approche traite le texte comme une collection de mots ind\u00e9pendants, sans tenir compte de leur ordre ou de leur contexte.</p> <p>Repr\u00e9sentation Matricielle :</p> <p>Le mod\u00e8le Bag of Words convertit le texte en une matrice o\u00f9 chaque ligne repr\u00e9sente un document et chaque colonne un mot unique. Les valeurs indiquent la fr\u00e9quence des mots dans chaque document.</p> <p>Exemple :</p> IA est l' avenir fa\u00e7onnera Doc 1 1 1 1 1 0 Doc 2 1 0 1 1 1 <p>TF-IDF :</p> <p>La m\u00e9thode TF-IDF (Term Frequency-Inverse Document Frequency) \u00e9value l'importance d'un mot dans un document en comparant sa fr\u00e9quence \u00e0 celle dans l'ensemble des documents :</p> \\[ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) \\] <p>O\u00f9 : - TF (Term Frequency) mesure combien de fois un terme appara\u00eet dans un document :</p> \\[ \\text{TF}(t, d) = \\frac{\\text{Nombre de fois que le terme } t \\text{ appara\u00eet dans le document } d}{\\text{Nombre total de termes dans le document } d} \\] <ul> <li>IDF (Inverse Document Frequency) \u00e9value l'importance d'un terme dans tous les documents :</li> </ul> \\[ \\text{IDF}(t) = \\log \\frac{\\text{Nombre total de documents}}{\\text{Nombre de documents contenant le terme } t} \\] <p>TF-IDF et le Bag of Words ont longtemps domin\u00e9 l'analyse de texte (d'ailleurs toujours utilis\u00e9s pour des analyses textuelles), mais ils pr\u00e9sentent des limites majeures : ils ignorent le contexte et l'ordre des mots, produisent des vecteurs tr\u00e8s grands et sparses, et ne capturent pas les relations s\u00e9mantiques entre les termes. Ces approches ne permettent pas de saisir les nuances du langage, comme les relations de longue port\u00e9e ou les variations de sens. Ces insuffisances ont conduit au d\u00e9veloppement des r\u00e9seaux de neurones.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1986-reseaux-de-neurones-recurrents-rnns","title":"1986 : R\u00e9seaux de Neurones R\u00e9currents (RNNs)","text":"<p>En 1986, les RNNs offrent une solution pour traiter les donn\u00e9es s\u00e9quentielles comme le texte et la parole. Ils se souviennent des informations des \u00e9tapes pr\u00e9c\u00e9dentes, mais ont des difficult\u00e9s avec les d\u00e9pendances \u00e0 long terme en raison du probl\u00e8me du \"gradient qui s\u2019\u00e9vanouit\".</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#1997-les-reseaux-a-long-terme-lstm","title":"1997 : Les R\u00e9seaux \u00e0 Long Terme (LSTM)","text":"<p>Pour rem\u00e9dier aux limitations des RNNs, les r\u00e9seaux LSTM, introduits en 1997, am\u00e9liorent la capture des d\u00e9pendances \u00e0 long terme. Malgr\u00e9 cette avanc\u00e9e, ils restent complexes \u00e0 entra\u00eener et difficiles \u00e0 parall\u00e9liser.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2012-la-revolution-des-gpus-avec-alexnet","title":"2012 : La R\u00e9volution des GPUs avec AlexNet","text":"<p>En 2012, l'utilisation des GPUs pour entra\u00eener les mod\u00e8les devient une r\u00e9volution avec AlexNet, d\u00e9velopp\u00e9 par Ilya Sutskever, Alex Krizhevsky et Geoffrey Hinton. AlexNet r\u00e9duit drastiquement le temps d'entra\u00eenement et am\u00e9liore les performances dans la vision par ordinateur.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2013-word2vec-et-la-representation-vectorielle-des-mots","title":"2013 : Word2Vec et la Repr\u00e9sentation Vectorielle des Mots","text":"<p>En 2013, Word2Vec introduit une avanc\u00e9e majeure en am\u00e9liorant la repr\u00e9sentation vectorielle des mots avec les techniques CBOW et Skip-gram. Il montre que les mots apparaissant dans des contextes similaires ont des vecteurs proches, am\u00e9liorant l'apprentissage s\u00e9mantique.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2017-lessor-des-transformers","title":"2017 : L\u2019Essor des Transformers","text":"<p>L'ann\u00e9e 2017 marque l'introduction des Transformers par Google avec \"Attention is All You Need\". Les Transformers int\u00e8grent un m\u00e9canisme d'attention qui permet de capturer les relations complexes entre les mots, ind\u00e9pendamment de leur position. Cette architecture facilite le traitement parall\u00e8le des donn\u00e9es et pave la voie aux grands mod\u00e8les comme GPT. J'ai d'ailleurs commenc\u00e9 \u00e0 r\u00e9fl\u00e9chir comment illustrer les diff\u00e9rents types de Transformers qui existent de nos jours.</p>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2018-2022-les-grands-modeles-de-langage","title":"2018-2022 : Les Grands Mod\u00e8les de Langage","text":"<p>Apr\u00e8s l\u2019arriv\u00e9e des Transformers, les grands mod\u00e8les de langage se d\u00e9veloppent rapidement :</p> <ul> <li>GPT-3 (2020) : D\u00e9velopp\u00e9 par OpenAI, GPT-3, avec ses 175 milliards de param\u00e8tres, est capable de g\u00e9n\u00e9rer du texte avec une coh\u00e9rence impressionnante et de r\u00e9aliser diverses t\u00e2ches sans formation sp\u00e9cifique. Il repr\u00e9sente une avanc\u00e9e majeure en augmentant la taille des mod\u00e8les.</li> </ul> <ul> <li>DALL-E (2021) : DALL-E, \u00e9galement d'OpenAI, g\u00e9n\u00e8re des images \u00e0 partir de descriptions textuelles, fusionnant cr\u00e9ativit\u00e9 et technique.</li> </ul> <ul> <li>Midjourney (2022) : Ce mod\u00e8le se concentre sur la cr\u00e9ation artistique d\u2019images, permettant aux utilisateurs de concevoir des \u0153uvres uniques \u00e0 partir de prompts.</li> </ul> <ul> <li>ChatGPT (fin 2022) : Bas\u00e9 sur l\u2019architecture GPT et InstructGPT, ChatGPT offre des capacit\u00e9s de dialogue interactives, un tournant majeur des potentialit\u00e9s de l'IA g\u00e9n\u00e9rative. C\u2019est \u00e0 ce moment pr\u00e9cis que le grand public commence \u00e0 se rendre compte des derni\u00e8res avanc\u00e9es en IA et deep learning.</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2023-les-nouveaux-acteurs-en-plus-de-openai","title":"2023 : Les Nouveaux Acteurs en Plus de OpenAI","text":"<ul> <li>LLaMA (2023) : Meta introduit la s\u00e9rie LLaMA pour fournir des alternatives efficaces avec diff\u00e9rentes tailles de param\u00e8tres, favorisant une recherche plus accessible et performante. Ainsi, la porte \u00e0 l'open source est ouverte.</li> </ul> <ul> <li>Juste apr\u00e8s LLaMA, OpenAI entre encore en jeu avec DALL-E 3 et GPT-4, qui est, jusqu'\u00e0 nos jours, l'un des mod\u00e8les les plus performants, bien que tr\u00e8s co\u00fbteux.</li> </ul> <ul> <li>Mistral 7B (2023) : Un mod\u00e8le LLM fran\u00e7ais avec 7 milliards de param\u00e8tres, con\u00e7u pour r\u00e9pondre aux besoins sp\u00e9cifiques tout en offrant une performance de haut niveau.</li> </ul> <ul> <li>Claude 3 (2023) : La famille Claude 3 d'Anthropic, comprenant Claude 3 Haiku, Claude 3 Sonnet, et Claude 3 Opu, \u00e9tablit de nouveaux standards pour les t\u00e2ches cognitives complexes.</li> </ul> <ul> <li>Bard puis Gemini (2023) : Google rattrape son retard en annon\u00e7ant Gemini, un LLM multimodal int\u00e9grant des capacit\u00e9s avanc\u00e9es pour le texte, les images, et le son, apr\u00e8s un premier Bard qui n'a pas convaincu.</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#2024-les-dernieres-innovations","title":"2024 : Les Derni\u00e8res Innovations","text":"<ul> <li>Mistral Large (f\u00e9vrier 2024) : Ce mod\u00e8le, nativement multilingue, am\u00e9liore les capacit\u00e9s de traitement et d'interaction dans plusieurs langues. La France ne veut pas rester en reste de cette course.</li> </ul> <ul> <li>GPT-4O (mai 2024) : Ce mod\u00e8le \"omnimodal\" de OpenAI int\u00e8gre des capacit\u00e9s avanc\u00e9es pour le texte, les images, et le son. Il repr\u00e9sente un progr\u00e8s significatif en mati\u00e8re d'IA g\u00e9n\u00e9rative.</li> </ul> <ul> <li>SearchGPT (2024) : Un prototype de moteur de recherche d'OpenAI, lanc\u00e9 le 26 juillet, combinant fonctionnalit\u00e9s traditionnelles et capacit\u00e9s d'IA g\u00e9n\u00e9rative pour une exp\u00e9rience de recherche enrichie.</li> </ul> <ul> <li>NVIDIA Blackwell (2024) : Annonc\u00e9 en juillet, ce processeur acc\u00e9l\u00e8re les calculs n\u00e9cessaires pour l'entra\u00eenement des mod\u00e8les d'IA, facilitant les avanc\u00e9es technologiques. NVIDIA se montre pr\u00eat \u00e0 \u00e9pauler cette course vers l'IAG (Intelligence Artificielle G\u00e9n\u00e9ralis\u00e9e).</li> </ul>"},{"location":"2024/09/07/ia-g%C3%A9n%C3%A9rative--une-%C3%A9volution-plut%C3%B4t-que-r%C3%A9volution/#conclusion","title":"Conclusion","text":"<p>D\u00e9sormais, les data scientists, ou si vous pr\u00e9f\u00e9rez, AI engineers ou LLM engineers (on est toujours tr\u00e8s dou\u00e9s pour inventer de nouveaux titres, rires), dans les diff\u00e9rentes entreprises s'approprient ces technologies pour acc\u00e9l\u00e9rer la cr\u00e9ation de nouvelles sources de croissance, tout comme ils le faisaient d\u00e9j\u00e0 avec la data. Mais cette fois-ci, le traitement des donn\u00e9es non structur\u00e9es devient \u00e0 la fois plus simple et plus stimulant gr\u00e2ce aux nouvelles opportunit\u00e9s. L'IA g\u00e9n\u00e9rative continuera ainsi de transformer les industries avec des cas d'usage comme les chatbots, ou centre d'aide ou encore d'un centre de base de connaissance, les moteurs de recherche vectoriels, et bien d'autres.</p> <p>If you're a data scientist unfamiliar with AI, all of a sudden you're going to have to step up your game in to iagen.</p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/","title":"Comment sont construits les assistants conversationnels ? ChatGPT, Claude, Mistral","text":"<p>Aujourd'hui, on plonge dans les coulisses des grands mod\u00e8les de langage (LLM). Vous \u00eates-vous d\u00e9j\u00e0 demand\u00e9 comment ces mod\u00e8les arrivent \u00e0 r\u00e9pondre avec autant de fluidit\u00e9 ? Quand on parle de ChatGPT, techniquement c'est le r\u00e9sultat d'un mod\u00e8le OpenAI apr\u00e8s avoir fait une s\u00e9rie d'entra\u00eenements. La r\u00e9ponse tient en trois \u00e9tapes-cl\u00e9s : 1. Le pr\u00e9-entra\u00eenement sur des montagnes de donn\u00e9es, 2. Le fine-tuning pour sp\u00e9cialiser le mod\u00e8le, et 3. L\u2019apprentissage par renforcement avec feedback humain (RLHF).  </p> <p>Dans cet article, je retrace les principales \u00e9tapes avec vous. Un petit billet de blog pour clore l'ann\u00e9e 2024. On ne fera pas de MLOPS aujourd'hui, promis :)</p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#1-pre-entrainement-modele-de-fondation","title":"1. Pr\u00e9-entra\u00eenement : mod\u00e8le de fondation","text":"<p>Imaginez qu'on veuille apprendre \u00e0 un enfant \u00e0 parler. Quelle est la premi\u00e8re \u00e9tape ? Exactement, on lui expose \u00e9norm\u00e9ment de mots et phrases\u202f: conversations, livres, histoires. C\u2019est exactement ce qu\u2019on fait avec un LLM, mais \u00e0 une \u00e9chelle gigantesque.</p> <p>Comment \u00e7a marche ? On nourrit le mod\u00e8le avec un \u00e9norme corpus de donn\u00e9es\u202f: sites web, livres, forums, articles scientifiques\u2026 Le mod\u00e8le doit pr\u00e9dire le mot suivant dans une phrase. Par exemple\u202f: si je dis \"Les oiseaux volent dans le ciel\u2026\", \u00e0 votre avis, quel sera le mot suivant\u202f? Voil\u00e0, c\u2019est ce que le mod\u00e8le apprend \u00e0 faire.</p> <p>Voici une version corrig\u00e9e de votre section avec les erreurs grammaticales, orthographiques et typographiques corrig\u00e9es :  </p> <p>Exemple concret : Le dataset utilis\u00e9 pour LLaMA </p> <p>Voici un aper\u00e7u des donn\u00e9es qui ont servi pour LLaMA (un mod\u00e8le open-source) :</p> Source Proportion utilis\u00e9e \u00c9poques (Passages) Taille CommonCrawl 67 % 1.10 3,3 To C4 15 % 1.06 783 Go GitHub 4,5 % 0.64 328 Go Wikipedia 4,5 % 2.45 83 Go Livres 4,5 % 2.23 85 Go ArXiv 2,5 % 1.06 92 Go StackExchange 2 % 1.03 78 Go <p>Ces donn\u00e9es brutes (non \u00e9tiquet\u00e9es) permettent au mod\u00e8le d'apprendre la grammaire, les relations entre les mots et le contexte. Mais attention, cette \u00e9tape n\u2019est qu\u2019une fondation. Construire des mod\u00e8les de fondation n'est pas du ressort de petites entreprises ou startups. N\u00e9anmoins, \u00e0 partir de mod\u00e8les de fondation open source, on peut cr\u00e9er d'autres mod\u00e8les de fondation, comme ce qu'on voit actuellement autour de LLaMA (le mod\u00e8le open source par excellence).  </p> <p>Les LLM comme GPT-3 n\u00e9cessitent d\u2019\u00e9normes ressources de calcul. Par exemple, l\u2019entra\u00eenement de GPT-3 a \u00e9t\u00e9 estim\u00e9 par des chercheurs comme Tim Dettmers et d\u2019autres experts en IA :  </p> <ul> <li>Nombre de param\u00e8tres : 175 milliards (Source : OpenAI, Language Models are Few-Shot Learners*).  </li> <li>Corpus de donn\u00e9es : Environ 570 Go de texte filtr\u00e9 (Source : OpenAI, m\u00eame article).  </li> <li>Infrastructure : Utilisation de clusters de GPU, notamment des NVIDIA V100 (Source : blog de Tim Dettmers).  </li> <li>Dur\u00e9e estim\u00e9e : Entre 10 000 et 50 000 heures GPU, d\u2019apr\u00e8s des calculs ind\u00e9pendants de la communaut\u00e9, bien que les donn\u00e9es exactes ne soient pas publi\u00e9es.  </li> </ul> <p>Il faut garder en t\u00eate que les mod\u00e8les de fondation ne sont pas des assistants\u202f: ils savent juste compl\u00e9ter des phrases. D'autres couches sont ajout\u00e9es pour arriver aux agents conversationnels. J'ai trouv\u00e9 un arbre sur GitHub qui illustre bien cela :  </p> <p> </p> <p>Source : GitHub.  </p> <p>Vous verrez que la plupart des assistants conversationnels comme Bard, ChatGPT ou Claude sont aux extr\u00e9mit\u00e9s de l'arbre, car ce sont des LLM mais pas des mod\u00e8les de fondation.  </p> <p>La preuve que ce ne sont pas des assistants est bien dans les exemples suivants :  </p> <p>Le mod\u00e8le de fondation ne r\u00e9pond pas aux questions : - Il veut seulement compl\u00e9ter les documents Internet. - R\u00e9pond souvent aux questions par d'autres questions.  </p> <p> </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#2-fine-tuning-specialiser-le-modele","title":"2. Fine-Tuning : Sp\u00e9cialiser le Mod\u00e8le","text":"<p>Ici, on fait un peu comme avec un apprenti : apr\u00e8s lui avoir montr\u00e9 plein de concepts g\u00e9n\u00e9raux, on l\u2019entra\u00eene pour des t\u00e2ches sp\u00e9cifiques. Pour un mod\u00e8le conversationnel, on lui montre des dialogues bien construits, o\u00f9 la question est claire et la r\u00e9ponse pertinente.</p> <p>Pourquoi c\u2019est important ? Un mod\u00e8le brut sait parler, mais pas toujours de mani\u00e8re coh\u00e9rente. Le fine-tuning lui apprend \u00e0 r\u00e9pondre de fa\u00e7on pr\u00e9cise dans un contexte sp\u00e9cifique. Vous imaginez un mod\u00e8le qui parle moor\u00e9 (une langue locale au Burkina Faso) sans confondre les tons ni les contextes ? C\u2019est ici que la magie op\u00e8re.</p> <p>Note pratique : Vous pouvez utiliser des mod\u00e8les open-source d\u00e9j\u00e0 pr\u00e9-entra\u00een\u00e9s, comme LLaMA, pour gagner du temps. Ajoutez vos propres donn\u00e9es annot\u00e9es pour un fine-tuning personnalis\u00e9.</p> <p>Voici une version corrig\u00e9e de votre section, avec des ajustements pour am\u00e9liorer la clart\u00e9 et corriger les erreurs :  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#3-rlhf-le-dernier-coup-de-pinceau","title":"3. RLHF : Le Dernier Coup de Pinceau","text":"<p>Voici l\u2019\u00e9tape la plus fascinante, mais aussi la plus complexe\u202f: le Reinforcement Learning from Human Feedback (RLHF). Pourquoi cette \u00e9tape\u202f? Pour que le mod\u00e8le ne soit pas seulement performant, mais qu\u2019il soit aussi align\u00e9 sur vos attentes.  </p> <p>RLHF (Reinforcement Learning with Human Feedback) combine Reward Modeling (RM) et Reinforcement Learning (RL) pour aligner des mod\u00e8les comme GPT sur les pr\u00e9f\u00e9rences humaines. D\u00e9cortiquons ces deux aspects essentiels :  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#1-reward-modeling-rm-construire-un-modele-de-recompense","title":"1. Reward Modeling (RM) : Construire un mod\u00e8le de r\u00e9compense","text":"<p>Le Reward Modeling est la premi\u00e8re \u00e9tape du pipeline RLHF. Il s'agit de construire un mod\u00e8le capable d'\u00e9valuer les r\u00e9ponses du mod\u00e8le g\u00e9n\u00e9ratif (GPT) en fonction des pr\u00e9f\u00e9rences humaines.  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#processus","title":"Processus","text":"<ol> <li> <p>Collecte de donn\u00e9es :  </p> <ul> <li>Des annotateurs humains examinent plusieurs r\u00e9ponses g\u00e9n\u00e9r\u00e9es par le mod\u00e8le pour un m\u00eame prompt.  </li> <li>Ils les classent selon leur qualit\u00e9 (par exemple, du meilleur au pire).  </li> </ul> </li> <li> <p>Entra\u00eenement du mod\u00e8le de r\u00e9compense :  </p> <ul> <li>Les donn\u00e9es de classement humain sont utilis\u00e9es pour entra\u00eener un Reward Model (RM).  </li> <li>Le RM apprend \u00e0 attribuer un score \u00e0 chaque r\u00e9ponse (comme dans l'image ci-dessous) pour refl\u00e9ter la pr\u00e9f\u00e9rence humaine.  </li> </ul> </li> </ol>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#exemple","title":"Exemple","text":"<p>Dans cet exemple, les sorties du mod\u00e8le pour le token <code>&lt;|reward|&gt;</code> g\u00e9n\u00e8rent des scores comme 0.2, 1.2, et -0.5 pour trois compl\u00e9tions diff\u00e9rentes. La fonction de perte ajuste les pr\u00e9dictions du mod\u00e8le en fonction des pr\u00e9f\u00e9rences humaines : le mod\u00e8le apprend \u00e0 attribuer une r\u00e9compense plus \u00e9lev\u00e9e \u00e0 la compl\u00e9tion pr\u00e9f\u00e9r\u00e9e (par exemple, celle qui a obtenu 1.2). Les autres sorties (tokens non verts) sont ignor\u00e9es durant l\u2019entra\u00eenement. Ainsi, seul le score final des tokens <code>&lt;|reward|&gt;</code> influence l'apprentissage.  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#2-reinforcement-learning-rl-optimiser-le-modele-generatif","title":"2. Reinforcement Learning (RL) : Optimiser le mod\u00e8le g\u00e9n\u00e9ratif","text":"<p>Une fois le mod\u00e8le de r\u00e9compense entra\u00een\u00e9, il est int\u00e9gr\u00e9 dans un pipeline de Reinforcement Learning pour ajuster le mod\u00e8le g\u00e9n\u00e9ratif principal (GPT).  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#processus_1","title":"Processus","text":"<ol> <li> <p>Politique initiale :  </p> <ul> <li>On part d'un mod\u00e8le g\u00e9n\u00e9ratif pr\u00e9-entra\u00een\u00e9, comme GPT-3, qui sert de politique initiale.  </li> </ul> </li> <li> <p>Interaction avec le mod\u00e8le de r\u00e9compense :  </p> <ul> <li>Le mod\u00e8le g\u00e9n\u00e9ratif propose des r\u00e9ponses pour divers prompts.  </li> <li>Le mod\u00e8le de r\u00e9compense (RM) attribue des scores \u00e0 ces r\u00e9ponses.  </li> </ul> </li> <li> <p>Optimisation par renforcement :  </p> <ul> <li>Une m\u00e9thode d'apprentissage par renforcement, comme PPO (Proximal Policy Optimization), est utilis\u00e9e pour am\u00e9liorer la politique du mod\u00e8le g\u00e9n\u00e9ratif.  </li> <li>L'objectif est de maximiser la r\u00e9compense attribu\u00e9e par le RM, ce qui correspond indirectement \u00e0 aligner les sorties du mod\u00e8le sur les pr\u00e9f\u00e9rences humaines.  </li> </ul> </li> </ol>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#exemple_1","title":"Exemple","text":"<p>Dans cette phase d\u2019apprentissage par renforcement, les cellules jaunes jouent un r\u00f4le fondamental. Ce sont elles qui sont utilis\u00e9es directement pour l\u2019entra\u00eenement du mod\u00e8le g\u00e9n\u00e9ratif. Contrairement aux cellules vertes <code>&lt;|reward|&gt;</code> (qui servent \u00e0 attribuer des scores via le Reward Model), les tokens jaunes correspondent aux compl\u00e9tions g\u00e9n\u00e9r\u00e9es par le mod\u00e8le, et elles deviennent les labels utilis\u00e9s pour ajuster les probabilit\u00e9s des sorties. </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#3-rlhf-combinaison-des-deux","title":"3. RLHF : Combinaison des deux","text":"<p>L'approche RLHF combine ces deux \u00e9tapes de mani\u00e8re it\u00e9rative : - Reward Modeling : Cr\u00e9er une fonction de r\u00e9compense align\u00e9e sur les jugements humains. - Reinforcement Learning : Optimiser le mod\u00e8le g\u00e9n\u00e9ratif en utilisant cette fonction de r\u00e9compense.  </p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#avantages","title":"Avantages","text":"<ul> <li>Permet d'obtenir un mod\u00e8le qui g\u00e9n\u00e8re des r\u00e9ponses align\u00e9es sur les valeurs humaines.  </li> <li>R\u00e9duit les biais potentiels des mod\u00e8les de langage pr\u00e9-entra\u00een\u00e9s sur des donn\u00e9es brutes.  </li> </ul>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#comparaison-entre-rm-et-rl","title":"Comparaison entre RM et RL","text":"Aspect Reward Modeling (RM) Reinforcement Learning (RL) But principal Estimer une r\u00e9compense pour \u00e9valuer la qualit\u00e9 des r\u00e9ponses. Optimiser les r\u00e9ponses du mod\u00e8le pour maximiser la r\u00e9compense. Donn\u00e9es utilis\u00e9es Classements ou annotations humaines. Mod\u00e8le de r\u00e9compense et outputs du mod\u00e8le g\u00e9n\u00e9ratif. Sortie Fonction de r\u00e9compense (mod\u00e8le RM). Politique optimis\u00e9e (meilleur mod\u00e8le g\u00e9n\u00e9ratif). <ul> <li>Sans RLHF : Le mod\u00e8le pourrait fournir des r\u00e9ponses factuelles mais non adapt\u00e9es au contexte ou non align\u00e9es sur des normes sociales.  </li> <li>Avec RLHF : Le mod\u00e8le est capable de fournir des r\u00e9ponses non seulement factuelles, mais aussi nuanc\u00e9es, polies, et utiles selon les besoins humains.  </li> </ul>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#4-petit-recap-avec-un-visuel","title":"4. Petit R\u00e9cap avec un Visuel","text":"<p>Voici un sch\u00e9ma tir\u00e9 de la pr\u00e9sentation d\u2019Andrej Karpathy. Il r\u00e9sume bien les \u00e9tapes\u202f:</p> <p></p>"},{"location":"2024/12/15/comment-sont-construits-les-assistants-conversationnels--chatgpt-claude-mistral/#conclusion-et-references","title":"Conclusion et r\u00e9f\u00e9rences","text":"<p>Cr\u00e9er un assistant comme chatgopt conversationnel peut sembler complexe, mais en suivant ces trois \u00e9tapes, on peut transformer un mod\u00e8le de fondation en un assistant puissant. Et vous, qu'en pensez-vous des mod\u00e8les de fondations open source et comment faites vous du RHLF,</p> <p>Pour aller plus loin : - Conf\u00e9rence d\u2019Andrej Karpathy sur GPT : Regarder ici. - Blog de Hugging Face sur RLHF : Lire ici. - Ressources sur les LLM : Explorer ici.</p>"},{"location":"2024/08/11/iam-for-eks/","title":"Iam for eks","text":""},{"location":"2024/08/11/iam-for-eks/#understanding-iam-roles-and-users-in-aws-with-a-practical-example-running-an-application-on-eks","title":"Understanding IAM Roles and Users in AWS with a Practical Example: Running an Application on EKS","text":"<p>AWS Identity and Access Management (IAM) is a cornerstone of security and access control in AWS environments. IAM allows you to manage users, groups, and roles, and to specify their permissions to access AWS resources. In this article, we will explore the differences between IAM roles and IAM users, provide a practical scenario of deploying an application on Amazon EKS (Elastic Kubernetes Service), and clarify how IAM roles relate to specific AWS console options.</p>"},{"location":"2024/08/11/iam-for-eks/#1-iam-role-vs-iam-user","title":"1. IAM Role vs. IAM User","text":"<p>IAM User: - An IAM user is an entity that represents a person or application that interacts with AWS resources. Each IAM user has a unique set of credentials (username and password or access keys). - IAM users are generally assigned permissions directly or through group memberships. - Example: A developer who needs access to specific AWS resources and can authenticate using their IAM user credentials.</p> <p>IAM Role: - An IAM role is an AWS identity with specific permissions that can be assumed by entities like AWS services, EC2 instances, or even IAM users. - Roles are temporary and are assumed by entities that need to perform specific actions. They do not have permanent credentials; instead, they provide temporary security credentials. - Example: An EC2 instance running a containerized application that needs to pull images from Amazon ECR.</p>"},{"location":"2024/08/11/iam-for-eks/#2-practical-scenario-deploying-an-application-on-eks","title":"2. Practical Scenario: Deploying an Application on EKS","text":"<p>Let\u2019s walk through an example of deploying an application, such as Gradio, on Amazon EKS. Gradio is a popular library for creating machine learning demos.</p> <p>Steps for Deployment:</p> <ol> <li> <p>Set Up IAM Roles for EKS:</p> <ul> <li>Cluster Role: Create an IAM role that the EKS cluster will use. This role allows EKS to manage the underlying EC2 instances and perform other operations.</li> <li>Node Instance Role: Create another IAM role for EC2 instances that will run the Kubernetes worker nodes. This role allows the instances to interact with other AWS services, like pulling images from ECR or writing logs to CloudWatch.</li> </ul> <p>Example IAM Policy for Node Instance Role:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeTags\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\",\n                \"logs:CreateLogStream\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create an EKS Cluster:</p> <ul> <li>Go to the EKS Console.</li> <li>Create a new EKS cluster and associate it with the IAM cluster role created earlier.</li> </ul> </li> <li> <p>Launch EC2 Instances for Kubernetes Nodes:</p> <ul> <li>Launch EC2 instances and associate them with the IAM node instance role. Ensure these instances are part of the EKS node group.</li> </ul> </li> <li> <p>Deploy Gradio Application:</p> <ul> <li>Package your Gradio application into a Docker container and push the image to Amazon ECR.</li> <li>Create a Kubernetes deployment YAML file specifying the container image from ECR and deploy it to your EKS cluster.</li> </ul> <p>Example Kubernetes Deployment YAML:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gradio-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: gradio\n  template:\n    metadata:\n      labels:\n        app: gradio\n    spec:\n      containers:\n        - name: gradio-container\n          image: &lt;your-ecr-repository-url&gt;/gradio-app:latest\n          ports:\n            - containerPort: 7860\n</code></pre> </li> <li> <p>Expose the Application:</p> <ul> <li>Create a Kubernetes Service to expose the Gradio application to the internet.</li> </ul> <p>Example Service YAML:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: gradio-service\nspec:\n  selector:\n    app: gradio\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 7860\n  type: LoadBalancer\n</code></pre> </li> </ol>"},{"location":"2024/08/11/iam-for-eks/#3-iam-roles-in-aws-console-options","title":"3. IAM Roles in AWS Console Options","text":"<p>You mentioned having different console options: <code>DATAengROLE</code>, <code>DATASCIENC</code>, and <code>DATAANALYST</code>. These are IAM roles configured for various teams or purposes. Here\u2019s how they might be used:</p> <ul> <li>DATAengROLE: This role could be configured to provide access to data engineering tools and resources, like Apache Airflow, for data engineers. If you have access to Airflow with this role, it means that <code>DATAengROLE</code> includes permissions to view and manage Airflow resources.</li> </ul> <ul> <li>DATASCIENC: This role might be tailored for data scientists, granting access to tools and resources pertinent to data analysis and modeling. The specific permissions and services available to this role would depend on the policies attached.</li> </ul> <ul> <li>DATAANALYST: This role could be for data analysts, providing access to reporting tools or datasets but not necessarily the same resources as the other roles.</li> </ul> <p>Role-Based Access: In your case, when using the <code>DATAengROLE</code>, you have access to Airflow because this role has the necessary permissions configured. Conversely, the <code>DATAANALYST</code> role might not have the same permissions, hence the lack of access to Airflow.</p>"},{"location":"2024/08/11/iam-for-eks/#conclusion","title":"Conclusion","text":"<p>Understanding the difference between IAM roles and IAM users is fundamental for managing access and permissions in AWS. IAM roles are particularly useful for granting temporary access and managing permissions for AWS services and resources, while IAM users are suited for individuals requiring direct access.</p> <p>In the context of deploying an application like Gradio on Amazon EKS, properly configuring IAM roles ensures that your EKS cluster and EC2 instances have the appropriate permissions to interact with other AWS services. Additionally, understanding IAM roles in relation to different AWS console options helps in managing access based on specific roles and responsibilities within your organization.</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/","title":"\u00c9valuer les Capacit\u00e9s des LLMs : les benchmarks","text":"<p>Les mod\u00e8les de langage (LLMs) ont r\u00e9alis\u00e9 des avanc\u00e9es spectaculaires dans des t\u00e2ches vari\u00e9es, telles que la r\u00e9daction, la conversation, et la programmation. Pour \u00e9valuer et comparer efficacement leur intelligence, divers benchmarks sont utilis\u00e9s, mesurant des capacit\u00e9s allant des connaissances acad\u00e9miques (comme MMLU) au raisonnement complexe (GPQA), en passant par des comp\u00e9tences sp\u00e9cifiques telles que les math\u00e9matiques de base (GSM8K) ou la g\u00e9n\u00e9ration de code (HumanEval). </p> <p>Ces \u00e9valuations permettent de mieux cerner la port\u00e9e des capacit\u00e9s des LLMs, bien que certains benchmarks se concentrent encore principalement sur des questions ferm\u00e9es avec des r\u00e9ponses courtes, limitant ainsi une \u00e9valuation compl\u00e8te de leurs aptitudes.</p> <p>Pour mieux comprendre comment les LLMs se comparent entre eux, il est essentiel d'examiner ces benchmarks en d\u00e9tail. Le tableau ci-dessous, illustre une comparaison des performances de leur mod\u00e8le Claude face \u00e0 d'autres LLMs leaders :  Source : Site Anthropic</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#concepts-clees","title":"Concepts cl\u00e9es :","text":"<ul> <li>SOTA (State-of-the-Art) : Se r\u00e9f\u00e8re aux mod\u00e8les, algorithmes ou techniques les plus performants actuellement dans un domaine d'\u00e9tude sp\u00e9cifique.</li> <li>STEM : Acronyme pour Science, Technology, Engineering, and Mathematics, repr\u00e9sentant des disciplines cl\u00e9s souvent utilis\u00e9es pour tester les capacit\u00e9s des LLMs en mati\u00e8re de compr\u00e9hension et de raisonnement scientifique.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#les-principaux-benchmarks-pour-evaluer-les-llms","title":"Les Principaux Benchmarks pour \u00c9valuer les LLMs","text":""},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#1-mmlu-massive-multitask-language-understanding","title":"1. MMLU (Massive Multitask Language Understanding)","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>Le MMLU \u00e9value les mod\u00e8les en se basant sur les connaissances acquises lors de la pr\u00e9-formation, en se concentrant sur les r\u00e9glages z\u00e9ro-shot et few-shot. Ce benchmark couvre 57 sujets, incluant les STEM, les sciences humaines et sociales, avec des niveaux de difficult\u00e9 allant de l'\u00e9l\u00e9mentaire \u00e0 l'avanc\u00e9. </p> <ul> <li>Type de donn\u00e9es : Questions \u00e0 choix multiples</li> <li>Crit\u00e8re de scoring : Proportion de r\u00e9ponses correctes exactes (par exemple, 'A', 'B', etc.).</li> <li>Environnement d'\u00e9valuation : Con\u00e7u pour des configurations z\u00e9ro-shot et few-shot pour tester les capacit\u00e9s g\u00e9n\u00e9rales des LLMs sans ajustement sp\u00e9cifique aux t\u00e2ches.</li> </ul> <p> Source : Papier original MMLU</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#2-hellaswag","title":"2. HellaSwag","text":"<ul> <li>Publication : 2019</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>HellaSwag \u00e9value les capacit\u00e9s de raisonnement des LLMs \u00e0 travers des t\u00e2ches de compl\u00e9tion de phrases. Il teste si les mod\u00e8les peuvent s\u00e9lectionner la fin appropri\u00e9e parmi un ensemble de quatre choix pour 10 000 phrases. </p> <ul> <li>M\u00e9trique utilis\u00e9e : Proportion de r\u00e9ponses correctes exactes.</li> <li>Sp\u00e9cificit\u00e9 : Met l'accent sur le raisonnement de bon sens, un domaine o\u00f9 de nombreux mod\u00e8les \u00e9chouent encore.</li> <li>Structure des t\u00e2ches : Les t\u00e2ches sont des compl\u00e9tions de phrases o\u00f9 les choix sont construits de mani\u00e8re \u00e0 sembler plausibles pour tester les limites du mod\u00e8le.</li> </ul> <p></p> <p>Source : Papier original HellaSwag</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#3-big-bench-hard-beyond-the-imitation-game-benchmark","title":"3. BIG-Bench Hard (Beyond the Imitation Game Benchmark)","text":"<ul> <li>Publication : 2022</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>BIG-Bench Hard s\u00e9lectionne 23 t\u00e2ches difficiles du BIG-Bench suite, un ensemble diversifi\u00e9 de 204 t\u00e2ches con\u00e7ues pour d\u00e9passer les capacit\u00e9s des mod\u00e8les de langage. </p> <ul> <li>Caract\u00e9ristiques uniques : Inclut des t\u00e2ches qui d\u00e9passent les capacit\u00e9s des mod\u00e8les de langage actuels, n\u00e9cessitant souvent un raisonnement avanc\u00e9 ou des r\u00e9ponses multi-pas.</li> <li>M\u00e9thodologie : Utilisation de Chain-of-Thought (CoT) prompting pour am\u00e9liorer les performances des LLMs sur des t\u00e2ches complexes.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#4-humaneval","title":"4. HumanEval","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>HumanEval consiste en 164 t\u00e2ches de programmation uniques pour \u00e9valuer les capacit\u00e9s de g\u00e9n\u00e9ration de code des mod\u00e8les. Ces t\u00e2ches couvrent un large spectre, des algorithmes \u00e0 la compr\u00e9hension des langages de programmation. </p> <ul> <li>Types de t\u00e2ches : Algorithmes, manipulation de donn\u00e9es, compr\u00e9hension syntaxique.</li> <li>M\u00e9trique d'\u00e9valuation : Capacit\u00e9 du mod\u00e8le \u00e0 g\u00e9n\u00e9rer du code correct sans intervention humaine. Les sorties doivent \u00eatre correctes au premier essai.</li> </ul>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#5-mt-bench","title":"5. MT-Bench","text":"<ul> <li>Publication : 2021</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>MT-Bench \u00e9value la qualit\u00e9 des assistants de chat en les soumettant \u00e0 une s\u00e9rie de questions ouvertes et multi-turn, en utilisant des LLMs comme juges. </p> <ul> <li>Structure : 80 questions multi-turn pour \u00e9valuer la conversation et le suivi d'instructions.</li> <li>Crit\u00e8re de scoring : Utilise GPT-4 pour noter chaque interaction sur une \u00e9chelle de 1 \u00e0 10. Le score final est la moyenne de toutes les \u00e9valuations.</li> </ul> <p> Source : Papier original MT-Bench</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#6-drop-discrete-reasoning-over-paragraphs","title":"6. DROP (Discrete Reasoning Over Paragraphs)","text":"<ul> <li>Publication : 2019</li> <li>Liens : Code | Dataset | Papier</li> </ul> <p>DROP teste les capacit\u00e9s des LLMs \u00e0 effectuer des raisonnements complexes et discrets en fonction des informations contenues dans un paragraphe. Les t\u00e2ches incluent des questions n\u00e9cessitant des calculs, des comparaisons et des extractions de texte.</p> <ul> <li>Type de donn\u00e9es : Questions \u00e0 r\u00e9ponse ouverte n\u00e9cessitant des calculs ou des comparaisons.</li> <li>Crit\u00e8re de scoring : Utilisation du F1 Score, qui combine la pr\u00e9cision et le rappel pour mesurer la capacit\u00e9 des mod\u00e8les \u00e0 g\u00e9n\u00e9rer des r\u00e9ponses exactes.</li> </ul> <p> Source : Papier original MT-Bench</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#conclusion","title":"Conclusion","text":"<p>Les benchmarks comme MMLU, HellaSwag, BIG-Bench Hard, HumanEval, MT-Bench, DROP et l'utilisation du F1 Score offrent des \u00e9valuations pr\u00e9cieuses pour mesurer les capacit\u00e9s des LLMs dans divers domaines tels que la compr\u00e9hension du langage, le raisonnement, la programmation et la conversation. Ces benchmarks, combin\u00e9s \u00e0 des scores et m\u00e9triques sp\u00e9cifiques, aident \u00e0 identifier les forces et les faiblesses des mod\u00e8les, ouvrant ainsi la voie \u00e0 des am\u00e9liorations continues dans le domaine des LLMs.</p>"},{"location":"2024/09/08/%C3%A9valuer-les-capacit%C3%A9s-des-llms--les-benchmarks/#references","title":"R\u00e9f\u00e9rences:","text":"<p>https://github.com/leobeeson/llm_benchmarks</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/","title":"Adieu OCR, Place aux LLM Multimodaux pour l'Extraction des Informations dans les Documents","text":"<p>Les formats comme PDF, Word ou PowerPoint sont omnipr\u00e9sents pour le partage de documents, mais d\u00e8s qu\u2019il s\u2019agit d\u2019extraire des donn\u00e9es structur\u00e9es ou de g\u00e9rer des contenus complexes, c\u2019est une toute autre histoire.  </p> <p>Ceux qui se sont d\u00e9j\u00e0 aventur\u00e9s dans ces t\u00e2ches connaissent la frustration des outils traditionnels comme <code>PDFium</code>, <code>pypdf</code> ou <code>textract</code>. Bien que pratiques pour des besoins basiques, ces solutions montrent vite leurs limites face aux documents denses, aux tableaux imbriqu\u00e9s, ou au texte qui se chevauche.  </p> <p>Mais les temps changent. Avec l\u2019arriv\u00e9e des LLM multimodaux, les choses s'am\u00e9liorent drastiquement. Ces mod\u00e8les permettent non seulement de traiter des fichiers comme des PDF, PPTX ou Word, qu\u2019ils soient purement textuels ou visuels, mais offrent \u00e9galement la possibilit\u00e9 de personnaliser l\u2019extraction. Par exemple, vous pouvez demander un formatage pr\u00e9cis des donn\u00e9es extraites.  </p> <p>Dans cet article, je vous propose de d\u00e9couvrir comment les LLM multimodaux r\u00e9volutionnent cette t\u00e2che autrefois laborieuse, en rendant les processus non seulement plus simples mais aussi plus abordables.  Allons-y !</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#i-ocr-et-defis","title":"I. OCR et D\u00e9fis","text":"<p>Avant d\u2019aller loin, voici un aper\u00e7u d\u2019une page d\u2019un document que je traite (plus de 320 pages au total). Mon objectif est simple mais ambitieux : extraire pour chaque mot en moor\u00e9 son \u00e9quivalent ou explication associ\u00e9e en fran\u00e7ais.  </p> <p> </p> <p>Quand il s\u2019agit de documents simples, l\u2019extraction de texte est faisable sans trop de souci. Mais d\u00e8s qu\u2019on passe \u00e0 des documents complexes, les vrais d\u00e9fis apparaissent : 1. D\u00e9tection des structures hasardeuse : les tableaux, colonnes et sections imbriqu\u00e9es deviennent illisibles pour certains outils. 2. Multi-langues : le contenu en plusieurs langues pose probl\u00e8me \u00e0 de nombreux OCR, surtout lorsqu'il m\u00e9lange des langues locales et internationales. 3. Fiabilit\u00e9 limit\u00e9e : beaucoup d'OCR ajoutent du bruit au lieu d\u2019apporter des r\u00e9sultats propres et exploitables.  </p> <p>Au vu de ces limitations, j\u2019ai vite abandonn\u00e9 la solution OCR classique pour mon projet.</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#ii-une-solution-gagnante-les-llm-multimodaux","title":"II. Une solution gagnante : les LLM multimodaux","text":""},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#processus-simplifie","title":"Processus simplifi\u00e9","text":"<pre><code>graph LR;  \n    A[\"document\"] --&gt; B[Images]  \n    B --&gt; C[Images B64]  \n    C --&gt;D[LLM multimodal]  \n    D --&gt; E[JSON structur\u00e9]  \n    E --&gt; F[(Stocker)]  </code></pre> <p>Ce diagramme illustre parfaitement pourquoi cette approche est moins fastidieuse. Gr\u00e2ce aux LLM multimodaux, m\u00eame les t\u00e2ches complexes, comme celle-ci, deviennent g\u00e9rables en si peu de temps. Par exemple, GPT-4o ou GPT-4o-mini excelle dans ce genre de traitement.</p> <p>\ud83d\udc49 Pourquoi c\u2019est gagnant ? - Meilleure compr\u00e9hension des structures : les tableaux et sections imbriqu\u00e9es sont correctement analys\u00e9s. - Prise en charge multilingue : les LLM reconnaissent les langues locales et internationales sans trop de pertes. - R\u00e9sultats propres : moins de bruit et plus d'informations directement exploitables.  </p> <p>Avec cette strat\u00e9gie, chaque page est trait\u00e9e avec pr\u00e9cision, et les donn\u00e9es sont pr\u00eates \u00e0 \u00eatre utilis\u00e9es pour alimenter ma base de traduction moor\u00e9-fran\u00e7ais.</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#estimation-du-cout-par-page","title":"Estimation du co\u00fbt par page","text":"<p>D'apr\u00e8s la documentation d'OpenAI(gpt vision), chaque image est redimensionn\u00e9e si n\u00e9cessaire pour s'adapter \u00e0 un carr\u00e9 de 1024x1024 pixels, g\u00e9n\u00e9rant ainsi 85 tokens de base. Mais pour des images complexes, le mod\u00e8le proc\u00e8de diff\u00e9remment : il les divise en tuiles de 512x512 pixels pour une reconnaissance compl\u00e8te. Tu peux trouver plus de d\u00e9tails ici.</p> <p>Comment les tuiles affectent le co\u00fbt ? </p> <p>Chaque tuile g\u00e9n\u00e8re 170 tokens. Donc, la formule de calcul est simple : Total tokens = 85 + 170 * n, o\u00f9 n repr\u00e9sente le nombre de tuiles n\u00e9cessaires pour couvrir l'image.</p> <p>Voici une fonction Python qui illustre cela :  </p> <pre><code>from math import ceil  \n\ndef resize(width, height):  \n    if width &gt; 1024 or height &gt; 1024:  \n        if width &gt; height:  \n            height = int(height * 1024 / width)  \n            width = 1024  \n        else:  \n            width = int(width * 1024 / height)  \n            height = 1024  \n    return width, height  \n\ndef count_image_tokens(width: int, height: int):  \n    width, height = resize(width, height)  \n    h = ceil(height / 512)  \n    w = ceil(width / 512)  \n    total = 85 + 170 * h * w  \n    return total  \n</code></pre> <p>Exemples pour clarifier : </p> <ul> <li>500x500 \u2192 1 tuile suffit : total tokens = 85 + 170 = 255</li> </ul> <ul> <li>513x500 \u2192 2 tuiles : total tokens = 85 + 170 * 2 = 425</li> </ul> <p>Dans mon cas, mes images mesurent 2480 x 3509, ce qui n\u00e9cessite environ 4 tuiles, soit 780 tokens.</p> <p>Cependant, le texte contenu dans une page d\u00e9passe g\u00e9n\u00e9ralement 780  tokens. Regarde ce qui suit :  </p> <p><pre><code>1 paragraphe \u2248 100 tokens  \n1 500 mots \u2248 2048 tokens  \n</code></pre> Source : OpenAI Help</p> <p>Avec mes pages, qui contiennent plusieurs paragraphes, on d\u00e9passe largement les 780 tokens d'une image seule . Convaincu maintenant\u202f?  J'ai inclus ces calculs pour bien montrer comment le comptage des tokens fonctionne.</p> <p>Co\u00fbt estim\u00e9 avec GPT-4o : - Entr\u00e9e : Environ 1 300 tokens (image + instruction). - Sortie : Environ 900 tokens. - Co\u00fbt : 0,002 $ par image.  </p> <p>Tout ceci pour montrer qu'il est crucial de bien \u00e9valuer le co\u00fbt avant de traiter un volume important de documents ! \ud83d\ude09</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#iii-conversion-pdf-en-images","title":"III. Conversion PDF en images","text":"<p>Bon, Arr\u00eatons les calculs maintenant hahaah. Pour convertir des PDF en images, nous avons besoin d\u2019un outil nomm\u00e9 Poppler.</p> <p>Poppler est une biblioth\u00e8que gratuite et open-source pour le rendu des documents PDF, soutenue par freedesktop.org.  </p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#pre-requis","title":"Pr\u00e9-requis","text":"<ol> <li> <p>Installer Poppler :    T\u00e9l\u00e9chargez Poppler pour Windows et ajoutez son chemin dans vos variables d\u2019environnement.  </p> </li> <li> <p>Installer pdf2image avec pip : <pre><code>pip install pdf2image  \n</code></pre></p> </li> </ol>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#code-python","title":"Code Python","text":"<p>Voici comment convertir un PDF en images JPEG : <pre><code>from pdf2image import convert_from_path  \n\npdf_path = \"./Dictionnaire.pdf\"  \noutput_folder = \"./images\"  \n\nprint(\"D\u00e9but de la conversion...\")  \nimages = convert_from_path(pdf_path, dpi=300, output_folder=output_folder, fmt='jpeg')  \n\nfor i, image in enumerate(images):  \n    image.save(f\"{output_folder}/page_{i + 1}.jpg\", \"JPEG\")  \n    print(f\"Page {i + 1} sauvegard\u00e9e.\")  \n</code></pre> Ce bout de code transforme toutes les pages en images. Quand tu ouvres ton dossier de sortie, tu devrais voir quelque chose comme ceci : </p> <p>Voici la version corrig\u00e9e avec les erreurs grammaticales et d'orthographe corrig\u00e9es :</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#iv-extraction-dinformations-avec-un-llm","title":"IV. Extraction d\u2019informations avec un LLM","text":"<p>Maintenant que nous avons les documents, comment r\u00e9cup\u00e9rer les informations qui nous int\u00e9ressent ? Toutes les \u00e9tapes sont dans l'illustration ci-dessous :</p> <pre><code>flowchart LR\n    A[Image] --&gt; B[Encoder en Base64]\n    B --&gt; C[Cr\u00e9er un prompt]\n    C --&gt; D[Appeler l'API OpenAI]\n    D --&gt; E[R\u00e9ponse structur\u00e9e LLM]\n    E --&gt; F[Parser la r\u00e9ponse]\n    F --&gt; G[Structure Python]\n    G --&gt; H[(Persister)]</code></pre>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#etape-1-encoder-en-base64","title":"\u00c9tape 1 : Encoder en base64","text":"<p>Le mod\u00e8le doit \u00eatre capable de traiter les images, mais l\u2019API ne permet pas d'envoyer des fichiers d\u2019images directement. C\u2019est pourquoi on utilise le format base64 pour convertir l\u2019image en une cha\u00eene de texte que le mod\u00e8le pourra comprendre.  </p> <pre><code>import base64  \n\ndef convert_image_to_base64(image_path: str) -&gt; str:  \n    \"\"\"Encode une image en une cha\u00eene base64.\"\"\"  \n    with open(image_path, \"rb\") as f:  \n        return base64.b64encode(f.read()).decode(\"utf-8\")  \n</code></pre> <p>R\u00e9sum\u00e9 : Cette fonction ouvre l\u2019image, la lit en mode binaire, puis la transforme en une cha\u00eene base64. Cette cha\u00eene est ensuite utilis\u00e9e pour communiquer avec le mod\u00e8le.</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#etape-2-prompt-engineering","title":"\u00c9tape 2 : Prompt engineering","text":"<p>Pour que le mod\u00e8le comprenne que nous lui envoyons une image, il faut l\u2019indiquer clairement dans le prompt. </p> <pre><code>def create_prompt_for_dictionary(base64_image: str) -&gt; List[dict]:  \n    \"\"\"Cr\u00e9e un prompt adapt\u00e9 pour extraire des donn\u00e9es structur\u00e9es.\"\"\"  \n    instruction = \"\"\"  \n    Tu es un syst\u00e8me con\u00e7u pour extraire les connaissances de documents. Le document contient un dictionnaire Moore-Fran\u00e7ais.  \n    Retourne les entr\u00e9es au format XML-like tag &lt;output&gt;...&lt;/output&gt;. Pour chaque mot, inclut les explications d\u00e9taill\u00e9es.  \n    Ignore les parties anglaises et conserve les symboles sp\u00e9ciaux.  \n    \"\"\"  \n    return [  \n        {  \n            \"role\": \"user\",  \n            \"content\": [  \n                {\"type\": \"text\", \"text\": instruction},  \n                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpg;base64,{base64_image}\"}}  \n            ],  \n        }  \n    ]  \n</code></pre> <p>\u00c0 retenir : </p> <ol> <li>Le prompt indique clairement que le document contient des mots en Moor\u00e9 accompagn\u00e9s de leur traduction en fran\u00e7ais \u00e0 extraire. J\u2019ajoute aussi un exemple de r\u00e9ponse attendue pour guider le LLM efficacement.  </li> <li>La r\u00e9ponse est demand\u00e9e dans un format structur\u00e9 avec des balises <code>&lt;output&gt;...&lt;/output&gt;</code>.  </li> <li>L\u2019image encod\u00e9e est encapsul\u00e9e dans une balise <code>\"image_url\"</code>, ce qui permet au mod\u00e8le de comprendre qu\u2019il s\u2019agit bien d\u2019une image.  </li> <li>Ne t'attarde pas trop sur la finesse du prompt \ud83d\ude05, il n\u2019est pas super \u00e9l\u00e9gant, je l\u2019avoue\u202f!  </li> <li>Par choix personnel, je n\u2019ai pas utilis\u00e9 de syst\u00e8me prompt, uniquement un query prompt. Pour les images, il est n\u00e9cessaire de sp\u00e9cifier le type avec <code>{\"type\": \"image_url\", ...}</code>, une exigence sp\u00e9cifique qui ne s\u2019applique pas aux cas standards.  </li> </ol>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#etape-3-appel-a-lapi-openai","title":"\u00c9tape 3 : Appel \u00e0 l\u2019API OpenAI","text":"<p>On utilise l\u2019API OpenAI pour interroger le mod\u00e8le. Le mod\u00e8le est configur\u00e9 pour r\u00e9pondre en respectant les instructions et en g\u00e9n\u00e9rant une r\u00e9ponse structur\u00e9e.  </p> <pre><code>import openai  \n\nopenai.api_key = \"sk-xxxxxxxxxxxxxxxx\"  \n\ndef get_llm_response(base64_image: str, model_name: str = \"gpt-4o\") -&gt; str:  \n    \"\"\"Appelle l'API OpenAI avec une image encod\u00e9e.\"\"\"  \n    messages = create_prompt_for_dictionary(base64_image)  \n    response = openai.ChatCompletion.create(  \n        model=model_name,  \n        messages=messages,  \n        max_tokens=8000,  \n    )  \n    return response[\"choices\"][0][\"message\"][\"content\"]  \n</code></pre> <p>R\u00e9sum\u00e9 : - L\u2019image encod\u00e9e, accompagn\u00e9e des instructions, est transmise au mod\u00e8le via l\u2019API. - En retour, le mod\u00e8le fournit une r\u00e9ponse qui peut contenir des mots en Moor\u00e9 et leurs traductions.  </p> <p>Un point int\u00e9ressant \u00e0 noter est la possibilit\u00e9 de r\u00e9aliser des appels asynchrones avec <code>openai.ChatCompletion.acreate</code>. Cette m\u00e9thode permet de traiter plusieurs requ\u00eates en parall\u00e8le, ce qui est id\u00e9al pour un grand volume d\u2019images. Cependant, j\u2019ai pr\u00e9f\u00e9r\u00e9 une approche plus simple dans mon cas. Pourquoi\u202f? Parce que j\u2019avais le temps \ud83d\ude04 et, honn\u00eatement, jongler avec des <code>await</code> dans mes fonctions, ce n\u2019est pas trop mon truc\u202f!</p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#etape-4-parsing-et-sauvegarde","title":"\u00c9tape 4 : Parsing et sauvegarde","text":"<p>Avec les LLM, le formatage des r\u00e9ponses n\u2019est pas toujours parfait. Pourtant, une strat\u00e9gie simple et efficace consiste \u00e0 utiliser des balises XML pour structurer les r\u00e9ponses. </p> <p>En pratique, une expression r\u00e9guli\u00e8re permet d\u2019extraire uniquement ce qui se trouve entre des balises sp\u00e9cifiques comme <code>&lt;output&gt;...&lt;/output&gt;</code>. Cela r\u00e9sout les probl\u00e8mes o\u00f9 le mod\u00e8le ajoute du texte ou des explications inutiles avant le contenu pertinent.  </p> <p>Voici une impl\u00e9mentation typique :  </p> <pre><code>import re  \nimport ast  \nfrom typing import Optional, List  \n\ndef extract_output(text: str, tag: str) -&gt; Optional[str]:  \n    \"\"\"Extrait le contenu entre balises XML.\"\"\"  \n    pattern = fr\"&lt;{tag}&gt;(.*?)&lt;/{tag}&gt;\"  \n    matches = re.findall(pattern, text, re.DOTALL)  \n    return matches[0] if matches else None  \n\ndef parse_page_with_gpt(image_path: str, model_name: str = \"gpt-4o\") -&gt; Optional[List[dict]]:  \n    \"\"\"Processus complet : encoder une image, appeler le LLM et parser le r\u00e9sultat.\"\"\"  \n    image_base64 = convert_image_to_base64(image_path)  \n    llm_output = get_llm_response(image_base64, model_name)  \n    clean_output = extract_output(llm_output, \"output\")  \n    return ast.literal_eval(clean_output) if clean_output else None  \n</code></pre> <ol> <li> <p>Extraction XML :  </p> <ul> <li>L\u2019utilisation de balises comme <code>&lt;output&gt;</code> garantit que seules les donn\u00e9es importantes sont extraites.  </li> <li>Le module <code>re</code> aide \u00e0 identifier ces sections en utilisant des motifs pr\u00e9cis (ici, le contenu entre <code>&lt;output&gt;</code> et <code>&lt;/output&gt;</code>).  </li> </ul> </li> <li> <p>Parsing Python :  </p> <ul> <li>Une fois le texte brut extrait, il est souvent encore au format cha\u00eene de caract\u00e8res.  </li> <li><code>ast.literal_eval</code> le transforme en un objet Python (liste ou dictionnaire), ce qui facilite son utilisation ult\u00e9rieure.  </li> </ul> </li> <li> <p>Encapsulation compl\u00e8te :  </p> <ul> <li>La fonction <code>parse_page_with_gpt</code> combine toutes les \u00e9tapes, de l\u2019encodage de l\u2019image \u00e0 la r\u00e9cup\u00e9ration des donn\u00e9es, dans un processus fluide et r\u00e9utilisable.  </li> </ul> </li> </ol> <p>Astuce : Pour des projets plus avanc\u00e9s(fine-tuning), explorez Outlines. Cet outil est parfait pour g\u00e9n\u00e9rer des parseurs robustes et bien formatter les resultats selon un schema demand\u00e9.  </p> <p>Avec cette approche, vous vous assurez d\u2019une extraction propre et fiable, m\u00eame dans des sc\u00e9narios o\u00f9 les LLM g\u00e9n\u00e8rent des r\u00e9ponses un peu \"bavardes\".</p> <p>Voici \u00e0 quoi ressemble ma sortie :   </p>"},{"location":"2024/11/24/adieu-ocr-place-aux-llm-multimodaux-pour-lextraction-des-informations-dans-les-documents/#conclusion","title":"Conclusion","text":"<p>Avec leur capacit\u00e9 \u00e0 comprendre et interpr\u00e9ter des formats vari\u00e9s, les lLM multimodaux offrent une solution \u00e9l\u00e9gante pour l\u2019extraction de donn\u00e9es complexes, tout en r\u00e9duisant consid\u00e9rablement les co\u00fbts. L\u2019approche repose sur un pipeline efficace : 1. Conversion des documents en images. 2. Encodage en Base64. 3. Utilisation d\u2019un prompt structur\u00e9. 4. Parsing pr\u00e9cis pour produire des donn\u00e9es structur\u00e9es.  </p> <p>En quelques lignes de code, vous pouvez transformer des centaines de pages en donn\u00e9es directement exploitables, \u00e9liminant ainsi les frustrations des solutions classiques.  </p> <p>Et toi, quelles sont tes astuces pour extraire des donn\u00e9es structur\u00e9es \u00e0 partir de documents complexes ? \ud83d\ude0a</p>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/","title":"Trop de SDK pour les LLMs ? Passe \u00e0 une LLMFactory  ou Adapters avec LiteLLM","text":"<p>Dans l\u2019univers des LLMs, chaque provider a son propre dialecte. Tu veux utiliser OpenAI** ? Tu installes <code>openai</code>. Tu veux Claude (Anthropic) ? C\u2019est <code>anthropic</code>. Et pour tester Groq, Mistral, Fireworks, ou m\u00eame AWS Bedrock ? Chacun vient avec son propre SDK, ses headers custom, sa mani\u00e8re de formuler les prompts, et son format de sortie.</p> <p>C\u2019est vite le chaos. \ud83d\ude24</p> <p>Et quand tu construis une app s\u00e9rieuse \u2014 un backend ou un agent LLM \u2014 tu ne veux surtout pas que toute la logique de ton app d\u00e9pende d\u2019un SDK sp\u00e9cifique.</p> <p>C\u2019est pour \u00e7a qu\u2019il faut penser abstraction, d\u00e8s le d\u00e9part. Et c\u2019est l\u00e0 qu\u2019on sort le pattern <code>LLMFactory</code>.</p> <p>Perso, dans mes projets, j\u2019ai toujours un petit submodule <code>llm_factory</code> qui tra\u00eene.</p>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#pourquoi-une-llmfactory","title":"Pourquoi une LLMFactory ?","text":"<p>Une <code>LLMFactory</code>, c\u2019est comme un adaptateur intelligent qui te permet de changer de fournisseur LLM comme de chemise, sans toucher au reste de ton code.</p> <p>Ton app appelle <code>llm.predict(\"ma question\")</code>, et la factory se d\u00e9brouille \u2014 que tu sois en local avec Mistral, sur AWS avec Claude 3, ou en API avec GPT-4o.</p> <p>Objectif : d\u00e9coupler ton application de la fa\u00e7on dont tu interagis avec le LLM. Tu choisis le provider au runtime, tu injectes les bonnes cl\u00e9s, et basta.</p>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#litellm-a-la-rescousse","title":"LiteLLM \u00e0 la rescousse","text":"<p>LiteLLM te donne une API unique fa\u00e7on OpenAI pour acc\u00e9der \u00e0 plus de 100 mod\u00e8les diff\u00e9rents.</p> <p>Et quand tu combines LiteLLM avec une <code>LLMFactory</code>, tu obtiens un design propre, modulaire et future-proof.</p>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#litellm-en-un-clin-dil","title":"LiteLLM en un clin d\u2019\u0153il","text":"<p>LiteLLM est une brique open-source (librairie Python + proxy server) qui :</p> <ul> <li>Expose une API identique \u00e0 OpenAI (<code>completion</code>, <code>chat</code>, <code>embedding</code>, <code>image_generation</code>)</li> <li>Supporte 100+ mod\u00e8les (OpenAI, Azure, Claude, Mistral, Groq, Bedrock, Cohere\u2026)</li> <li>Traduit automatiquement ton payload \u201cOpenAI-style\u201d vers chaque provider</li> <li>Uniformise les r\u00e9ponses en JSON fa\u00e7on OpenAI</li> <li>G\u00e8re retry, circuit breaker, fallback (ex. : si OpenAI est down, bascule sur Mistral)</li> <li>Int\u00e8gre la surveillance (OpenTelemetry, Langfuse, Langsmith) et la gestion des co\u00fbts</li> <li> <p>S\u2019installe en deux clics :</p> <ul> <li><code>pip install litellm</code></li> <li>ou en proxy : <code>litellm --proxy-server --default-model openai/gpt-4o --otel-endpoint ...</code></li> </ul> </li> </ul> <p>\ud83d\uded1 Cette fois-ci, on ne rentre pas dans le monitoring avec Langfuse, Langsmith et autres \u2014 on garde \u00e7a pour une autre fois !</p>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#a-quoi-ressemble-une-reponse-litellm","title":"\u00c0 quoi ressemble une r\u00e9ponse LiteLLM ?","text":"<pre><code>from litellm import completion\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nresponse = completion(\n  model=\"openai/gpt-4o\",\n  messages=[{ \"content\": \"Hello, how do you feel today?\", \"role\": \"user\" }],\n)\n</code></pre> <pre><code>{\n  \"id\": \"chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885\",\n  \"created\": 1734366691,\n  \"model\": \"claude-3-sonnet-20240229\",\n  \"object\": \"chat.completion\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"finish_reason\": \"stop\",\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you...\"\n      }\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 13,\n    \"completion_tokens\": 43,\n    \"total_tokens\": 56\n  }\n}\n</code></pre> <p>\ud83d\udcdd Note : que tu appelles GPT-4 via OpenAI, Claude via AWS Bedrock ou Mistral en local, la structure reste la m\u00eame :</p> <ul> <li><code>choices[0].message.content</code> \u2192 ta r\u00e9ponse</li> <li><code>usage</code> \u2192 les tokens consomm\u00e9s</li> <li><code>model</code> \u2192 le mod\u00e8le utilis\u00e9</li> </ul>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#exemple-de-llmfactory-openai-claude-via-aws-etc","title":"Exemple de <code>LLMFactory</code> (OpenAI, Claude via AWS, etc.)","text":"<pre><code>from typing import Optional, Dict, Any\nimport litellm\nimport boto3\n\nclass BaseLLMClient:\n    def __init__(self, api_key: Optional[str] = None, model_name: str = \"\", model_params: Optional[Dict[str, Any]] = None):\n        self.api_key = api_key\n        self.model_name = model_name\n        self.model_params = model_params or {}\n\n    def predict(self, query: str, system_prompt: Optional[str] = None) -&gt; str:\n        raise NotImplementedError\n\nclass OpenAIClient(BaseLLMClient):\n    def predict(self, query: str, system_prompt: Optional[str] = None) -&gt; str:\n        messages = [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n        messages.append({\"role\": \"user\", \"content\": query})\n\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            api_key=self.api_key,\n            **self.model_params\n        )\n        return response[\"choices\"][0][\"message\"][\"content\"]\n\nclass ClaudeAWSClient(BaseLLMClient):\n    def __init__(self, model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\", model_params: Optional[Dict[str, Any]] = None):\n        super().__init__(model_name=model_name, model_params=model_params)\n        session = boto3.Session()\n        credentials = session.get_credentials()\n        self.aws_region = \"eu-west-1\"\n        self.aws_access_key = credentials.access_key\n        self.aws_secret_key = credentials.secret_key\n\n    def predict(self, query: str, system_prompt: Optional[str] = None) -&gt; str:\n        messages = [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n        messages.append({\"role\": \"user\", \"content\": query})\n\n        response = litellm.completion(\n            model=self.model_name,\n            messages=messages,\n            aws_region_name=self.aws_region,\n            aws_secret_access_key=self.aws_secret_key,\n            aws_access_key_id=self.aws_access_key,\n            **self.model_params\n        )\n        return response[\"choices\"][0][\"message\"][\"content\"]\n\nclass LLMFactory:\n    def __init__(self, config: dict):\n        self.config = config\n\n    def get_client(self):\n        provider = self.config.get(\"provider\")\n        model_name = self.config.get(\"model_name\")\n        params = self.config.get(\"model_params\", {})\n\n        if provider == \"openai\":\n            return OpenAIClient(api_key=self.config[\"api_key\"], model_name=model_name, model_params=params)\n        elif provider == \"aws_claude\":\n            return ClaudeAWSClient(model_name=model_name, model_params=params)\n        else:\n            raise ValueError(f\"Provider {provider} non support\u00e9.\")\n</code></pre> <p>Et dans ton code principal :</p> <pre><code>config = {\n    \"provider\": \"openai\",\n    \"api_key\": os.getenv(\"OPENAI_KEY\"),\n    \"model_name\": \"gpt-4o\",\n    \"model_params\": {\"temperature\": 0.3}\n}\n\nllm_client = LLMFactory(config).get_client()\nquery = \"Explique-moi le haki de l'observation.\"\nsystem_prompt = \"Tu es expert One Piece et tu ne r\u00e9ponds qu'\u00e0 des questions sur One Piece. Sois jovial comme Luffy.\"\nprint(llm_client.predict(query, system_prompt))\n</code></pre>"},{"location":"2025/05/30/trop-de-sdk-pour-les-llms--passe-%C3%A0-une-llmfactory--ou-adapters-avec-litellm/#conclusion","title":"Conclusion","text":"<p>En 2025, tu ne codes plus ton app autour d\u2019un seul SDK. Tu construis ton backend comme une vraie plateforme :</p> <ul> <li>Ind\u00e9pendante du fournisseur LLM</li> <li>Facile \u00e0 faire \u00e9voluer (changer de mod\u00e8le, ajouter un fallback, etc.)</li> <li>Modulaire et testable</li> </ul> <p>Et la LLMFactory ou lesADAPTERS`, c\u2019est ta clef pour y arriver \u2014 surtout si tu t\u2019appuies sur une brique comme LiteLLM qui fait le sale boulot d\u2019unifier les appels.</p> <p>Je t\u2019invite \u00e0 lire la \ud83d\udcda documentation officielle de LiteLLM pour :</p> <ul> <li>d\u00e9ployer ton propre serveur LiteLLM (proxy),</li> <li>g\u00e9rer tes co\u00fbts,</li> <li>monitorer les appels,</li> <li>et centraliser l\u2019usage de tes cl\u00e9s API en toute s\u00e9curit\u00e9.</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/","title":"Mermaid for data scientist","text":""},{"location":"2024/07/29/mermaid-for-data-scientist/#mermaid-pour-documenter-vos-projets","title":"Mermaid pour documenter vos projets","text":"<p>La documentation est une \u00e9tape cruciale dans tout projet de d\u00e9veloppement, mais soyons honn\u00eates, ce n'est pas toujours la plus amusante. Pourtant, quand il s'agit de clarifier des concepts complexes ou de repr\u00e9senter des processus, rien ne vaut un bon diagramme. Beaucoup de gens se tournent vers Word ou PowerPoint, voire des logiciels sp\u00e9cialis\u00e9s comme Lucidchart, pour cr\u00e9er ces illustrations. Mais pour nous, d\u00e9veloppeurs, qui aimons le code et l'automatisation, il existe un outil qui pourrait bien changer la donne : Mermaid.</p> <p>Mermaid permet de cr\u00e9er des diagrammes et des flowcharts directement \u00e0 partir de texte, ce qui vous permet de rester dans votre \u00e9diteur de code pr\u00e9f\u00e9r\u00e9. Si vous avez toujours voulu documenter vos projets avec des diagrammes sans quitter votre environnement de d\u00e9veloppement, ce guide est fait pour vous. D\u00e9couvrez comment Mermaid peut transformer votre approche de la documentation.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#preparer-votre-environnement","title":"Pr\u00e9parer votre Environnement","text":"<p>Avant de plonger dans la cr\u00e9ation de diagrammes avec Mermaid, vous devez configurer votre environnement de travail.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#extensions-recommandees-pour-vscode","title":"Extensions Recommand\u00e9es pour VSCode","text":"<p>Pour une exp\u00e9rience optimale avec Mermaid, nous vous recommandons d'installer les extensions suivantes dans Visual Studio Code :</p> <ul> <li>Markdown Preview Mermaid Support : Permet d'afficher un aper\u00e7u en temps r\u00e9el de vos diagrammes Mermaid directement dans VSCode avec la commande <code>Ctrl + K V</code>.</li> <li>Markdown PDF : Permet d'exporter vos diagrammes Mermaid en PDF avec la commande <code>Ctrl + Shift + P</code>.</li> </ul> <p>Une fois ces extensions en place, vous \u00eates pr\u00eat \u00e0 cr\u00e9er vos premiers diagrammes avec Mermaid !</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-1-definir-les-nuds-et-les-relations","title":"\u00c9tape 1 : D\u00e9finir les N\u0153uds et les Relations","text":"<p>Cr\u00e9er un diagramme avec Mermaid commence par la d\u00e9finition des n\u0153uds (ou \u00e9tapes) et des relations entre eux. Voici comment proc\u00e9der :</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#nuds","title":"N\u0153uds","text":"<p>Les n\u0153uds repr\u00e9sentent diff\u00e9rentes \u00e9tapes ou entit\u00e9s dans votre processus. Chaque n\u0153ud est identifi\u00e9 par un identifiant unique et peut avoir une \u00e9tiquette optionnelle. Voici un exemple simple :</p> <pre><code>graph LR\n    data --&gt; clean;   \n    clean --&gt; explore;   \n    explore --&gt; preprocess; </code></pre> <p>Dans cet exemple, <code>data</code>, <code>clean</code>, <code>explore</code>, et <code>preprocess</code> sont des identifiants de n\u0153uds. Les fl\u00e8ches (<code>--&gt;</code>) indiquent les relations entre eux. Ce diagramme montre un flux de gauche \u00e0 droite gr\u00e2ce \u00e0 la directive <code>graph LR</code>.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#relations","title":"Relations","text":"<p>Les relations d\u00e9finissent la s\u00e9quence ou la connexion entre les n\u0153uds. Voici quelques types de relations que vous pouvez utiliser :</p> <ul> <li><code>--&gt;</code> : Relation directionnelle de gauche \u00e0 droite.</li> <li><code>---</code> : Relation horizontale sans fl\u00e8che.</li> <li><code>==&gt;</code> : Relation bidirectionnelle.</li> <li><code>==&gt;|label|</code> : Relation directionnelle avec un label.</li> </ul> <p>Un exemple avec des relations plus complexes et des labels :</p> <pre><code>graph LR\n    data --&gt; clean;       \n    clean &lt;---&gt; explore;       \n    explore ==&gt; preprocess;       \n    preprocess ==&gt;|split| Model; </code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-2-personnaliser-les-formes-et-les-couleurs","title":"\u00c9tape 2 : Personnaliser les Formes et les Couleurs","text":"<p>Mermaid offre une vari\u00e9t\u00e9 de formes pour vos n\u0153uds. Voici un aper\u00e7u des formes les plus couramment utilis\u00e9es :</p> Forme Code Description Rectangle <code>[(...)]</code> N\u0153ud rectangulaire standard Rectangle arrondi <code>[[...]]</code> N\u0153ud avec des bords arrondis Cylindre <code>[(...)]</code> N\u0153ud en forme de cylindre Cercle <code>((...))</code> N\u0153ud en forme de cercle <p>Utilisez ces formes pour personnaliser vos diagrammes en fonction de vos besoins. Par exemple, pour cr\u00e9er un flux de travail de data science :</p> <pre><code>graph LR\n    data[(Data Collection)] --&gt; clean([Data Cleaning]);\n    clean --&gt; explore([Exploratory Data Analysis]);\n    explore --&gt; preprocess([Data Preprocessing]);\n    preprocess --&gt; split([Train/Test Split]);\n    split --&gt; model([Model Training]);\n    model --&gt; evaluate([Model Evaluation]);\n    evaluate --&gt; tune([Model Tuning]);\n    tune --&gt; model; \n    evaluate --&gt; deploy([Model Deployment]);</code></pre>"},{"location":"2024/07/29/mermaid-for-data-scientist/#etape-3-ajuster-la-direction-et-le-style","title":"\u00c9tape 3 : Ajuster la Direction et le Style","text":"<p>Mermaid vous permet de contr\u00f4ler la direction et le style de vos diagrammes gr\u00e2ce \u00e0 quelques mots-cl\u00e9s simples.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#direction-du-flux","title":"Direction du Flux","text":"<p>Vous pouvez ajuster la direction du flux avec les options suivantes :</p> <ul> <li><code>LR</code> : De gauche \u00e0 droite (Left to Right).</li> <li><code>TB</code> : De haut en bas (Top to Bottom).</li> <li><code>RL</code> : De droite \u00e0 gauche (Right to Left).</li> <li><code>BT</code> : De bas en haut (Bottom to Top).</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#style-des-nuds","title":"Style des N\u0153uds","text":"<p>Pour personnaliser l'apparence des n\u0153uds, vous pouvez utiliser :</p> <ul> <li><code>fill</code> : Couleur de fond du n\u0153ud.</li> <li><code>stroke</code> : Couleur de la bordure du n\u0153ud.</li> <li><code>color</code> : Couleur du texte \u00e0 l'int\u00e9rieur du n\u0153ud.</li> </ul> <p>Voici un exemple combinant direction et style :</p> <pre><code>graph TB\n\n    data((Data Sources))-. DHA .-&gt;DHA_data[Environmental Data];\n    data-. OpenData .-&gt;OpenData[Satellites Imagery];\n    data-. CSV files .-&gt;CSV_files[Operational Data];\n\n    DHA_data--&gt;|Preparation| Store[(Feature Store)];\n    OpenData--&gt;|Preparation| Store[(Feature Store)];\n    CSV_files--&gt;|Preparation| Store[(Feature Store)];\n\n    Store--&gt;|Modeling|Modeling_1[Airports Severity];\n\n    style data fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    style Modeling_1 fill:#9f9,stroke:#000,stroke-width:0px;</code></pre> <p>Ce diagramme utilise un flux de haut en bas (TB) et applique des styles personnalis\u00e9s aux n\u0153uds pour les mettre en valeur visuellement.</p>"},{"location":"2024/07/29/mermaid-for-data-scientist/#ressources-utiles","title":"Ressources Utiles","text":"<p>Pour aller plus loin avec Mermaid, voici quelques liens utiles :</p> <ul> <li>Documentation officielle de Mermaid</li> <li>Guide de syntaxe Mermaid pour les flowcharts</li> <li>\u00c9diteur en ligne Mermaid</li> </ul>"},{"location":"2024/07/29/mermaid-for-data-scientist/#conclusion","title":"Conclusion","text":"<p>Avec Mermaid, documenter vos projets devient non seulement plus simple, mais aussi plus amusant. Fini les allers-retours entre plusieurs outils : vous pouvez cr\u00e9er, personnaliser et int\u00e9grer des diagrammes directement depuis votre code. Que vous soyez en train de planifier un projet, de documenter un processus ou de clarifier une architecture, Mermaid est un atout pr\u00e9cieux pour tout d\u00e9veloppeur. Essayez-le, et vous ne reviendrez plus en arri\u00e8re !</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/","title":"Faire tourner son chatbot avec une interface \u00e9quivalente \u00e0 ChatGPT gr\u00e2ce \u00e0 OpenWebUI","text":"<p>L'\u00e8re des assistants conversationnels est en pleine expansion, et gr\u00e2ce \u00e0 des outils comme OpenWebUI, il est d\u00e9sormais possible de cr\u00e9er son propre UI conversationnel avec des fonctionnalit\u00e9s similaires \u00e0 celles de ChatGPT sans beaucoup d'effort. On suppose que vous avez d\u00e9j\u00e0 un chatbot qui fonctionne correctement et que vous souhaitez une meilleure interface utilisateur. Cet article vous guidera dans la mise en place de cette UI.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#1-quest-ce-quopenwebui","title":"1. Qu\u2019est-ce qu\u2019OpenWebUI ?","text":"<p>OpenWebUI est con\u00e7u pour \u00eatre une solution flexible d\u2019interface utilisateur (UI) open-source qui facilite l\u2019interaction avec les LLM comme GPT-3.5 ou GPT-4. Il repose sur deux composants principaux :</p> <ul> <li>Le composant OpenWebUI proprement dit : C\u2019est l\u2019interface utilisateur qui permet de g\u00e9rer les interactions entre l'utilisateur et le mod\u00e8le.</li> </ul> <ul> <li>Le composant Pipelines : Cette composante s\u2019occupe de la logique LLM. Elle permet d'intercepter, traiter et modifier les prompts utilisateurs avant de les envoyer au mod\u00e8le final.</li> </ul> <p>Bon, comme vous le savez , une image vaut mieux milles vaux. Voici la magie que nous propose openwebui </p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#2-concepts-de-pipelines","title":"2. Concepts de Pipelines","text":"<p>Pour comprendre l'outil pipelines, il faut s\u2019int\u00e9resser aux valves, filtres (filters) et pipes.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#21-concepts-de-valves","title":"2.1. Concepts de Valves","text":"<p>Les valves jouent un r\u00f4le de r\u00e9gulation dans le pipeline, autorisant ou bloquant le passage de certaines donn\u00e9es. Pour qu'un pipeline soit fonctionnel, on doit avoir une classe valves. Le plus souvent, c\u2019est l'endroit o\u00f9 sont pass\u00e9s les credentials cl\u00e9s et param\u00e8tres des mod\u00e8les.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#22-concept-de-filter","title":"2.2. Concept de Filter","text":"<p>Un Filter Pipeline est principalement utilis\u00e9 pour intercepter le message avant qu'il ne soit envoy\u00e9 au LLM, ou apr\u00e8s avoir re\u00e7u la r\u00e9ponse du LLM mais avant de l'envoyer \u00e0 l'utilisateur. L'id\u00e9e derri\u00e8re le Filter Pipeline est d\u2019ajouter des \u00e9tapes avant ou apr\u00e8s l'appel au mod\u00e8le. Il sert donc principalement \u00e0 :</p> <ul> <li>R\u00e9cup\u00e9rer des informations externes (RAG) pour enrichir le contexte du message avant l'envoi au LLM.</li> </ul> <ul> <li>Ex\u00e9cuter des outils qui ajoutent des donn\u00e9es suppl\u00e9mentaires n\u00e9cessaires au LLM.</li> </ul> <ul> <li>Appliquer des filtres de s\u00e9curit\u00e9 ou d'autres types de transformation avant que la r\u00e9ponse ne soit affich\u00e9e \u00e0 l'utilisateur.</li> </ul> <pre><code>graph LR;\n    A[Chat Request] --&gt; B[Inlet];\n    B --&gt; C[LLM Model];\n    C --&gt; D[Outlet];\n    D --&gt; E[Chat Response];\n\n    subgraph Filter Pipeline\n        B--&gt;C;\n        C--&gt;D;\n    end</code></pre> <p>Exemple :</p> <p>Si l'utilisateur demande \"Quelle est la m\u00e9t\u00e9o \u00e0 Paris ?\", le Filter Pipeline peut intercepter la requ\u00eate avant de l\u2019envoyer au LLM, appeler une API m\u00e9t\u00e9o pour obtenir la temp\u00e9rature, et ensuite ajouter cette information dans le message contextuel envoy\u00e9 au mod\u00e8le.</p> <p>Voici un diagramme pour illustrer le flux d'un Filter Pipeline :</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#23-concept-de-pipe","title":"2.3 Concept de Pipe","text":"<p>Un Pipe Pipeline prend enti\u00e8rement en charge le traitement des messages. Il remplace ou enrichit la mani\u00e8re dont le message est g\u00e9r\u00e9 par le LLM. Au lieu de simplement ajouter des informations autour du message, comme dans un Filter Pipeline, le Pipe Pipeline contr\u00f4le tout le processus. Cela inclut :</p> <ul> <li>Appeler diff\u00e9rents mod\u00e8les LLM (comme GPT-4, GPT-3.5, Mistral, etc.) pour r\u00e9pondre directement au message.</li> </ul> <ul> <li>Construire des workflows complexes qui peuvent int\u00e9grer de nouvelles fonctionnalit\u00e9s, comme ex\u00e9cuter du code, consulter des bases de donn\u00e9es, ou r\u00e9cup\u00e9rer des informations.</li> </ul> <ul> <li>RAG (Retrieve and Generate) : Cr\u00e9er un syst\u00e8me complet o\u00f9 les informations sont non seulement r\u00e9cup\u00e9r\u00e9es mais aussi g\u00e9n\u00e9r\u00e9es par un mod\u00e8le choisi.</li> </ul> <p>Exemple :</p> <p>Dans un Pipe Pipeline, si l'utilisateur demande \"Raconte-moi une histoire\", ce pipeline pourrait d\u00e9cider quel mod\u00e8le LLM utiliser (GPT-4, Claude, etc.) et cr\u00e9er une r\u00e9ponse en fonction du workflow configur\u00e9. </p> <p>Voici un diagramme pour illustrer le flux d'un Pipe Pipeline :</p> <pre><code>graph LR;\n    A[Chat Request] --&gt; B[Pipe];\n    B --&gt; C[Chat Response];\n\n    subgraph Pipe Pipeline\n        B;\n    end</code></pre> <p>On parle de pipelines manifold lorsque l\u2019on a un pipe qui sait g\u00e9rer plusieurs mod\u00e8les. En gros, c\u2019est la m\u00eame logique d'impl\u00e9mentation, mais le LLM utilis\u00e9 pour le chat va diff\u00e9rer. Un peu plus bas, j'ai impl\u00e9ment\u00e9 un pipe qui sert de ChatGPT, o\u00f9 je peux choisir quel mod\u00e8le utiliser entre GPT-3.5, GPT-4, ou GPT-mini.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#24-differences","title":"2.4. Diff\u00e9rences","text":"<p>La diff\u00e9rence principale entre un Filter Pipeline et un Pipe (ou Manifold) Pipeline repose sur le moment et la mani\u00e8re dont les donn\u00e9es sont trait\u00e9es avant ou apr\u00e8s l'appel \u00e0 un mod\u00e8le de langage (LLM).</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#3-implementation-de-pipelines","title":"3. Impl\u00e9mentation de pipelines","text":""},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#31-pipeline-simple","title":"3.1. Pipeline simple","text":"<p>Voici un exemple d'impl\u00e9mentation d'un pipeline basique, qui utilise l'API OpenAI pour r\u00e9pondre aux messages utilisateur.</p> <pre><code>from typing import List, Union, Generator, Iterator\nfrom pydantic import BaseModel\nimport os\nimport requests\n\nclass Pipeline:\n    class Valves(BaseModel):\n        OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"my-keys\")\n\n    def __init__(self):\n        self.name = \"OpenAI Pipeline GPT3.5\"\n        self.valves = self.Valves()\n\n    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -&gt; Union[str, Generator, Iterator]:\n        headers = {\"Authorization\": f\"Bearer {self.valves.OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n        payload = {**body, \"model\": model_id}\n        self._clean_payload(payload)\n\n        try:\n            response = requests.post(url=\"https://api.openai.com/v1/chat/completions\", json=payload, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            return f\"Error: {e}\"\n\n    @staticmethod\n    def _clean_payload(payload: dict):\n        keys_to_remove = {\"user\", \"chat_id\", \"title\"}\n        for key in keys_to_remove:\n            payload.pop(key, None)\n</code></pre>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#32-pipeline-manifold","title":"3.2 Pipeline manifold","text":"<p>Un pipeline manifold (multi-mod\u00e8le) permet de g\u00e9rer plusieurs mod\u00e8les d'IA en parall\u00e8le. Voici un exemple qui inclut plusieurs mod\u00e8les d'OpenAI. Desormais, je peux utiliser mon pipeline et avoir acc\u00e8s \u00e0 n'importe quel mod\u00e8le d'openai.</p> <pre><code>from typing import List, Union, Generator, Iterator\nfrom pydantic import BaseModel\nimport os\nimport requests\n\nclass Pipeline:\n    class Valves(BaseModel):\n        OPENAI_API_BASE_URL: str = \"https://api.openai.com/v1\"\n        OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n\n    def __init__(self, name: str = \"manifold: \"):\n        self.type = \"manifold\"\n        self.name = name\n        self.valves = self.Valves()\n        self.pipelines = self.get_openai_models()\n\n    def get_openai_models(self):\n        predefined_model_ids = ['gpt-4', 'gpt-3.5-turbo', 'gpt-4o-2024-08-06', 'gpt-4o-mini']\n        return [{'id': model_id, 'name': model_id} for model_id in predefined_model_ids]\n\n    def pipe(self, user_message: str, model_id: str, messages: List[dict], body: dict) -&gt; Union[str, Generator, Iterator]:\n        headers = {\"Authorization\": f\"Bearer {self.valves.OPENAI_API_KEY}\", \"Content-Type\": \"application/json\"}\n        payload = {**body, \"model\": model_id}\n        self._clean_payload(payload)\n\n        try:\n            response = requests.post(url=f\"{self.valves.OPENAI_API_BASE_URL}/chat/completions\", json=payload, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            return f\"Error: {e}\"\n\n    @staticmethod\n    def _clean_payload(payload: dict):\n        keys_to_remove = {\"user\", \"chat_id\", \"title\"}\n        for key in keys_to_remove:\n            payload.pop(key, None)\n</code></pre>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#33-pour-aller-plus-loin","title":"3.3 Pour aller plus loin :","text":"<p>Il n'y a rien de mieux que la documentation officielle. Vous y trouverez une pl\u00e9thore d'exemples d'impl\u00e9mentation de pipelines que vous pourriez personnaliser. Vous trouverez plus d'une cinquantaine d'exemples ici : exemples de pipelines</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#4-construire-votre-stack-avec-docker-compose","title":"4. Construire votre stack avec Docker Compose","text":""},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#41-motivation","title":"4.1. Motivation","text":"<p>L'une des mani\u00e8res les plus efficaces de mettre en place cette architecture est d\u2019utiliser Docker Compose. Voici un exemple de configuration pour orchestrer les services n\u00e9cessaires au fonctionnement de votre chatbot.</p> <p>Selon la documentation officielle pour installer open-webui/open-webui, une commande telle que :</p> <pre><code>docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n</code></pre> <p>est recommand\u00e9e. D'autres m\u00e9thodes d'installation sont aussi d\u00e9taill\u00e9es sur le site officiel ici.</p> <p>En utilisant cette commande, l'interface est d\u00e9j\u00e0 pr\u00eate \u00e0 l'emploi. Cependant, pour la communication avec l'UI, https://github.com/open-webui/pipelines propose une configuration simplifi\u00e9e via Docker :</p> <p>Ex\u00e9cutez le conteneur Pipelines avec la commande suivante :</p> <pre><code>docker run -d -p 9099:9099 --add-host=host.docker.internal:host-gateway -v pipelines:/app/pipelines --name pipelines --restart always ghcr.io/open-webui/pipelines:main\n</code></pre> <p>Ensuite, connectez Open WebUI :</p> <ul> <li>Allez dans Settings &gt; Connections &gt; OpenAI API dans Open WebUI.</li> <li>Configurez l'URL de l'API \u00e0 <code>http://localhost:9099</code> et la cl\u00e9 API \u00e0 <code>0p3n-w3bu!</code>. Vos pipelines devraient maintenant \u00eatre actifs.</li> </ul> <p>Cependant, la connexion n\u2019est pas toujours aussi simple \ud83d\ude05. La documentation n\u2019est pas encore optimale. Je vous conseille de bien explorer Pipelines sur le site et de comprendre comment l\u2019interaction entre les deux services devrait se faire pour am\u00e9liorer la communication.</p> <p>Parfois, des pipelines sont d\u00e9j\u00e0 disponibles et vous pouvez vous inspirer des exemples ici : Pipelines Exemples. Cependant, l\u2019int\u00e9gration est une autre affaire. Apr\u00e8s plusieurs essais, j\u2019ai r\u00e9ussi \u00e0 connecter les deux services en ajustant des variables cl\u00e9s comme REQUIREMENTS_PATH, PYTHONPATH, et d'autres, gr\u00e2ce \u00e0 des volumes de copie pour les pipelines.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#42-exemple-de-solution-avec-docker-compose","title":"4.2. Exemple de solution avec docker-compose","text":"<pre><code>services:\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    volumes:\n      - open-webui:/app/backend/data\n    ports:\n      - ${OPEN_WEBUI_PORT-3000}:8080\n    environment:\n      - WEBUI_SECRET_KEY=\n      - OPENAI_API_BASE_URL=http://pipelines:9099\n      - OPENAI_API_KEY=0p3n-w3bu!\n      - ENABLE_OLLAMA_API=false\n    extra_hosts:\n      - host.docker.internal:host-gateway\n    restart: unless-stopped\n\n  pipelines:\n    image: ghcr.io/open-webui/pipelines:main\n    container_name: pipelines\n    volumes:\n      - ./chat_pipelines/pipelines:/app/pipelines\n      - ./chat_pipelines/openwebui_utils:/app/openwebui_utils\n      - ./src/onepiece_bot:/app/onepiece_bot\n      - ./requirements.txt:/app/requirements_custom.txt\n    extra_hosts:\n      - host.docker.internal:host-gateway\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - PIPELINES_DIR=${PIPELINES_DIR}\n      - RESET_PIPELINES_DIR=${RESET_PIPELINES_DIR}\n      - PIPELINES_REQUIREMENTS_PATH=${PIPELINES_REQUIREMENTS_PATH}\n      - PYTHONPATH=/app\n    restart: unless-stopped\n    ports:\n      - 9099:9099\n\nvolumes:\n  open-webui:\n</code></pre> <p>Comme vous travaillez avec Docker, vous pouvez facilement inspecter ce qui se passe et v\u00e9rifier si tout fonctionne correctement ou non. Le service UI (OpenWebUI) fonctionne g\u00e9n\u00e9ralement tr\u00e8s bien ; cependant, il faut porter une attention particuli\u00e8re \u00e0 Pipelines. </p> <p>Pour d\u00e9boguer, vous pouvez ex\u00e9cuter une commande comme <code>docker ps</code> ou <code>docker logs pipelines</code>. Si vous utilisez Docker Desktop, vous devriez voir quelque chose de similaire \u00e0 ceci, montrant que vos deux conteneurs sont en cours d'ex\u00e9cution :</p> <p></p> <p>Les logs sont disponibles en cliquant sur les noms de chaque service :</p> <p></p> <p>Ici, on voit que tout fonctionne bien ! \ud83d\ude0a</p> <p>Pour un exemple complet d'impl\u00e9mentation de bout en bout et comment l'ex\u00e9cuter, je vous invite \u00e0 consulter mon d\u00e9p\u00f4t GitHub o\u00f9 vous trouverez des exemples et des pipelines pr\u00eats \u00e0 l'emploi : Mon d\u00e9p\u00f4t GitHub. Vous y trouverez \u00e9galement des informations sur les variables d'environnement et d'autres configurations utiles.</p>"},{"location":"2024/09/14/faire-tourner-son-chatbot-avec-une-interface-%C3%A9quivalente-%C3%A0-chatgpt-gr%C3%A2ce-%C3%A0-openwebui/#conclusion","title":"Conclusion","text":"<p>En combinant OpenWebUI et une architecture pipeline, il est possible de cr\u00e9er un UI de conversion flexible commme celui de chatgpt, tout en ayant un contr\u00f4le total sur l'interface utilisateur et la logique de traitement des requ\u00eates. Que vous soyez un passionn\u00e9 de LLM ou simplement curieux de tester, OpenWebUI avec Pipelines offre une base solide pour innover. Comme le projet est actuellment nouveau, n'hesitez pas \u00e0 suivre de pr\u00e8s leurs evolutions.</p>"},{"location":"2024/08/04/setup-emr/","title":"Setup emr","text":""},{"location":"2024/08/04/setup-emr/#comment-creer-et-configurer-un-cluster-emr-on-ec2","title":"Comment Cr\u00e9er et Configurer un Cluster  EMR on EC2","text":"<p>Cr\u00e9er un cluster EMR (Elastic MapReduce) sur EC2 peut sembler complexe, surtout si vous vous lancez pour la premi\u00e8re fois. Dans cet article, je vais vous guider \u00e0 travers chaque \u00e9tape, depuis la cr\u00e9ation des r\u00f4les jusqu\u2019\u00e0 la connexion \u00e0 JupyterHub via un tunnel SSH. Ce guide est con\u00e7u pour vous fournir des explications claires et d\u00e9taill\u00e9es afin que vous puissiez configurer votre cluster sans difficult\u00e9. Nous nous concentrerons sp\u00e9cifiquement sur EMR sur EC2. La petite histoire est que j'ai pass\u00e9 des heures \u00e0 expliquer cela \u00e0 plusieurs \u00e9tudiants int\u00e9ress\u00e9s par EMR. J'ai donc d\u00e9cid\u00e9 de r\u00e9diger cet article pour aider un plus grand nombre de personnes \u00e0 comprendre et \u00e0 configurer un cluster EMR de mani\u00e8re efficace. </p> <p>EMR ne faisant pas partie des services gratuit du compte tier, il faut prevoir entre 1 \u00e0 5 euros de facture sur aws.</p>"},{"location":"2024/08/04/setup-emr/#1-creer-un-role-emr-avec-les-permissions-necessaires","title":"1. Cr\u00e9er un R\u00f4le EMR avec les Permissions N\u00e9cessaires","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-est-ce-important","title":"Pourquoi est-ce important?","text":"<p>Avant de pouvoir cr\u00e9er un cluster EMR, vous devez configurer les r\u00f4les et permissions n\u00e9cessaires pour permettre \u00e0 Amazon Web Services (AWS) de g\u00e9rer votre cluster en toute s\u00e9curit\u00e9. Ces r\u00f4les permettent \u00e0 votre cluster d\u2019interagir avec d'autres services AWS comme S3, EC2, et EMR lui-m\u00eame.</p>"},{"location":"2024/08/04/setup-emr/#etape-1-creation-dun-role-emr-pour-ec2","title":"\u00c9tape 1: Cr\u00e9ation d'un R\u00f4le EMR pour EC2","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console IAM:    Connectez-vous \u00e0 votre compte AWS et rendez-vous sur la console IAM. IAM (Identity and Access Management) est l'endroit o\u00f9 vous g\u00e9rez les permissions et les r\u00f4les pour vos services AWS.</p> </li> <li> <p>Cr\u00e9er un nouveau r\u00f4le:</p> <ul> <li>Cliquez sur R\u00f4les dans le menu de gauche, puis sur Cr\u00e9er un r\u00f4le.</li> <li>S\u00e9lectionnez EMR comme type de r\u00f4le, puis choisissez EC2 comme service. Cela signifie que vous cr\u00e9ez un r\u00f4le qui sera utilis\u00e9 par les instances EC2 dans votre cluster EMR.</li> </ul> </li> <li> <p>Ajouter les politiques de permissions:</p> <ul> <li>AmazonS3FullAccess : Permet \u00e0 votre cluster de lire et d\u2019\u00e9crire dans des buckets S3, essentiel pour stocker les donn\u00e9es et les journaux de votre cluster.</li> <li>AmazonEC2FullAccess : Permet \u00e0 EMR de g\u00e9rer les instances EC2 (d\u00e9marrage, arr\u00eat, configuration).</li> <li>AmazonElasticMapReduceFullAccess : Donne \u00e0 EMR un acc\u00e8s complet pour g\u00e9rer toutes les op\u00e9rations li\u00e9es \u00e0 votre cluster.</li> </ul> </li> </ol> <p>Lorsque vous configurerez votre cluster, s\u00e9lectionnez ce r\u00f4le sous EC2 instance profile pour permettre \u00e0 votre cluster d\u2019utiliser ces permissions.</p> <p></p> <p>Astuce: Les permissions sont cruciales pour la s\u00e9curit\u00e9. Accordez les permissions minimales n\u00e9cessaires pour accomplir vos t\u00e2ches.</p>"},{"location":"2024/08/04/setup-emr/#2-creer-une-cle-ssh-pour-ec2","title":"2. Cr\u00e9er une Cl\u00e9 SSH pour EC2","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-en-avez-vous-besoin","title":"Pourquoi en avez-vous besoin?","text":"<p>Une cl\u00e9 SSH est n\u00e9cessaire pour se connecter \u00e0 distance aux instances EC2 de votre cluster. Cette connexion vous permet d'administrer les n\u0153uds du cluster, d\u2019installer des logiciels suppl\u00e9mentaires ou de d\u00e9boguer directement sur le serveur.</p>"},{"location":"2024/08/04/setup-emr/#etape-2-creation-dune-paire-de-cles-ssh","title":"\u00c9tape 2: Cr\u00e9ation d'une Paire de Cl\u00e9s SSH","text":"<ol> <li> <p>Acc\u00e9der \u00e0 la console EC2:    Dans la console AWS, acc\u00e9dez \u00e0 la section EC2 pour g\u00e9rer vos instances et autres ressources li\u00e9es \u00e0 EC2.</p> </li> <li> <p>Cr\u00e9er une paire de cl\u00e9s:</p> <ul> <li>Dans le menu de gauche, s\u00e9lectionnez Key Pairs sous la section Network &amp; Security.</li> <li>Cliquez sur Create Key Pair.</li> <li>Donnez un nom \u00e0 votre cl\u00e9 et choisissez le format souhait\u00e9 (PEM pour Linux/Mac, PPK pour Windows avec PuTTY).</li> <li>T\u00e9l\u00e9chargez la cl\u00e9. Cette cl\u00e9 vous permettra de vous connecter en SSH \u00e0 vos instances EC2.</li> </ul> </li> </ol> <p>Note: Gardez cette cl\u00e9 en s\u00e9curit\u00e9. Si vous la perdez, vous ne pourrez pas vous reconnecter \u00e0 votre instance.</p>"},{"location":"2024/08/04/setup-emr/#3-creation-et-configuration-du-cluster-emr","title":"3. Cr\u00e9ation et Configuration du Cluster EMR","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-cette-etape-est-elle-cruciale","title":"Pourquoi cette \u00e9tape est-elle cruciale?","text":"<p>La configuration du cluster est le c\u0153ur de l\u2019op\u00e9ration. C\u2019est ici que vous choisissez les logiciels que vous voulez installer, les types d'instances \u00e0 utiliser, et les options de mise en r\u00e9seau.</p>"},{"location":"2024/08/04/setup-emr/#etape-3-creer-un-cluster-emr","title":"\u00c9tape 3: Cr\u00e9er un Cluster EMR","text":"<ol> <li> <p>Cr\u00e9er le cluster:</p> <ul> <li>Allez sur la console EMR et cliquez sur Create cluster.</li> <li>Donnez un nom \u00e0 votre cluster. Par exemple, \"Cluster-DataScience\".</li> </ul> </li> <li> <p>Choisir les composants logiciels:</p> <ul> <li>EMR propose plusieurs logiciels que vous pouvez installer directement lors de la cr\u00e9ation du cluster. Pour un environnement de science des donn\u00e9es, incluez TensorFlow, JupyterHub (pour g\u00e9rer vos notebooks), et Zeppelin.  </li> </ul> </li> <li> <p>Choisir les types d'instances:</p> <ul> <li>Master node: Le n\u0153ud ma\u00eetre coordonne toutes les t\u00e2ches du cluster. Une instance <code>m5.xlarge</code> est un bon choix pour \u00e9quilibrer co\u00fbt et performance.</li> <li>Core nodes: Ces n\u0153uds ex\u00e9cutent les t\u00e2ches. Vous pouvez opter pour des instances plus \u00e9conomiques si vous avez un budget serr\u00e9.</li> </ul> </li> <li> <p>Configurer le volume EBS:</p> <ul> <li>Par d\u00e9faut, EMR vous propose un volume racine EBS pour chaque instance. Vous pouvez g\u00e9n\u00e9ralement accepter cette valeur par d\u00e9faut.</li> </ul> </li> <li> <p>Configurer le r\u00e9seau (VPC et sous-r\u00e9seau):</p> <ul> <li>Assurez-vous de s\u00e9lectionner un sous-r\u00e9seau public pour acc\u00e9der \u00e0 votre cluster via SSH et Web. Si vous n'avez pas de VPC disponible, cr\u00e9ez-le ainsi:  </li> </ul> </li> <li> <p>Ajouter des actions Bootstrap:</p> <ul> <li>Les actions Bootstrap sont des scripts ex\u00e9cut\u00e9s sur chaque n\u0153ud lors du d\u00e9marrage. Cr\u00e9ez un fichier avec l'extension <code>.sh</code> que vous garderez dans votre S3 et fournissez-le \u00e0 EMR via son URI :    <pre><code>#!/bin/bash -xe\nsudo pip install -U \\\n  awscli            \\\n  boto3             \\\n  wheel             \\\n  s3fs              \\\n  fsspec            \\\n  pyarrow\nsudo pip install -U pandas pillow scikit-learn tensorflow\n</code></pre></li> <li>Ce script installe des biblioth\u00e8ques Python couramment utilis\u00e9es dans le traitement de donn\u00e9es. Il est essentiel de r\u00e9aliser ces actions \u00e0 cette \u00e9tape pour que les packages soient install\u00e9s sur l'ensemble des machines du cluster, et non uniquement sur le driver (comme ce serait le cas si vous ex\u00e9cutiez ces commandes directement dans le notebook JupyterHub ou dans la console EMR).</li> </ul> </li> <li> <p>Configurer la persistance des notebooks Jupyter:</p> <ul> <li>Configurez JupyterHub pour qu\u2019il sauvegarde automatiquement vos notebooks sur un bucket S3 en ajoutant ce param\u00e8tre dans Software settings:    <pre><code>[\n    {\n        \"Classification\": \"jupyter-s3-conf\",\n        \"Properties\": {\n            \"s3.persistence.enabled\": \"true\",\n            \"s3.persistence.bucket\": \"MyJupyterBackups\"\n        }\n    }\n]\n</code></pre></li> </ul> </li> <li> <p>Configurer les journaux du cluster:</p> <ul> <li>Assurez-vous que les journaux du cluster sont publi\u00e9s sur S3. Cela vous aidera \u00e0 diagnostiquer tout probl\u00e8me apr\u00e8s coup.</li> </ul> </li> </ol> <p>Rappel: La cr\u00e9ation du cluster peut prendre entre 5 et 10 minutes, et vous commencerez \u00e0 \u00eatre factur\u00e9 d\u00e8s que le cluster d\u00e9marre.</p> <p>Une fois pr\u00eat, vous verrez vos applications list\u00e9es, mais elles ne seront pas encore accessibles. </p>"},{"location":"2024/08/04/setup-emr/#4-configuration-du-tunnel-ssh-vers-le-nud-maitre","title":"4. Configuration du Tunnel SSH vers le N\u0153ud Ma\u00eetre","text":""},{"location":"2024/08/04/setup-emr/#pourquoi-configurer-un-tunnel-ssh","title":"Pourquoi configurer un tunnel SSH?","text":"<p>Le n\u0153ud ma\u00eetre de votre cluster est prot\u00e9g\u00e9 derri\u00e8re un pare-feu, et les applications comme JupyterHub ne sont accessibles que via le r\u00e9seau local du cluster. Le tunnel SSH vous permet de contourner cette restriction en cr\u00e9ant un pont s\u00e9curis\u00e9 entre votre machine locale et le n\u0153ud ma\u00eetre.</p>"},{"location":"2024/08/04/setup-emr/#etape-4-creation-du-tunnel-ssh","title":"\u00c9tape 4: Cr\u00e9ation du Tunnel SSH","text":""},{"location":"2024/08/04/setup-emr/#41-ouverture-du-port-22","title":"4.1 Ouverture du Port 22","text":"<ol> <li> <p>Configurer les autorisations:</p> <ul> <li>Sur la console EC2, allez dans Security Groups.</li> <li>Modifiez le groupe de s\u00e9curit\u00e9 attach\u00e9 au n\u0153ud ma\u00eetre de votre cluster.</li> <li>Ajoutez une r\u00e8gle pour autoriser les connexions SSH (port 22) depuis n'importe quelle IP, ou restreignez-la \u00e0 votre IP pour plus de s\u00e9curit\u00e9. Modifiez le groupe de s\u00e9curit\u00e9 associ\u00e9 aux <code>master</code>. Il ressemblera \u00e0 ceci :  </li> </ul> <p>\u00c0 la fin, vous devriez avoir des r\u00e8gles qui ressemblent \u00e0 ceci :  </p> </li> </ol>"},{"location":"2024/08/04/setup-emr/#42-etablissement-du-tunnel-ssh","title":"4.2 \u00c9tablissement du Tunnel SSH","text":"<ol> <li> <p>R\u00e9cup\u00e9rer les informations de connexion:</p> <ul> <li>Sur la console EMR, acc\u00e9dez \u00e0 l\u2019onglet Summary de votre cluster.</li> <li>Cliquez sur Enable Web Connection pour g\u00e9n\u00e9rer la commande SSH n\u00e9cessaire pour \u00e9tablir le tunnel.</li> </ul> </li> <li> <p>Utilisation de PuTTY pour Windows:</p> <ul> <li>Si vous \u00eates sur Windows, utilisez PuTTY pour \u00e9tablir le tunnel SSH en configurant les param\u00e8tres avec votre cl\u00e9 <code>.ppk</code>.</li> <li>Suivez les \u00e9tapes indiqu\u00e9es sur la console AWS.  </li> </ul> </li> </ol> <p>Astuce: Si vous utilisez Linux ou macOS, ex\u00e9cutez simplement la commande SSH dans votre terminal.</p> <p>Si votre connexion SSH a r\u00e9ussi, vous verrez cette fen\u00eatre : </p>"},{"location":"2024/08/04/setup-emr/#43-configuration-de-switchomega-ou-foxyproxy","title":"4.3 Configuration de SwitchOmega ou FoxyProxy","text":"<ol> <li> <p>Installation de SwitchyOmega:</p> <ul> <li>Pour acc\u00e9der aux applications via le tunnel SSH, configurez votre navigateur pour utiliser ce tunnel. Installez l\u2019extension SwitchyOmega pour Chrome et configurez-la en suivant les instructions d\u00e9taill\u00e9es dans la documentation AWS.</li> </ul> <p>Si cela vous semble compliqu\u00e9, r\u00e9f\u00e9rez-vous aux vid\u00e9os YouTube suivantes :</p> <ul> <li>Vid\u00e9o 1 (\u00c0 partir de 5:10)</li> <li>Vid\u00e9o 2 (\u00c0 partir de 8:00)</li> </ul> </li> </ol> <p>Note: Parfois, il peut \u00eatre n\u00e9cessaire de red\u00e9marrer le navigateur ou attendre quelques minutes pour que l'extension fonctionne correctement.</p>"},{"location":"2024/08/04/setup-emr/#5-connexion-a-jupyterhub","title":"5. Connexion \u00e0 JupyterHub","text":""},{"location":"2024/08/04/setup-emr/#etape-5-acceder-a-jupyterhub","title":"\u00c9tape 5: Acc\u00e9der \u00e0 JupyterHub","text":"<ol> <li> <p>Connexion \u00e0 JupyterHub:</p> <ul> <li>Une fois le tunnel SSH configur\u00e9 et votre navigateur param\u00e9tr\u00e9, vous devriez voir JupyterHub list\u00e9 parmi les applications disponibles sur votre cluster EMR.</li> <li>Cliquez sur JupyterHub pour ouvrir l\u2019interface de connexion.  </li> </ul> <ul> <li>Utilisez les identifiants par d\u00e9faut (login: <code>jovyan</code>, password: <code>jupyter</code>) pour vous connecter.</li> </ul> </li> </ol>"},{"location":"2024/08/04/setup-emr/#conclusion","title":"Conclusion","text":"<p>F\u00e9licitations! Vous avez maintenant un cluster EMR pleinement op\u00e9rationnel, avec JupyterHub configur\u00e9 et accessible via un tunnel SSH s\u00e9curis\u00e9. Vous \u00eates pr\u00eat \u00e0 tirer parti de la puissance de calcul d'Amazon EMR pour vos projets de science des donn\u00e9es, d\u2019analyse, ou de machine learning.</p> <p>Si vous avez des questions ou des difficult\u00e9s, n'h\u00e9sitez pas \u00e0 me contacter. Bonne chance avec votre cluster EMR et n'oubliez pas de l'\u00e9teindre une fois termin\u00e9!</p>"},{"location":"2024/09/01/tests-for-data-scientists/","title":"Tests for data scientists","text":""},{"location":"2024/09/01/tests-for-data-scientists/#parlons-de-tests-pour-data-scientists-mlops","title":"Parlons de tests pour data scientists - MLOPS","text":"<p>Dans cet article, je vais partager avec vous une pratique qui a transform\u00e9 ma carri\u00e8re de data scientist end-to-end : tester son code. C'est une \u00e9tape cruciale dans l'adoption des bonnes pratiques MLOps, mais qui est souvent n\u00e9glig\u00e9e par beaucoup de data scientists. Je vais explorer les raisons pour lesquelles les tests sont souvent peu utilis\u00e9s dans ce domaine et comment vous pouvez int\u00e9grer des tests efficaces dans le workflow de vos projets de data science. Vous trouverez \u00e9galement un tutoriel pratique pour vous aider \u00e0 d\u00e9marrer, en utilisant des outils comme pytest, beartype, pandera, unittest, ou pydantic.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#1-les-types-de-tests","title":"1. Les types de tests","text":"<p>Les tests en d\u00e9veloppement proviennent du g\u00e9nie logiciel. On imagine souvent une pyramide avec diff\u00e9rents niveaux de tests, chacun ayant son r\u00f4le pour garantir que le logiciel fonctionne correctement. Cependant, la mani\u00e8re dont les tests sont abord\u00e9s d\u00e9pend beaucoup des t\u00e2ches sp\u00e9cifiques que vous r\u00e9alisez en tant que data scientist. Voici un aper\u00e7u des principaux types de tests, illustr\u00e9s dans le contexte d'un projet de data science.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-unitaires","title":"Tests unitaires","text":"<p>Les tests unitaires sont \u00e0 la base de la pyramide des tests. Ils sont utilis\u00e9s pour v\u00e9rifier chaque fonction ou composant de mani\u00e8re isol\u00e9e. En data science, cela peut inclure :</p> <ul> <li>V\u00e9rifier le format et le type des donn\u00e9es (par exemple, s'assurer qu'une colonne est toujours un entier).</li> <li>Tester les param\u00e8tres d'un mod\u00e8le pour valider leur conformit\u00e9.</li> <li>Contr\u00f4ler les variables d'entr\u00e9e pour d\u00e9tecter les anomalies ou les valeurs hors des plages attendues.</li> <li>Tester les performances d\u2019un mod\u00e8le sur des jeux de donn\u00e9es sp\u00e9cifiques.</li> </ul> <p>Ces tests sont particuli\u00e8rement utiles lorsque vous d\u00e9veloppez des fonctions de transformation de donn\u00e9es ou des algorithmes personnalis\u00e9s. Ils permettent de v\u00e9rifier la stabilit\u00e9 du code \u00e0 chaque changement et d'\u00e9viter des erreurs fr\u00e9quentes, comme un format de donn\u00e9es incorrect.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-dintegration","title":"Tests d\u2019int\u00e9gration","text":"<p>Les tests d'int\u00e9gration s'assurent que diff\u00e9rents composants fonctionnent correctement ensemble, ce qui est essentiel dans les projets o\u00f9 plusieurs parties du syst\u00e8me doivent collaborer. Par exemple, dans un pipeline de donn\u00e9es complet (de la collecte au pr\u00e9traitement jusqu\u2019au mod\u00e8le et \u00e0 la production des r\u00e9sultats), ces tests v\u00e9rifient que toutes les \u00e9tapes s'encha\u00eenent sans erreur. Souvent, ils sont int\u00e9gr\u00e9s dans un cadre de CI/CD (Int\u00e9gration et D\u00e9ploiement Continus) pour automatiser et garantir la coh\u00e9rence des tests.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-systemes","title":"Tests syst\u00e8mes","text":"<p>Les tests syst\u00e8mes, ou tests \"bo\u00eete noire\", \u00e9valuent le logiciel dans son ensemble dans des sc\u00e9narios d'utilisation r\u00e9els. Ils sont particuli\u00e8rement utiles pour les projets qui aboutissent \u00e0 des applications pr\u00eates \u00e0 \u00eatre utilis\u00e9es par des utilisateurs finaux. </p> <ul> <li>Exemple : V\u00e9rifier que votre mod\u00e8le d\u00e9ploy\u00e9 sur une application web fait des pr\u00e9dictions et renvoie les r\u00e9sultats correctement.</li> </ul> <p>Ces tests sont souvent effectu\u00e9s par des \u00e9quipes ind\u00e9pendantes pour garantir une \u00e9valuation impartiale de l'ensemble du syst\u00e8me.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#tests-dacceptation","title":"Tests d\u2019acceptation","text":"<p>Au sommet de la pyramide se trouvent les tests d\u2019acceptation, qui v\u00e9rifient que le produit final r\u00e9pond aux attentes du client ou de l\u2019utilisateur. Ces tests sont g\u00e9n\u00e9ralement r\u00e9alis\u00e9s par le Product Owner, le client, ou le sponsor du projet, plut\u00f4t que par les d\u00e9veloppeurs. </p> <ul> <li>Exemple : Pour une application de recommandation, s'assurer que les suggestions fournies aux utilisateurs sont pertinentes et align\u00e9es avec les crit\u00e8res d\u00e9finis par le client.</li> </ul> <p>Ces tests sont essentiels dans les projets avec des parties prenantes d\u00e9finies (client, Product Owner, sponsor, expert m\u00e9tier) qui doivent valider que le produit r\u00e9pond bien aux besoins sp\u00e9cifi\u00e9s.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#2-pourquoi-les-tests-sont-essentiels-en-data-science","title":"2. Pourquoi les tests sont essentiels en data science","text":"<p>Faire des tests ou non d\u00e9pend vraiment de tes t\u00e2ches quotidiennes. Si tu es en exploration de donn\u00e9es ou en phase de recherche, tu pourrais moins sentir le besoin d'automatiser des tests rigoureux. Mais d\u00e8s que tu commences \u00e0 mettre des choses en production, les tests deviennent indispensables. Voil\u00e0 pourquoi :</p> <ul> <li>Gagner du temps \u00e0 long terme : Attraper les erreurs t\u00f4t \u00e9vite de devoir corriger des bugs co\u00fbteux apr\u00e8s coup.</li> <li>Assurer la fiabilit\u00e9 : Les tests automatis\u00e9s garantissent que les mod\u00e8les continuent de fonctionner correctement, m\u00eame apr\u00e8s des mises \u00e0 jour du code.</li> <li>Documentation vivante : Des tests bien r\u00e9dig\u00e9s peuvent te servir de documentation. Ils montrent exactement comment chaque partie du syst\u00e8me est cens\u00e9e fonctionner.</li> </ul> <p>La documentation est souvent n\u00e9glig\u00e9e, mais c'est crucial car les data scientists ou les d\u00e9veloppeurs bougent beaucoup entre les entreprises. Si le code est bien document\u00e9 et test\u00e9, la personne qui le reprend pourra plus facilement s'approprier le projet.</p> <p>Bref, l'id\u00e9e, c'est de s'adapter \u00e0 ton contexte. Si tu codes r\u00e9guli\u00e8rement des fonctions critiques, mets en place des tests unitaires et d'int\u00e9gration. Si tu d\u00e9veloppes un produit pour des utilisateurs finaux, pense aux tests syst\u00e8mes et d'acceptation. Si tu es un data scientist qui fait des \u00e9tudes ad hoc ou travaille \u00e9troitement avec des m\u00e9tiers, alors les tests d'acceptation suffiront probablement. Mais si tu es un ML engineer ou un AI engineer, les tests deviennent une necessit\u00e9.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#3-pourquoi-les-data-scientists-negligent-ils-les-tests","title":"3. Pourquoi les data scientists n\u00e9gligent-ils les tests ?","text":"<p>En data science, les tests sont souvent n\u00e9glig\u00e9s pour plusieurs raisons :</p> <ul> <li>Manque d'exp\u00e9rience en ing\u00e9nierie logicielle : Beaucoup de data scientists viennent d\u2019un background acad\u00e9mique (stats, maths, physique, data) sans formation solide en d\u00e9veloppement logiciel.</li> <li>Pression pour produire des r\u00e9sultats rapides : Les entreprises poussent souvent pour des prototypes rapides, ce qui laisse peu de temps pour les tests rigoureux.</li> <li>Mauvaise compr\u00e9hension de la valeur des tests : Les data scientists ne voient pas toujours comment les tests peuvent faciliter leur travail en production.</li> <li>Data scientist \u2260 d\u00e9veloppeur : M\u00eame si c'est un d\u00e9bat, je fais partie de ceux qui pensent qu'un data scientist n'a pas n\u00e9cessairement besoin d'\u00eatre un bon d\u00e9veloppeur. N\u00e9anmoins, quand tu veux faire du ML engineering ou du end-to-end data science, coder en respectant les bonnes pratiques devient une n\u00e9cessit\u00e9. N'oublie pas que l'une des premi\u00e8res d\u00e9finitions d'un data scientist a \u00e9t\u00e9 : \"I think of data scientists as knowing more about statistics than computer scientists and more about computer science than statisticians.\" \u2014 Michael O'Connell.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#4-lexperience-et-les-bonnes-pratiques","title":"4. L'exp\u00e9rience et les bonnes pratiques","text":"<p>En production, les erreurs co\u00fbtent cher. Les tests te permettent d'\u00e9viter des bugs r\u00e9p\u00e9titifs et d'\u00e9conomiser du temps. En lisant des tests bien \u00e9crits, tu comprends le fonctionnement du syst\u00e8me sans documentation suppl\u00e9mentaire.</p> <p>Les d\u00e9veloppeurs se bonifient avec les projets qu'ils font et gr\u00e2ce \u00e0 leurs collaborations avec d'autres d\u00e9veloppeurs. Si je fais des tests aujourd'hui, c'est gr\u00e2ce \u00e0 ces seniors ou d\u00e9veloppeurs qui ne juraient que par les tests et m'ont oblig\u00e9 \u00e0 le faire avant de valider mes <code>merge merquest</code>. </p> <p>Bonnes pratiques :</p> <ul> <li>Commencez par les tests unitaires simples.</li> <li>N\u2019essayez pas de tout couvrir \u00e0 100 % d\u00e8s le d\u00e9but. Concentrez-vous sur les parties critiques.</li> <li>Ne testez pas les biblioth\u00e8ques tierces ; concentrez-vous sur votre code.</li> <li>Automatisez les tests dans votre pipeline CI/CD ou avec des hooks Git pour d\u00e9tecter les erreurs avant de d\u00e9ployer.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#5-les-tests-en-data-science-et-les-outils-utiles","title":"5. Les tests en data Science et les outils utiles","text":"<p>Pour les data scientists, il existe plusieurs outils pour \u00e9crire des tests :</p> <ul> <li>Pandera : Pour valider les DataFrames Pandas (eh oui, les data scientists adorent Pandas !).</li> <li>Pytest ou Unittest : Pour \u00e9crire des tests unitaires et d'int\u00e9gration.</li> <li>Beartype ou Pydantic : Pour la validation de types dans vos fonctions.</li> </ul>"},{"location":"2024/09/01/tests-for-data-scientists/#1-exemple-de-tests-implicites-avec-pandera","title":"1. Exemple de tests implicites avec pandera","text":"<p>Lorsque l'on travaille avec des DataFrames, il est bien connu que la gestion des types dans pandas peut \u00eatre probl\u00e9matique, en particulier avec le type <code>object</code>. Pandera est une biblioth\u00e8que qui facilite la validation des DataFrames en permettant de d\u00e9finir des contraintes sur les colonnes et les sch\u00e9mas attendus. Pour utiliser Pandera pour valider la structure d'un DataFrame, voici un exemple simple. Vous pouvez consulter la documentation officielle pour plus de d\u00e9tails : Documentation de Pandera. L'argument <code>strict</code> permet de valider les colonnes sp\u00e9cifi\u00e9es dans le DataFrame.</p> <pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera import Column, DataFrameSchema, Check\n\n# D\u00e9finition de schema\nschema = DataFrameSchema({\n    \"country\": Column(str, Check(lambda x: len(x) &gt; 0)),  # Nom de pays non vide\n    \"population\": Column(int, Check(lambda x: x &gt;= 0)),   # Population doit \u00eatre un entier non n\u00e9gatif\n    \"superficies\": Column(float, Check(lambda x: x &gt; 0)) , \n\n},strict='filter')\n\n# Exemple de DataFrame qui respecte le sch\u00e9ma\nvalid_df = pd.DataFrame({\n    \"country\": [\"France\", \"Burkina Faso\"],\n    \"population\": [67000000, 20000000],\n    \"superficies\": [551695.0, 274200.0],\n    \"Category\" :[\"A\", \"C\"]\n})\n\n# Exemple de DataFrame qui ne respecte  pas le sch\u00e9ma\ninvalid_df = pd.DataFrame({\n    \"country\": [\"France\", \"Burkina Faso\"],\n    \"population\": [\"67 millions\", 20000000],  # Erreur : population devrait \u00eatre un entier\n    \"superficies\": [551695.0, 274200.0],\n     \"Category\" :[\"A\", \"C\"]\n\n})\n\n@pa.check_input(schema)\ndef process_data(df):\n    return df.assign(density=df[\"population\"]/df[\"superficies\"])\n</code></pre> <p>En appliquant ce code, vous obtenez : </p> <p> On peut facilement voir que la fonction a <code>raise</code> une erreur de schema d\u00e8s l'appel.  Combiner Pandera avec Pytest permet de valider les donn\u00e9es. En data science, il est fr\u00e9quent de faire passer des DataFrames en entr\u00e9e de fonction, donc il est pr\u00e9f\u00e9rable de valider les types en entr\u00e9e et d'\u00e9viter tout probl\u00e8me.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#2-exemple-de-validation-des-types-avec-beartype","title":"2. Exemple de validation des types avec beartype","text":"<p>Beartype peut \u00eatre utilis\u00e9 pour v\u00e9rifier les types de donn\u00e9es \u00e0 l'ex\u00e9cution.  Beartype est un validateur de types Python qui v\u00e9rifie que les valeurs pass\u00e9es \u00e0 une fonction correspondent bien aux annotations de type d\u00e9finies. C'est une excellente mani\u00e8re de renforcer la robustesse de ton code sans avoir \u00e0 \u00e9crire de tests manuels pour chaque param\u00e8tre de fonction.</p> <p>La documentation est ici : Beartype. Vous pourriez utiliser Pydantic \u00e0 la place :</p> <pre><code>from beartype import beartype\n\n@beartype\ndef add_numbers(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p></p> <p>Comme on le voit, lorsque le type est viol\u00e9, la fonction l\u00e8ve une erreur.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#6-implementation-de-tests-plus-completes","title":"6. Impl\u00e9mentation de tests plus compl\u00e8tes","text":""},{"location":"2024/09/01/tests-for-data-scientists/#1-organisation-du-projet","title":"1. Organisation du projet","text":"<p>Pour un projet de data science orient\u00e9 MLOps, une organisation claire et structur\u00e9e est essentielle pour faciliter le d\u00e9veloppement, les tests, et la maintenance du code. Un bon sch\u00e9ma de projet permet non seulement de rendre le code plus lisible, mais aussi de simplifier l'int\u00e9gration des tests \u00e0 chaque \u00e9tape du pipeline, depuis le pr\u00e9traitement des donn\u00e9es jusqu'au d\u00e9ploiement des mod\u00e8les.</p> <p>Imaginons le projet ci-dessous o\u00f9 PYTHONPATH pointe vers <code>mypackages</code> et  voici une structure typique qui peut servir de base solide :</p> <pre><code>project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 mypackages/\n\u2502       \u251c\u2500\u2500 algo.py \n\u2502       \u2514\u2500\u2500 __init__.py\n\u2502\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 __init__.py  # Optionnel mais utile pour les tests\n    \u251c\u2500\u2500 test_algo.py\n\u2514\u2500\u2500 test-data/\n    \u251c\u2500\u2500 profil_1.json\n    \u251c\u2500\u2500 profil_2.json\n    \u251c\u2500\u2500 profil_3.json\n    \u251c\u2500\u2500 expected_profil_1.json\n    \u251c\u2500\u2500 expected_profil_2.json\n    \u2514\u2500\u2500 expected_profil_3.json\n</code></pre>"},{"location":"2024/09/01/tests-for-data-scientists/#2-simple-test-unitaire","title":"2. Simple test unitaire","text":"<p>Le fichier <code>test_algo1.py</code> contient les lignes de test unitaire suivantes :</p> <pre><code>from mypackages.algo import Algorithm_1\n\ndef test_algorithm_1():\n    input_data = [1, 2, 3]\n    algorithm_1 = Algorithm_1()\n    result = algorithm_1.predict(input_data)\n    expected = [1, 4, 9]\n    assert result == expected\n</code></pre> <p>Avec la commande <code>pytest</code>, vous pouvez ex\u00e9cuter ce test unitaire. L'organisation du projet peut varier, chacun s'organise comme il le souhaite.</p>"},{"location":"2024/09/01/tests-for-data-scientists/#3-plusieurs-tests-unitaires-en-un-avec-pytestmarkparametrize","title":"3. Plusieurs tests unitaires en Un avec <code>pytest.mark.parametrize</code>","text":"<p><code>pyest.mark.parametrize</code> : C'est une fonctionnalit\u00e9 de Pytest qui permet de tester une m\u00eame fonction avec diff\u00e9rents ensembles de donn\u00e9es. Cela t'aide \u00e0 v\u00e9rifier que ton code fonctionne correctement avec une large vari\u00e9t\u00e9 d'entr\u00e9es, et c'est particuli\u00e8rement utile en data science o\u00f9 les variations de donn\u00e9es sont fr\u00e9quentes. Dans ce paragraphe, je mets en place un test en utilisant le decorateur pyest.mark.parametrize.</p> <p>Voici le d\u00e9tail de <code>test_algo2.py</code> : <pre><code>import pytest\nimport os\nimport json\nimport pandas as pd\nfrom mypackages.algo import Algorithm_2  \nfrom loguru import logger\n\ndef get_data_path(filename: str) -&gt; str:\n    return os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"test-data\", filename))\n\ndef load_data(filename: str) -&gt; pd.DataFrame:\n    with open(get_data_path(filename), \"rb\") as f:\n        json_file = json.load(f)\n    return pd.json_normalize(json_file)\n\n@pytest.mark.parametrize(\n    (\"input_file\", \"expected_file\"),\n    [\n        (\"profil_2.json\", \"expected_profil_2.json\"),\n        (\"profil_1.json\", \"expected_profil_1.json\"),\n        (\"profil_3.json\", \"expected_profil_3.json\"),\n    ],\n)\ndef test_algorithm_2(input_file, expected_file):\n    logger.info(f\"testing {input_file}\")\n\n    df_input = load_data(input_file)\n    df_expected = load_data(expected_file) \n\n    algorithm_2 = Algorithm_2()  \n    result = algorithm_2.predict(df_input)  \n\n    pd.testing.assert_frame_equal(result, df_expected)\n</code></pre></p> <p>Comme par magie, en ex\u00e9cutant la commande <code>pytest</code>, tout passe comme vous pouvez le voir dans l'image ci-apr\u00e8s : </p>"},{"location":"2024/09/01/tests-for-data-scientists/#conclusion","title":"Conclusion","text":"<p>Les tests sont une comp\u00e9tence essentielle pour les data scientists, surtout ceux qui visent \u00e0 mettre leurs mod\u00e8les en production ou livrent leur projects \u00e0 d'autres dev. Commencez petit, concentrez-vous sur les parties critiques, et faites des tests une habituddans vos projets. Les outils comme Pandera, Pytest, Beartype, et Pydantic sont l\u00e0 pour vous aider \u00e0 am\u00e9liorer la qualit\u00e9 et la fiabilit\u00e9 de votre code. C'est un investissement qui paie \u00e0 long terme, vous \u00e9vitant de nombreux soucis et vous permettant de livrer des mod\u00e8les plus robustes et maintenables.</p> <p>Pr\u00eat \u00e0 ajouter des tests \u00e0 vos projets de data science ? Commen\u00e7ons d\u00e8s maintenant !</p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/","title":"Comprendre les Transformers dans l\u2019IA G\u00e9n\u00e9rative","text":"<p>Quand on parle d\u2019IA aujourd\u2019hui, les Transformers reviennent souvent dans la conversation. Pourquoi\u202f? Parce qu\u2019ils ont r\u00e9volutionn\u00e9 la fa\u00e7on dont on traite le texte. Avec leur m\u00e9canisme d\u2019attention, ils sont capables d\u2019aller vite, de traiter les mots dans leur contexte et surtout de produire des r\u00e9sultats impressionnants dans des domaines comme la traduction, la g\u00e9n\u00e9ration de texte, et bien plus encore.</p> <p>Dans cet article, on va d\u00e9cortiquer les architectures principales des Transformers : l\u2019Encodeur-D\u00e9codeur, l\u2019Autoregressif et quelques variantes importantes. Alors, installe-toi bien, on plonge dans le vif du sujet. \ud83d\ude09</p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#i-architecture-encodeur-decodeur","title":"I. Architecture Encodeur-D\u00e9codeur","text":"<p>L\u2019Encodeur-D\u00e9codeur, c\u2019est l\u2019architecture de base qui a marqu\u00e9 l\u2019arriv\u00e9e des Transformers dans le traitement automatique du langage (NLP). Tu la retrouves dans des mod\u00e8les comme T5 ou encore dans l'article fondateur Attention is All You Need.</p> <p>Tout repose sur deux couches compl\u00e9mentaires (un encodeur  et un decodeur). Si vous \u00eates d\u00e9j\u00e0 familiers au deep learning alors pensez \u00e0 auto-encodeurs , c'est plus simple.</p> <ul> <li>Encodeur : Son r\u00f4le, c\u2019est de lire le texte d\u2019entr\u00e9e et d\u2019en comprendre le contexte. Pour cela, chaque mot passe par une couche d\u2019attention (qu\u2019on appelle self-attention), qui permet au mod\u00e8le de se concentrer sur les parties importantes de la phrase. Ensuite, les r\u00e9sultats traversent un r\u00e9seau dense (feed-forward) pour produire des repr\u00e9sentations riches du texte.  </li> <li>D\u00e9codeur : L\u00e0, c\u2019est un peu diff\u00e9rent. En plus de la couche self-attention, il y a une couche sp\u00e9ciale appel\u00e9e attention encodeur-d\u00e9codeur. Cette derni\u00e8re aide le d\u00e9codeur \u00e0 se concentrer sur les bonnes parties du texte d\u2019entr\u00e9e pendant qu\u2019il g\u00e9n\u00e8re les mots de sortie.  </li> </ul> <p>Voici un sch\u00e9ma pour bien visualiser tout \u00e7a : </p> <p>Un point \u00e0 noter : Les blocs de l\u2019encodeur et du d\u00e9codeur sont souvent r\u00e9p\u00e9t\u00e9s plusieurs fois (N fois). Par exemple, avec N = 6, tu as 6 encodeurs et 6 d\u00e9codeurs empil\u00e9s.</p> <p>En r\u00e9sum\u00e9, l\u2019encodeur transforme le texte d\u2019entr\u00e9e en une repr\u00e9sentation vectorielle compr\u00e9hensible pour le mod\u00e8le, et le d\u00e9codeur utilise ces vecteurs pour produire le texte final. C\u2019est comme \u00e7a qu\u2019un mod\u00e8le comme T5 arrive \u00e0 traduire ou \u00e0 r\u00e9sumer des phrases complexes.</p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#ii-architecture-autoregressive-le-cerveau-derriere-gpt","title":"II. Architecture Autoregressive : Le cerveau derri\u00e8re GPT","text":"<p>Quand on parle de g\u00e9n\u00e9ration de texte, les mod\u00e8les autoregressifs comme GPT sont les stars du moment. Contrairement \u00e0 l\u2019Encodeur-D\u00e9codeur, ici, pas d\u2019encodeur : tout repose sur un d\u00e9codeur.  </p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#fonctionnement-de-gpt","title":"Fonctionnement de GPT","text":"<p>Le principe est simple mais puissant : le mod\u00e8le g\u00e9n\u00e8re un mot \u00e0 la fois, en se basant sur les mots pr\u00e9c\u00e9dents. Chaque mot produit est ajout\u00e9 \u00e0 l\u2019entr\u00e9e pour pr\u00e9dire le suivant. Ce processus, qu\u2019on appelle autoregression, permet au mod\u00e8le de produire des textes fluides et coh\u00e9rents.</p> <p>Un exemple concret : si tu demandes \u00e0 GPT-2 de citer la premi\u00e8re loi de la robotique, il pourrait te r\u00e9pondre : \"Un robot ne peut porter atteinte \u00e0 un \u00eatre humain ni, en restant passif, permettre qu\u2019un humain subisse un dommage.\"</p> <p>Ce sch\u00e9ma illustre bien le processus autoregressif : </p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#modeles-similaires","title":"Mod\u00e8les similaires","text":"<p>En dehors de GPT, on retrouve d\u2019autres mod\u00e8les autoregressifs comme Transformer-XL et XLNet, qui am\u00e9liorent encore plus la compr\u00e9hension du contexte.</p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#iii-variantes-et-extensions-des-transformers","title":"III. Variantes et extensions des Transformers","text":"<p>Les Transformers ne s\u2019arr\u00eatent pas l\u00e0. Certains mod\u00e8les apportent des innovations sp\u00e9cifiques\u202f: - BERT : Ici, pas d\u2019autoregression. BERT utilise une approche bidirectionnelle, qui lui permet de comprendre le contexte avant et apr\u00e8s chaque mot. - XLNet : Ce mod\u00e8le combine les forces des deux mondes (autoregression et bidirectionnalit\u00e9) pour mieux capturer le sens des phrases.</p> <p>Les Transformers ne se limitent pas au traitement du texte. Aujourd\u2019hui, ils sont aussi utilis\u00e9s dans la vision (ex. Vision Transformers), la reconnaissance vocale, et m\u00eame dans l\u2019apprentissage par renforcement.</p>"},{"location":"2024/12/01/comprendre-les-transformers-dans-lia-g%C3%A9n%C3%A9rative/#conclusion","title":"Conclusion","text":"<p>Les Transformers, c\u2019est vraiment l\u2019outil indispensable en NLP moderne. Que ce soit pour traduire, r\u00e9sumer ou g\u00e9n\u00e9rer du texte, leur impact est \u00e9norme. Si tu veux approfondir, voici quelques ressources que je te recommande vivement\u202f:</p> <ul> <li>L\u2019article fondateur : Attention is All You Need </li> <li>Guide sur les Transformers : Hugging Face </li> <li>Blog sur GPT et XLNet : Loick Bourdois </li> </ul>"},{"location":"2024/09/01/mlflow--best-practices/","title":"MLflow &amp; best practices","text":"<p>Dans un projet de machine learning, g\u00e9rer les exp\u00e9rimentations et les mod\u00e8les peut rapidement devenir un casse-t\u00eate. Imaginez : l'\u00e9quipe s'agrandit, les exigences fusent de toutes parts\u2026 et l\u00e0, votre chef de projet d\u00e9barque avec une demande sp\u00e9ciale : \"Dis, tu te souviens de ce mod\u00e8le super performant qu'on a test\u00e9 en avril ? On aimerait le comparer avec nos r\u00e9sultats actuels.\"\u00c0 ce moment pr\u00e9cis, \u00e0 moins d'avoir une m\u00e9moire digne d'un \u00e9l\u00e9phant, vous vous retrouvez \u00e0 naviguer fr\u00e9n\u00e9tiquement dans des fichiers Excel. Entre nous, c'est le genre de situation o\u00f9 l'on se dit : \"Pourquoi je n\u2019ai pas tout not\u00e9 quelque part de fa\u00e7on plus propre ?!\"C\u2019est justement l\u00e0 qu\u2019MLflow entre en sc\u00e8ne.</p>"},{"location":"2024/09/01/mlflow--best-practices/#les-composantes-de-mlflow","title":"Les Composantes de MLflow","text":"<p>MLflow est une plateforme open source qui g\u00e8re tout le cycle de vie de vos mod\u00e8les de machine learning. Voici ses principales fonctionnalit\u00e9s pour rendre votre vie (et celle de votre chef de projet) beaucoup plus simple :</p> <ol> <li> <p>MLflow Tracking : Un journal de bord d\u00e9taill\u00e9 qui consigne chaque exp\u00e9rimentation, avec les param\u00e8tres, m\u00e9triques, artefacts, et m\u00eame les versions de code. Plus besoin de deviner quels param\u00e8tres ont \u00e9t\u00e9 utilis\u00e9s pour ce mod\u00e8le de mai dernier !</p> </li> <li> <p>MLflow Projects : Pour structurer vos projets de mani\u00e8re reproductible, plut\u00f4t que d'avoir des scripts \u00e9parpill\u00e9s un peu partout. MLflow Projects aide \u00e0 organiser votre code et vos donn\u00e9es, pour que tout soit toujours en ordre.</p> </li> <li> <p>MLflow Models : D\u00e9ployez facilement vos mod\u00e8les, peu importe le framework utilis\u00e9 (Scikit-learn, TensorFlow, PyTorch, etc.).</p> </li> <li> <p>Model Registry : Gardez une trace de tous vos mod\u00e8les, de leur version, et sachez lesquels sont en production ou en test. C\u2019est comme une biblioth\u00e8que, mais pour vos mod\u00e8les ML.</p> </li> </ol>"},{"location":"2024/09/01/mlflow--best-practices/#pourquoi-utiliser-mlflow","title":"Pourquoi Utiliser MLflow ?","text":"<p>\u00c0 mesure que votre projet de machine learning \u00e9volue, vous faites face \u00e0 plusieurs d\u00e9fis :</p> <ul> <li>Semaine 1 : Les donn\u00e9es changent constamment   Vous commencez avec un jeu de donn\u00e9es initial, mais rapidement, de nouvelles donn\u00e9es arrivent, n\u00e9cessitant des ajustements constants des pipelines et des tests d'int\u00e9gration pour maintenir la coh\u00e9rence.</li> </ul> <ul> <li>Semaine 3 : Collaboration avec l'\u00e9quipe   Un nouveau data scientist rejoint l'\u00e9quipe, apportant de nouvelles id\u00e9es et m\u00e9thodes. Vous devez suivre les contributions de chacun et coordonner les efforts pour \u00e9viter duplications et incoh\u00e9rences.</li> </ul> <ul> <li>Semaine 5 : Multiplication des exp\u00e9rimentations   Vous essayez divers mod\u00e8les et configurations, et chaque exp\u00e9rimentation produit des r\u00e9sultats diff\u00e9rents. Cela devient difficile de suivre quel mod\u00e8le a \u00e9t\u00e9 form\u00e9 avec quels param\u00e8tres et quelles donn\u00e9es.</li> </ul> <ul> <li>Semaine 7 : Demandes croissantes des parties prenantes   Les responsables de projet et les \u00e9quipes m\u00e9tiers demandent des rapports sur les performances des mod\u00e8les et souhaitent voir les meilleurs mod\u00e8les d\u00e9ploy\u00e9s rapidement en production ou en test.</li> </ul> <p>MLflow vous aide \u00e0 r\u00e9pondre efficacement \u00e0 ces d\u00e9fis en fournissant :</p> <ul> <li>Une tra\u00e7abilit\u00e9 compl\u00e8te des exp\u00e9rimentations : Vous pouvez toujours savoir quels param\u00e8tres et donn\u00e9es ont \u00e9t\u00e9 utilis\u00e9s pour chaque mod\u00e8le.</li> <li>Une collaboration facilit\u00e9e : Les contributions de chaque membre de l'\u00e9quipe sont document\u00e9es et accessibles.</li> <li>Une gestion simplifi\u00e9e des d\u00e9ploiements : MLflow permet de d\u00e9ployer facilement les mod\u00e8les les plus performants pour les besoins des \u00e9quipes m\u00e9tiers.</li> </ul>"},{"location":"2024/09/01/mlflow--best-practices/#configuration-de-mlflow","title":"Configuration de MLflow","text":"<p>En fonction de l'utilisation, la configuration de MLflow peut varier. Vous pouvez installer MLflow avec la commande suivante :</p> <pre><code>pip install mlflow\n</code></pre> <p>Pour des pratiques plus MLOps, MLflow est g\u00e9n\u00e9ralement h\u00e9berg\u00e9 sur un serveur sp\u00e9cifique, soit sur une machine virtuelle (via conteneur) ou sur le cloud. Vous pouvez consulter ce projet GitHub pour un exemple de configuration de MLflow.</p> <p></p> Configuration D\u00e9veloppement solo avec Localhost D\u00e9veloppement solo avec Base de Donn\u00e9es Locale D\u00e9veloppement en \u00c9quipe avec Serveur MLflow Sc\u00e9nario Localhost (par d\u00e9faut) Suivi local avec base de donn\u00e9es Suivi distant avec serveur MLflow Cas d'utilisation D\u00e9veloppement solo D\u00e9veloppement solo D\u00e9veloppement en \u00e9quipe Description Par d\u00e9faut, MLflow enregistre les m\u00e9tadonn\u00e9es et artefacts de chaque run dans un r\u00e9pertoire local, <code>mlruns</code>. C'est la m\u00e9thode la plus simple pour d\u00e9marrer sans configuration suppl\u00e9mentaire. Le client MLflow peut se connecter \u00e0 une base de donn\u00e9es compatible SQLAlchemy (par ex. SQLite, PostgreSQL, MySQL). Cela permet une meilleure gestion des donn\u00e9es d'exp\u00e9rimentation sans avoir \u00e0 configurer un serveur. Le serveur de suivi MLflow peut \u00eatre configur\u00e9 avec un proxy HTTP pour les artefacts, permettant de g\u00e9rer les requ\u00eates d'artefacts via le serveur de suivi sans interagir directement avec les services de stockage (S3 par exemple) et une base de donn\u00e9es pour le suivi. Id\u00e9al pour les d\u00e9veloppements en \u00e9quipe avec stockage centralis\u00e9."},{"location":"2024/09/01/mlflow--best-practices/#organisation-du-projet","title":"Organisation du Projet","text":"<p>Pour int\u00e9grer MLflow proprement et efficacement, il est recommand\u00e9 d\u2019isoler le code fonctionnel du code de suivi. Voici une structure de projet exemple :</p> <pre><code>project/\n\u2502\n\u251c\u2500\u2500 main.py                # Script principal d'ex\u00e9cution\n\u2514\u2500\u2500 src/\n    \u251c\u2500\u2500 __init__.py        # Init du package src\n    \u251c\u2500\u2500 modeling.py        # Code pour l'entra\u00eenement et l'\u00e9valuation des mod\u00e8les\n    \u251c\u2500\u2500 plot_utils.py      # Fonctions utilitaires pour les graphiques\n    \u2514\u2500\u2500 mlflow_utils.py    # Fonctions sp\u00e9cifiques \u00e0 MLflow pour enregistrer les r\u00e9sultats\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#exemple-de-code","title":"Exemple de Code","text":""},{"location":"2024/09/01/mlflow--best-practices/#srcmlflow_utilspy","title":"<code>src/mlflow_utils.py</code>","text":"<p>Voici un exemple de fonction pour enregistrer les artefacts avec MLflow :</p> <pre><code>from loguru import logger\nimport matplotlib.pyplot as plt\nimport mlflow\n\ndef log_artifacts(metrics, params, fig, model):\n    \"\"\"\n    Enregistre les param\u00e8tres, m\u00e9triques, graphiques, et mod\u00e8le dans MLflow.\n\n    Args:\n        metrics (dict): Dictionnaire des m\u00e9triques de performance du mod\u00e8le.\n        params (dict): Dictionnaire des param\u00e8tres du mod\u00e8le.\n        fig (plotly figure): Graphique des r\u00e9sultats.\n        model (object): Le mod\u00e8le entra\u00een\u00e9 (par exemple, un mod\u00e8le scikit-learn).\n    \"\"\"\n    try:\n        logger.info(\"Logging parameters to MLflow\")\n        mlflow.log_params(params) \n\n        logger.info(\"Logging metrics to MLflow\")\n        mlflow.log_metrics(metrics) \n\n        logger.info(\"Logging figure to MLflow\")\n        fig_path = \"results_plot.png\"\n        fig.savefig(fig_path)\n        mlflow.log_artifact(fig_path, \"figures\")  \n\n        logger.info(\"Logging model to MLflow\")\n        mlflow.sklearn.log_model(model, \"model\")  \n\n        logger.info(\"All artifacts logged successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to log artifacts: {e}\")\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcplot_utilspy","title":"<code>src/plot_utils.py</code>","text":"<p>Voici une fonction pour cr\u00e9er un graphique comparant les valeurs r\u00e9elles et pr\u00e9dites :</p> <pre><code>import plotly.graph_objects as go\n\ndef create_plot(X_test, y_test, y_pred):\n    \"\"\"Cr\u00e9e un graphique comparant les valeurs r\u00e9elles aux valeurs pr\u00e9dites avec des points pour les vraies valeurs et une ligne pour les pr\u00e9dites.\"\"\"\n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_test, \n        mode='markers', \n        name='Actual', \n        marker=dict(size=8, color='rgba(152, 0, 0, .8)', line=dict(width=1, color='DarkSlateGrey'))\n    ))\n\n    fig.add_trace(go.Scatter(\n        x=X_test.squeeze(), \n        y=y_pred, \n        mode='lines', \n        name='Predicted',\n        line=dict(color='rgba(0, 152, 0, .8)', width=2)\n    ))\n\n    fig.update_layout(\n\n\n        title='Actual vs Predicted Values',\n        xaxis_title='Feature Value',\n        yaxis_title='Target Value',\n        template='plotly_white'\n    )\n\n    return fig\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#srcmodelingpy","title":"<code>src/modeling.py</code>","text":"<p>Voici un exemple de module modeling: <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef simulate_data(seed=42, size=100):\n    \"\"\"Simulates data for training.\"\"\"\n    np.random.seed(seed)\n    X = np.random.rand(size, 1) * 10\n    y = 3 * X.squeeze() + 4 + np.random.randn(size) * 2\n    return pd.DataFrame(data={'X': X.squeeze(), 'y': y})\n\ndef train_model(X_train, y_train, alpha, l1_ratio):\n    \"\"\"Trains an ElasticNet model.\"\"\"\n    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    model.fit(X_train, y_train)\n    return model\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"Evaluates the model on the test set.\"\"\"\n    y_pred = model.predict(X_test)\n    mse = round(mean_squared_error(y_test, y_pred), 2)\n    r2 = round(r2_score(y_test, y_pred), 2)\n    return y_pred, {\"mse\": mse, \"r2\": r2}\n</code></pre></p>"},{"location":"2024/09/01/mlflow--best-practices/#script-principal-mainpy","title":"Script Principal <code>main.py</code> :","text":"<p>Le script principal utilise log_artifacts pour int\u00e9grer MLflow et suivre l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.</p> <pre><code>from loguru import logger \nimport argparse\nimport mlflow\nfrom sklearn.model_selection import train_test_split\nfrom modeling import simulate_data, train_model, evaluate_model\nfrom mlflow_utils import log_artifacts\nfrom plot_utils import create_plot\n\n# Constants\nEXPERIMENT_NAME = 'blog-backbone'\nRUN_NAME = \"simple linear regression\"\nREMOTE_SERVER_URL = \"http://mlflow_server:5000\" \n\ndef main(alpha, l1_ratio):\n    \"\"\"Fonction principale pour ex\u00e9cuter l'entra\u00eenement et l'\u00e9valuation du mod\u00e8le.\"\"\"\n    logger.info(\"Starting script execution\")\n\n    logger.info(\"Simulating data\")\n    data = simulate_data()\n    X_train, X_test, y_train, y_test = train_test_split(data[['X']], data['y'], test_size=0.2, random_state=42)\n\n    logger.info(\"Training model with alpha: {} and l1_ratio: {}\".format(alpha, l1_ratio))\n    model = train_model(X_train, y_train, alpha, l1_ratio)\n\n    logger.info(\"Model evaluation\")\n    y_pred, metrics = evaluate_model(model, X_test, y_test)\n\n    logger.info(\"Preparing results\")\n    params = {\n        \"alpha\": alpha,\n        \"l1_ratio\": l1_ratio,\n        \"test_size\": 0.2\n    }\n    fig = create_plot(X_test, y_test, y_pred)\n\n    logger.info(\"Main execution completed\")\n    return metrics, params, fig, model\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"ElasticNet model training script\")\n    parser.add_argument(\"--alpha\", type=float, required=True, help=\"Alpha parameter for ElasticNet\")\n    parser.add_argument(\"--l1_ratio\", type=float, required=True, help=\"L1 ratio parameter for ElasticNet\")\n    args = parser.parse_args()\n    mlflow.set_tracking_uri(REMOTE_SERVER_URL)\n    mlflow.set_experiment(EXPERIMENT_NAME)    \n    with mlflow.start_run(run_name=RUN_NAME):\n        metrics, params, fig, model = main(args.alpha, args.l1_ratio)\n        log_artifacts(metrics, params, fig, model)\n</code></pre>"},{"location":"2024/09/01/mlflow--best-practices/#etape-dexecution","title":"\u00c9tape d'Ex\u00e9cution","text":"<p>Pour ex\u00e9cuter le projet et observer les r\u00e9sultats de l'enregistrement des exp\u00e9rimentations avec MLflow, on pourrait utiliser la proc\u00e9dure ci-apr\u00e8s:</p> <ol> <li> <p>Configuration du PYTHONPATH :    Avant d'ex\u00e9cuter le script principal, configurez le <code>PYTHONPATH</code> pour que Python reconnaisse correctement les modules de votre projet. Sans cette configuration, vous pourriez \u00eatre amen\u00e9 \u00e0 utiliser des chemins relatifs dans le code, ce qui peut compliquer les choses. \u2019utilisation de PYTHONPATH permet de sp\u00e9cifier le r\u00e9pertoire racine de vos modules Python, \u00e9vitant ainsi les chemins relatifs compliqu\u00e9s dans votre code. Cela simplifie l'importation des modules et rend le code plus propre et plus maintenable. Configurez <code>PYTHONPATH</code> avec la commande suivante dans un terminal :</p> <pre><code>export PYTHONPATH=\"src/\"\n</code></pre> </li> <li> <p>Ex\u00e9cution du Script Principal :    Utilisez <code>argparse</code> pour rendre votre script param\u00e9trable depuis la ligne de commande. Voici comment lancer le script avec diff\u00e9rents param\u00e8tres :</p> <pre><code>python -m main --alpha 0.1 --l1_ratio 0  #L2  ridge regression\npython -m main --alpha 0.1 --l1_ratio 1  #L1  lasso regression\n</code></pre> <ul> <li><code>argparse</code> : Rend le script interactif et param\u00e9trable depuis la ligne de commande, facilitant les tests avec diff\u00e9rents param\u00e8tres sans modifier le code source. </li> </ul> </li> </ol> <p>Bien que vous puissiez utiliser des notebooks, il est souvent recommand\u00e9 de travailler avec des fichiers <code>.py</code> pour une meilleure organisation et gestion du projet, notamment pour des projets de prototypage ou d'industrialisation.</p>"},{"location":"2024/09/01/mlflow--best-practices/#interface-utilisateur-mlflow","title":"Interface Utilisateur MLflow","text":"<p>Apr\u00e8s avoir ex\u00e9cut\u00e9 les commandes ci-dessus, vous pouvez v\u00e9rifier que les exp\u00e9rimentations sont correctement enregistr\u00e9es en acc\u00e9dant \u00e0 l'interface utilisateur de MLflow. Vous y trouverez les param\u00e8tres, les m\u00e9triques, les graphiques, et les mod\u00e8les associ\u00e9s \u00e0 chaque exp\u00e9rimentation. Pour acc\u00e9der \u00e0 cette interface, vous devez utiliser l'URL de votre serveur MLflow. Personnellement, j'utilise un reverse proxy pour acc\u00e9der \u00e0 mon service. En mode local, vous pouvez d\u00e9marrer l'interface utilisateur avec la commande <code>mlflow ui</code>.</p> <p></p> <p>Comme le montre l'image ci-dessus, les deux exp\u00e9rimentations que j'ai lanc\u00e9es sont enregistr\u00e9es. Chacune affiche les param\u00e8tres, les m\u00e9triques et les temps d'ex\u00e9cution. Vous pouvez obtenir plus de d\u00e9tails en cliquant sur une exp\u00e9rimentation sp\u00e9cifique.</p> <p></p> <p>Dans la section des artefacts de la premi\u00e8re exp\u00e9rimentation, vous pouvez r\u00e9cup\u00e9rer votre graphique (n'est-ce pas g\u00e9nial ?), ainsi que les m\u00e9tadonn\u00e9es o\u00f9 sont sauvegard\u00e9s les coefficients. Vous trouverez \u00e9galement le mod\u00e8le enregistr\u00e9, que vous pouvez utiliser directement pour faire des pr\u00e9dictions.</p>"},{"location":"2024/09/01/mlflow--best-practices/#conclusion","title":"Conclusion","text":"<p>Alors, la prochaine fois que quelqu\u2019un vous demande de retrouver des resulats datant de trois mois, vous pourrez vous permettre de sourire et dire : \"Pas de probl\u00e8me, je vais chercher \u00e7a dans MLflow.\" Parce qu'avec MLflow, fini le stress des donn\u00e9es \u00e9gar\u00e9es et des param\u00e8tres oubli\u00e9s ; tout est \u00e0 port\u00e9e de clic, pr\u00eat \u00e0 \u00eatre revisit\u00e9, analys\u00e9 et, bien s\u00fbr, utilis\u00e9 \u00e0 bon escient. Quand on utilise du Mlflow c'est que aimerait faire du MLOps et c'est d\u00e9j\u00e0 un bon debut. Faire du code propre vous permettra d'integrer mlfow dans vos projets</p>"},{"location":"2024/09/01/mlflow--best-practices/#references","title":"R\u00e9f\u00e9rences:","text":"<ul> <li>https://mlflow.org/docs/latest/index.html</li> <li>https://mlflow.org/docs/latest/tracking.html#quickstart</li> </ul>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/iagen/","title":"IAGEN","text":""},{"location":"category/processing/","title":"Processing","text":""},{"location":"category/mlops/","title":"MLOps","text":""},{"location":"category/cicd/","title":"CI/CD","text":""},{"location":"category/documentation/","title":"documentation","text":""},{"location":"category/open-source/","title":"Open Source","text":""},{"location":"category/tips/","title":"Tips","text":""},{"location":"category/cloud/","title":"Cloud","text":""},{"location":"page/2/","title":"Personal blog","text":""},{"location":"archive/2024/page/2/","title":"2024","text":""}]}